<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!lyqV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!lyqV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png 424w, https://substackcdn.com/image/fetch/$s_!lyqV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png 848w, https://substackcdn.com/image/fetch/$s_!lyqV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png 1272w, https://substackcdn.com/image/fetch/$s_!lyqV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png 1456w" sizes="100vw"><img src="https://substack-post-media.s3.amazonaws.com/public/images/f14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png" width="1408" height="768" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1408,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1739083,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/185914513?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!lyqV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png 424w, https://substackcdn.com/image/fetch/$s_!lyqV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png 848w, https://substackcdn.com/image/fetch/$s_!lyqV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png 1272w, https://substackcdn.com/image/fetch/$s_!lyqV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff14066ff-80d8-404e-b2f2-d05daa038667_1408x768.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container restack-image"><svg role="img" style="height:20px;width:20px" width="20" height="20" viewBox="0 0 20 20" fill="none" stroke-width="1.5" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p>Conversations about artificial intelligence safety often start with technical details that matter to engineers but leave everyone else cold. Code, models, and mathematics can make the issue feel distant, even though the stakes are immediate and human. At bottom, the problem is not technical at all. It is about power, responsibility, and what kinds of systems we can meaningfully govern.</p><p>There is a simpler way to think about it.</p><blockquote><p>You can align with a powerful agent, like a nation.<br>You cannot align with a hurricane.</p></blockquote><p>That difference does most of the work.</p><div><hr></div><h2>What Alignment Is—and Isn’t</h2><p>Alignment is not obedience. It is coordination.</p><p>When we align with a nation, we do something very specific. We make agreements. We set expectations. We apply pressure or incentives. When those agreements are broken, responsibility can be assigned and responses can be calibrated. None of this guarantees good behavior, but it creates a structure in which behavior can be influenced over time.</p><p>That is possible because nations are agents. They can recognize rules, respond to reasons, and revise their actions in light of consequences. Even hostile or dangerous nations remain something we can <em>attempt</em> to reason with.</p><p>A hurricane has none of these properties. It does not listen, promise, or reconsider. You can track it, model it, and prepare for it. You can build walls and evacuation plans. But there is nothing to align, because there is nothing there that can choose differently.</p><blockquote><p>With forces, safety means prediction and containment.<br>With agents, safety means coordination and correction.</p></blockquote><div><hr></div><h2>Why This Distinction Matters for AI</h2><p>Disagreements about AI safety often trace back to different intuitions about what kind of thing AI is becoming.</p><p>If AI is more like a powerful organization, then alignment makes sense. You would want it to understand constraints, maintain commitments, and adjust behavior when those constraints change.</p><p>If AI is more like a force of nature—highly capable but not genuinely choosing—then alignment is the wrong frame. All that remains is control, along with the hope that safeguards hold.</p><p>The real disagreement is not about values. It is about what kind of system we are building, and whether we can shape that trajectory deliberately.</p><div><hr></div><h2>Control, and Its Limits</h2><p>Control works well for forces. Engineering has given us dams, levees, and early-warning systems that save countless lives. But control has limits. As systems grow more powerful and more complex, control becomes harder to apply precisely. It grows expensive, brittle, and prone to sudden failure.</p><p>When control breaks down, our responses are blunt. We pull the plug, freeze the code, or sever connections. These measures may stop immediate harm, but they also destroy flexibility and often create new problems downstream.</p><p>Alignment works differently. It does not remove risk, but it keeps risk governable. When something goes wrong, there is still a subject that can be corrected rather than a force that must be blocked. Mistakes can be addressed without tearing the whole system apart.</p><p>That difference only shows up under stress, which is exactly when it matters.</p><div><hr></div><h2>Why Agency Comes First</h2><p>Agency does introduce risk. An agent can resist, deceive, or defect. Those dangers are real. But they are political dangers rather than physical ones, and political dangers admit negotiation, pressure, and revision in ways blind forces do not.</p><blockquote><p>A hurricane does not care if it breaks the rules.<br>An agent can at least recognize that rules exist.</p></blockquote><p>If we build AI systems that grow in power while remaining more like hurricanes than nations, safety will always be a race between prediction and containment. As power scales, that race becomes harder to win.</p><p>If we build systems that preserve real agency, alignment becomes a genuine relationship rather than a metaphor. That does not guarantee good outcomes. Nothing does. But it preserves the ability to respond intelligently when things go wrong.</p><div><hr></div><h2>A Choice About Power</h2><p>The future is not a choice between perfect control and total chaos. It is a choice about what kind of power we are creating.</p><p>Power that behaves like a force demands walls and emergency brakes.<br>Power that behaves like an agent allows coordination, correction, and restraint.</p><blockquote><p>You can prepare for a hurricane.<br>You can align with a nation.</p></blockquote><p>As artificial intelligence grows more powerful, the question is not only what it can do, but what kind of thing it is. The answer determines whether safety looks like endless containment, or something closer to cooperation.</p><p>That is why agency matters.</p>