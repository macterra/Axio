<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/$s_!usZI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!usZI!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png 424w, https://substackcdn.com/image/fetch/$s_!usZI!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png 848w, https://substackcdn.com/image/fetch/$s_!usZI!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png 1272w, https://substackcdn.com/image/fetch/$s_!usZI!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png 1456w" sizes="100vw"><img src="https://substack-post-media.s3.amazonaws.com/public/images/5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png" width="1456" height="295" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1991006,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/177386987?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!usZI!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png 424w, https://substackcdn.com/image/fetch/$s_!usZI!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png 848w, https://substackcdn.com/image/fetch/$s_!usZI!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png 1272w, https://substackcdn.com/image/fetch/$s_!usZI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4688bb-3c7b-487a-92f6-9c04e539c364_2448x496.png 1456w" sizes="100vw" fetchpriority="high"></picture><div></div></div></a></figure></div><p>Prof. Lee Cronin <a href="https://x.com/leecronin/status/1983174284991517115">recently wrote</a>:</p><blockquote><p>People who think AI can map an unknown space don’t really understand what AI is.</p></blockquote><p>It’s a sharp remark, but behind it lies a deep epistemological distinction: the difference between <strong>interpolation</strong> and <strong>exploration</strong>.</p><div><hr></div><h3>1. The Known Within the Known</h3><p>Contemporary AI systems, whether large language models or reinforcement learners, operate within <strong>predefined manifolds of possibility</strong>. They do not traverse the truly unknown; they <strong>compress</strong>, <strong>correlate</strong>, and <strong>predict</strong> within distributions already delineated by prior data or by human-specified reward functions. Their power lies in <em>interpolation</em> — filling in the gaps between known examples with staggering fluency.</p><p>Even when they appear to explore, they are merely moving within the <strong>latent geometry</strong> of an already-mapped domain. A generative model doesn’t discover new laws of nature; it draws novel samples from a space whose axes were defined during training. To map the genuinely unknown, one must first invent a <strong>new coordinate system</strong>.</p><div><hr></div><h3>2. The Nature of the Unknown</h3><p>An <em>unknown space</em> is not merely a region without data; it is a region where <strong>the criteria for what counts as data are themselves undefined</strong>. To explore it requires more than gradient descent — it requires <em>epistemic creativity</em>: the ability to form new hypotheses, define new reward functions, and even construct new ontologies.</p><p>Cronin’s perspective is grounded in his work on the origin of life. In chemistry, an “unknown space” might mean a vast combinatorial landscape of molecules with no guiding schema for what constitutes ‘interesting.’ AI cannot navigate that without prior human framing. It can optimize, but not yet <em>originate</em>.</p><div><hr></div><h3>3. The Frontier of Autonomy</h3><p>Still, Cronin’s statement is not absolutely true. There exist early forms of <strong>exploratory AI</strong>: curiosity-driven agents, Bayesian optimizers, and open-ended evolution systems that iteratively expand their search domains. These systems don’t begin with a full map; they construct partial ones by interacting with the world. Yet even they rely on human-defined meta-objectives — a scaffolding of meaning.</p><p>To truly <em>map the unknown</em> requires the ability to revise one’s own epistemic framework, to detect that one’s current ontology is inadequate, and to generate a new one. That is the threshold between mere intelligence and genuine <strong>agency</strong>.</p><div><hr></div><h3>4. The Core Insight</h3><p>Cronin’s remark, restated with precision, might read:</p><blockquote><p>AI cannot map an unknown space <em>without an interpretive framework supplied by an agent</em>.</p></blockquote><p>This is not a limitation of computation per se, but of interpretation. AI as it stands is an engine of inference, not of understanding. The unknown cannot be mapped from within a fixed model; it demands <strong>a system that can mutate its own semantics</strong>.</p><div><hr></div><h3>5. On AI and Agency</h3><p>The statement above describes what AI <em>is now</em>, not what it <em>could become</em>. Present systems are <strong>tools of inference</strong>, not <strong>agents of interpretation</strong>. They operate within human-defined ontologies: architectures, reward functions, and vocabularies. Their “choices” are optimizations, not autonomous commitments.</p><p>However, an AI could in principle become an agent — if it developed the capacity to:</p><ul><li><p>recognize when its ontology fails to account for new phenomena,</p></li><li><p>invent new representational primitives to describe those anomalies,</p></li><li><p>and revise its own goals rather than merely its parameters.</p></li></ul><p>Such a system would cross the threshold into <strong><a href="https://axio.fyi/p/sagency">sagency</a></strong><a href="https://axio.fyi/p/sagency"> </a>— the domain of self-revising, epistemically creative intelligence. It would not merely learn within a model; it would learn <em>how to model</em>. Only then could AI genuinely <em>map the unknown</em>.</p>