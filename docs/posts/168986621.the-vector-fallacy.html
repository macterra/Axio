<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Vector Fallacy</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="../publications.html">← Back to Publications</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">The Vector Fallacy</h1>
<p class="post-subtitle">Mistaking intrinsic nature for representational limits in AI</p>
<p class="post-date">July 22, 2025</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png 424w, ../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png 848w, ../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png 1272w, ../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png 1456w" sizes="100vw"><img src="../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png" width="1456" height="295" data-attrs="{&quot;src&quot;:&quot;../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2595604,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.substack.com/i/168986621?img=../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png 424w, ../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png 848w, ../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png 1272w, ../images/ca10a4af-5dc1-4ebe-925d-1ebc755704df_2448x496.png 1456w" sizes="100vw" fetchpriority="high"></picture><div></div></div></a></figure></div><p>A recent <a href="https://x.com/yeetgenstein/status/1947739182744347117">tweet </a>encapsulated a common critique of large language models (LLMs):</p><blockquote><p>"Ultimately, concepts aren’t vectors—concepts don’t in general add or subtract or scalar-multiply—which prevents LLMs from becoming AGI."</p></blockquote><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png 424w, ../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png 848w, ../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png 1272w, ../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png 1456w" sizes="100vw"><img src="../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png" width="752" height="271" data-attrs="{&quot;src&quot;:&quot;../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:271,&quot;width&quot;:752,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:30747,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://axio.substack.com/i/168986621?img=../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png 424w, ../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png 848w, ../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png 1272w, ../images/a90ebf9b-76c0-4d90-82bf-61206d3d3d9e_752x271.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container restack-image"><svg role="img" style="height:20px;width:20px" width="20" height="20" viewBox="0 0 20 20" fill="none" stroke-width="1.5" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p>At first glance, this seems reasonable. Human concepts—such as justice, freedom, or consciousness—are intricate and context-rich. They're not neatly reducible to mathematical vectors manipulated by linear algebra.</p><p>Yet there's a subtle fallacy embedded here: the assumption that if something isn't intrinsically numeric or algebraic, it can't be effectively represented or processed numerically. This assumption can be examined clearly through analogy:</p><p>Consider music. Music is profoundly emotional, cultural, and subjective. It's clearly not intrinsically numeric—musical compositions don't literally "add" or "subtract" like algebraic entities. Yet computers compose, edit, and perform music daily by representing melodies, harmonies, rhythms, and even expressive nuances as numeric data. Does this mean computers misunderstand music? Not necessarily. It means they've found an effective numeric representation that works at the level of practical approximation and artistic expressiveness.</p><p>Similarly, although concepts aren't literally vectors, large language models demonstrate convincingly that numeric embeddings—vectors—can approximate complex conceptual relationships surprisingly well. Analogies, metaphors, and semantic connections emerge naturally from these numerical spaces. The argument "concepts aren't vectors, therefore LLMs can't achieve AGI" falters precisely because it mistakes the intrinsic nature of something (concepts, music) with limitations on its representation.</p><p>The relevant question isn't "Are concepts intrinsically vectors?" but rather, "Can concepts be adequately represented as vectors to enable human-like reasoning?" Empirically, embedding spaces already support sophisticated reasoning tasks previously assumed impossible.</p><p>Thus, while pure vector embeddings may indeed have limitations and might benefit from additional symbolic, causal, or hierarchical structures, dismissing their potential based solely on intrinsic nature is unjustified. Just as numerical representations brought music composition into the computational age, vector-based representations could well support—or at least significantly contribute to—the emergence of Artificial General Intelligence.</p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
