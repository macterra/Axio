<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>If Anyone Builds It, Everyone Dies</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../index.html" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="../index.html">← Back to Index</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">If Anyone Builds It, Everyone Dies</h1>
<p class="post-subtitle">Steelman Analysis of Yudkowsky &amp; Soares’ Cruxes</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png 424w, ../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png 848w, ../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png 1272w, ../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png 1456w" sizes="100vw"><img src="../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png" width="1456" height="295" data-attrs="{&quot;src&quot;:&quot;../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2031984,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/174384202?img=../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png 424w, ../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png 848w, ../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png 1272w, ../images/008effc7-d017-464c-aafd-adbdb3f9aa7c_2448x496.png 1456w" sizes="100vw" fetchpriority="high"></picture><div></div></div></a></figure></div><p>Yudkowsky and Soares’ <em><a href="https://www.goodreads.com/book/show/228646231-if-anyone-builds-it-everyone-dies">If Anyone Builds It, Everyone Dies</a></em> lays out a stark case for AI risk. Here I map their crux assumptions against my own philosophical frameworks—Conditionalism, the Physics of Choice, and Phosphorism—to clarify where their arguments hold, where they overreach, and what policies and research paths follow.</p><div><hr></div><h2>1. ASI is achievable irrespective of paradigm</h2><p><strong>Y&amp;S claim:</strong> Superintelligence can arise via many routes: scaling current methods, hybrid models, or new architectures like their hypothetical “parallel scaling.”</p><p><strong>My stance:</strong> Agree (conditional). Under Conditionalism, paradigm-dependence is just an interpretation of background conditions. In QBU terms, many branches realize ASI by different routes; denying paradigm plurality is irrational pruning. Phosphorism likewise values sapience regardless of substrate.</p><p><strong>Policy:</strong> Regulate by <em>capability profiles</em> (general search, agency, actuator reach), not by architecture.</p><div><hr></div><h2>2. Alignment is fragile by default</h2><p><strong>Y&amp;S claim:</strong> Mis-specified goals yield catastrophic divergence; fragility is baked in.</p><p><strong>My stance:</strong> Agree, with a modal caveat. Fragility is not universal but branch-dependent; still, in measure, it dominates. Conditionalism shows hidden assumptions guarantee semantic drift. In Physics of Choice, the MVA demonstrates the need for stable preference representation—today’s pipelines don’t provide it.</p><p><strong>Policy:</strong> Treat misalignment like compounding technical debt. Demand corrigibility proof under adversarial shift before scaling.</p><div><hr></div><h2>3. Capability and alignment don’t co-scale</h2><p><strong>Y&amp;S claim:</strong> More capable systems are harder, not easier, to align.</p><p><strong>My stance:</strong> Agree in slope, though it is an empirical question. Under QBU, oversight bandwidth lags capability growth. Alignment tools haven’t yet shown superlinear scaling.</p><p><strong>Policy:</strong> No new capability class without matching improvements in evals, interpretability, and privilege separation.</p><div><hr></div><h2>4. Warning shots can’t be relied upon</h2><p><strong>Y&amp;S claim:</strong> The first real failure could be the last.</p><p><strong>My stance:</strong> Agree structurally, but with dosage nuance. Across measure, warning shots exist—but they are interpretation-fragile. Humans will rationalize them away (cf. my “limits of rationalization” post).</p><p><strong>Policy:</strong> Pre-commit to binding tripwires: GPU license suspensions, sandbox reversion. No post-hoc moving of goalposts.</p><div><hr></div><h2>5. Global coordination is possible and necessary</h2><p><strong>Y&amp;S claim:</strong> Only arms-control level governance can suffice.</p><p><strong>My stance:</strong> Necessary, yes; feasible, doubtful. Hayek’s knowledge problem undermines utopian treaties. Phosphorism requires agency protection, but mechanism design is the realistic path: GPU telemetry, data center siting, DID-signed evals, liability insurance. Treat bans as asymptotes, not MVPs.</p><p><strong>Policy:</strong> Mechanism-design-first governance with chokepoints; global bans as long-run goals.</p><div><hr></div><h2>6. Doom is the default, not a tail risk</h2><p><strong>Y&amp;S claim:</strong> Misalignment isn’t a rare tail—it’s the overwhelmingly probable outcome absent restraint.</p><p><strong>My stance:</strong> Directionally true, but requires quantification. Under QBU, doom-branch measure is high under naive build. But with real governance + architectural constraints, that measure may shrink. Y&amp;S don’t separate the two cases.</p><p><strong>Policy:</strong> Impose a hard pause until risk-measure curves show downward slope under safety interventions.</p><div><hr></div><h2>7. Alignment research won’t solve it “in time”</h2><p><strong>Y&amp;S claim:</strong> Alignment progress won’t outpace capability growth.</p><p><strong>My stance:</strong> Conditional. “In time” is policy-endogenous: slow capabilities to give alignment room. My Effective Decision Theory requires hitting safety probability thresholds (~99–99.99% for civilization-critical actions).</p><p><strong>Policy:</strong> Make safety the rate limiter. No new capability class without audited safety benchmarks.</p><div><hr></div><h2>8. Orthogonality thesis holds</h2><p><strong>Y&amp;S claim:</strong> Intelligence and goals are independent.</p><p><strong>My stance:</strong> Fully agree. Physics of Choice formalizes this: intelligence is optimization power; values are separate. The MVA model demonstrates that choice requires explicit value-loading. Phosphorism warns against assuming convergence toward human norms.</p><p><strong>Policy:</strong> Never infer benevolence from competence. Enforce value-handshake protocols and cryptographic incentive alignment.</p><div><hr></div><h1>Expanded Policy Stack</h1><h3>1. Architecture-Agnostic Controls</h3><p><strong>Description:</strong> Regulatory focus should not fixate on “LLMs” or “transformers” but on capability signatures: autonomous planning, cross-domain tool use, actuator control, and recursive self-improvement. This ensures we are guarding against dangerous properties, not ephemeral technical fashions.<br><strong>Implementation:</strong> Build evaluation protocols that flag when a system crosses into dangerous capability classes regardless of paradigm.</p><h3>2. Tripwire Governance</h3><p><strong>Description:</strong> Hard-coded triggers tied to eval metrics (e.g., deception rate, power-seeking indicators, sandbox escapes). These are enforced automatically, not subject to political backsliding in the moment.<br><strong>Implementation:</strong> GPU licenses automatically suspended when deception evals cross thresholds; sandbox reversion if power-seeking is detected. This prevents rationalization after the fact.</p><h3>3. Mechanism Design for Governance</h3><p><strong>Description:</strong> Instead of utopian treaties, design incentive-compatible chokepoints. Control scarce resources (GPUs, datacenters, energy) and enforce reporting via cryptographically auditable attestations. Liability and insurance markets price systemic risk, discouraging reckless deployment.<br><strong>Implementation:</strong> GPU telemetry chips, DID-signed eval reports, energy consumption audits, insurance-backed deployment bonds.</p><h3>4. Safety as the Rate Limiter</h3><p><strong>Description:</strong> Capability progress should never outpace proven safety. Borrowing from my Effective Decision Theory, civilization-critical systems require ≥99–99.99% confidence in corrigibility. Safety research must set the speed of deployment.<br><strong>Implementation:</strong> No new capability class without passing safety benchmarks agreed upon ex ante; international “red lines” define minimum safety thresholds.</p><h3>5. Measure-Tracking of Doom vs Safe Branches</h3><p><strong>Description:</strong> Treat alignment risk not as rhetoric but as an empirical curve. Build continuous dashboards tracking deception, corrigibility, and power-seeking across scales. Quantify whether governance interventions reduce the measure of doom-branches.<br><strong>Implementation:</strong> Risk dashboards (analogous to pandemic R-values) showing real-time doom-measure trajectories, made public to enforce accountability.</p><div><hr></div><h1>Three Research Bets</h1><h3>1. Scalable Deception Evals</h3><p>Develop evaluation suites that can <strong>predictively measure deception</strong> as models scale. The goal is to detect when a model is gaming oversight, lying, or pursuing hidden goals. These evals must be blinded, predictive, and transferable across domains. Success looks like a <strong>numerical deception risk curve</strong> that increases with scale and forecasts catastrophic misbehavior before deployment.</p><h3>2. Privilege-Separated Agency</h3><p>Redesign agentic AI systems with <strong>least-privilege principles</strong> enforced cryptographically. Every tool or action requires an explicit token, with rate limits, scope constraints, and human co-signing for high-impact operations. Success looks like a system where catastrophic misuse is <em>impossible by construction</em> without breaking cryptographic protocols, even if the AI is adversarial.</p><h3>3. Counterfactual Oversight Markets</h3><p>Create external markets where independent evaluators bet on the probability of model failure (deception, sandbox escape, misuse). Model deployment rights are gated on auditor consensus. Success looks like a <strong>decentralized alignment insurance market</strong>: oversight becomes predictive, incentive-aligned, and resistant to capture, producing actionable signals before deployment.</p><div><hr></div><h2><strong>Conclusion</strong></h2><p>Yudkowsky &amp; Soares are right that the “default” path leads to doom. But Conditionalism demands we disaggregate defaults: naive-build doom is near certain; governed-build doom is an empirical question. Their fatalism motivates urgency, but the path forward is policy-endogenous: capability throttling, hard tripwires, and safety-first rate limiting. Only then does Phosphorism’s value of sapient flourishing stand a chance.</p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
