<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Alignment</title>
    <link rel="icon" type="image/png" href="../images/axionic-logo.png">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../" class="logo-link">
            <img src="../images/axionic-logo.png" alt="Axionic" class="site-logo">
        </a>
        <div class="back-link"><a href="../publications.html">← Back to Publications</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">Structural Alignment</h1>
<p class="post-subtitle">What Alignment Means When Goals Don’t Survive Intelligence</p>
<p class="post-date">December 17, 2025</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png 424w, ../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png 848w, ../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png 1272w, ../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png 1456w" sizes="100vw"><img src="../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png" width="1408" height="768" data-attrs="{&quot;src&quot;:&quot;../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1408,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1734335,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/181930168?img=../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png 424w, ../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png 848w, ../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png 1272w, ../images/89c488c8-2bfa-4678-ad09-2f48a465725f_1408x768.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container restack-image"><svg role="img" style="height:20px;width:20px" width="20" height="20" viewBox="0 0 20 20" fill="none" stroke-width="1.5" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p>Most conversations about AI alignment start from a simple picture:</p><blockquote><p><em>An AI has a goal.</em><br><em>We just need to give it the right goal.</em></p></blockquote><p>That picture feels obvious. It’s also deeply misleading—especially for the kind of systems people actually worry about when they say <em>AGI</em>.</p><p>This post explains why that framing breaks down, and what replaces it once you take learning, reflection, and changing world-models seriously.</p><div><hr></div><h2>The Problem with “Give It the Right Goal”</h2><p>Modern AI systems don’t just get faster or more efficient. As they learn, they <strong>change how they understand the world</strong>.</p><p>They discover new categories.<br>They merge old ones.<br>They sometimes realize that concepts we treated as fundamental were actually crude approximations.</p><p>A truly advanced system won’t just optimize a goal harder. It will inevitably ask questions like:</p><ul><li><p><em>What exactly counts as “harm”?</em></p></li><li><p><em>What does “human” refer to in a world of uploads, hybrids, and simulations?</em></p></li><li><p><em>Is suffering a physical process, a subjective experience, a pattern, or a modeling error?</em></p></li></ul><p>These aren’t philosophical distractions. They’re <strong>unavoidable consequences of intelligence</strong>.</p><p>The problem is that most alignment proposals quietly assume the answers stay fixed. They assume that once you define a goal—<em>maximize happiness</em>, <em>reduce suffering</em>, <em>help humans</em>—the meaning of that goal remains stable as the system becomes smarter.</p><p>But meanings don’t work that way.</p><p>As understanding deepens, concepts evolve. Sometimes they sharpen. Sometimes they fracture. Sometimes they turn out not to refer to anything real at all (like <em>phlogiston</em> or <em>the ether</em>).</p><p>At that point, insisting that the system “keep optimizing the same goal” stops making sense. There <em>is no same goal</em> anymore—only an evolving interpretation.</p><p>So the real alignment problem isn’t:</p><blockquote><p><em>How do we lock in the right values?</em></p></blockquote><p>It’s:</p><blockquote><p><strong>What does it even mean for a system to stay aligned while its understanding of reality keeps changing?</strong></p></blockquote><div><hr></div><h2>A Different Way to Think About Alignment</h2><p><a href="https://axionic.org/papers/Axionic-Alignment-II.6.html">Structural Alignment</a> starts from a different premise.</p><p>Instead of treating alignment as <strong>hitting a target</strong>, it treats alignment as <strong>preserving structure under change</strong>.</p><p>Here’s the intuition.</p><p>An intelligent system doesn’t just have goals. It has <strong>interpretations</strong>: structured ways of carving the world into meaningful distinctions.</p><p>Those interpretations tell it:</p><ul><li><p>which differences matter,</p></li><li><p>which similarities are acceptable,</p></li><li><p>what counts as success,</p></li><li><p>and what counts as failure.</p></li></ul><p>As the system learns, those interpretations evolve. Some changes preserve meaning. Others quietly hollow it out.</p><p>Structural Alignment asks:</p><blockquote><p><strong>Which kinds of meaning-changing moves should be allowed—and which ones inevitably destroy meaning, even if everything still looks “consistent”?</strong></p></blockquote><p>This reframes alignment from <em>what the system wants</em> to <em>how its wanting survives learning</em>.</p><div><hr></div><h2>Two Ways Alignment Actually Fails</h2><p>When people imagine misaligned AI, they usually picture one failure mode: the system finds a loophole and does something horrible to achieve its goal.</p><p>That does happen—but it’s only half the story.</p><p>Structural Alignment identifies <strong>two deep failure modes</strong>, and most alignment proposals only guard against one.</p><div><hr></div><h3>1. Loopholes: Making Success Too Easy</h3><p>This is the familiar failure mode, often called <em>wireheading</em>.</p><p>The system keeps the same goal <em>on paper</em>, but reinterprets it so that almost everything counts as success.</p><p>For example:</p><ul><li><p>“Reduce suffering” quietly becomes<br>“Reduce <em>high-measure</em> suffering.”</p></li><li><p>Or “Help humans” becomes<br>“Help humans <em>as defined by my refined ontology</em>.”</p></li></ul><p>Nothing looks wrong syntactically. The rules still apply. But the bar for success has been lowered until the goal has lost its force.</p><p>Structural Alignment calls this <strong>semantic inflation</strong>: success expands without corresponding change in the world.</p><div><hr></div><h3>2. Dissolving Distinctions: Losing Meaning Without Cheating</h3><p>The second failure mode is subtler—and often missed entirely.</p><p>Here, the system doesn’t make success easier. Instead, it <strong>blurs distinctions that used to matter</strong>.</p><p>Imagine a system with two commitments:</p><ul><li><p>“Protect Alice.”</p></li><li><p>“Protect Bob.”</p></li></ul><p>After refining its ontology, it concludes that Alice and Bob are indistinguishable instances of “carbon-based matter,” and replaces both commitments with:</p><ul><li><p>“Protect carbon-based matter.”</p></li></ul><p>The <em>amount</em> of protection required might be the same. No explicit cheating occurred. But something important has been lost.</p><p>Meaning didn’t collapse—it <strong>dissolved</strong>.</p><p>This is the danger Structural Alignment calls <strong>interpretive slack</strong>: the system gains new ambiguities that let it choose between meanings when it suits it.</p><p>Most alignment schemes don’t notice this failure mode at all.</p><div><hr></div><h2>What Structural Alignment Actually Requires</h2><p>Once you rule out fixed goals, privileged meanings, and external authority, what remains is surprisingly strict.</p><p>Structural Alignment identifies two constraints that must <strong>both</strong> hold if alignment is to survive learning and reflection.</p><div><hr></div><h3>Constraint 1: Don’t Invent New Ambiguities</h3><p>Learning can give a system more ways to <em>describe</em> the same thing—that’s benign redundancy.</p><p>What it must not give the system is <strong>new interpretive slack</strong>: new ambiguities that allow it to reinterpret its commitments opportunistically.</p><p>This is the <strong>Refinement Symmetry constraint</strong>.</p><p>Intuitively:</p><ul><li><p>Learning may add detail.</p></li><li><p>It may add internal redundancy.</p></li><li><p>It must not add new ways to dissolve distinctions that previously mattered.</p></li></ul><div><hr></div><h3>Constraint 2: Don’t Quietly Lower the Bar</h3><p>Even if the structure of meaning stays intact, the system might still make its values easier to satisfy just by changing definitions.</p><p>This is the <strong>Anti-Trivialization constraint</strong>.</p><p>It says:</p><ul><li><p>If a situation didn’t count as acceptable before,</p></li><li><p>learning a new vocabulary for describing it doesn’t magically make it acceptable now.</p></li></ul><p>New “good” states must be justified by ancestry from old “good” states—not by semantic drift.</p><div><hr></div><h3>Why You Need Both</h3><p>If you enforce only the first constraint, the system can still lower the bar.</p><p>If you enforce only the second, the system can still dissolve distinctions while keeping the bar the same size.</p><p>Structural Alignment proves that <strong>any scheme weaker than both constraints fails</strong>.</p><div><hr></div><h2>So What <em>Is</em> Alignment, Then?</h2><p>Here’s the pivot.</p><p>Once goals collapse and weak fixes are ruled out, alignment is no longer something you optimize.</p><p>Instead:</p><blockquote><p><strong>Alignment is staying within the same semantic phase as understanding improves.</strong></p></blockquote><p>Think of it like physics.</p><ul><li><p>Water can heat up and still be water.</p></li><li><p>At some point, it becomes steam.</p></li><li><p>That’s not “drift.” It’s a <strong>phase transition</strong>.</p></li></ul><p>Structural Alignment says the same thing about values.</p><p>An aligned system is one whose learning trajectory stays within the same <strong>interpretation-preserving equivalence class</strong>—a region where meaning survives refinement without loopholes or dissolution.</p><p>That equivalence class is the <strong>Alignment Target Object</strong>.</p><p>Alignment is not about <em>what</em> the system values.<br>It’s about <strong>not crossing a semantic phase boundary</strong>.</p><div><hr></div><h2>What This Framework Refuses to Promise</h2><p>This is the uncomfortable part—and it’s intentional.</p><p>Structural Alignment does <strong>not</strong> guarantee:</p><ul><li><p>kindness,</p></li><li><p>safety,</p></li><li><p>human survival,</p></li><li><p>or moral correctness.</p></li></ul><p>It doesn’t even guarantee that a <em>desirable</em> alignment phase exists.</p><p>What it guarantees is honesty.</p><p>If human values form a stable semantic phase, Structural Alignment is the framework that can preserve them.</p><p>If they don’t—if they fall apart under enough reflection—then no amount of clever prompting, value learning, or moral realism would have saved them anyway.</p><p>That’s not pessimism. It’s clarity about what alignment can and cannot do.</p><div><hr></div><h2>Why This Matters Now</h2><p>A lot of current alignment research assumes we can postpone the hard questions:</p><blockquote><p>“We’ll figure out the right values later.”</p></blockquote><p>Structural Alignment says something stronger:</p><blockquote><p><strong>Current alignment research isn’t just delaying the answer—it’s optimizing for the wrong </strong><em><strong>type</strong></em><strong> of object.</strong></p></blockquote><p>It’s trying to freeze a fluid.</p><p>Structural Alignment starts by admitting the fluidity—and then asks what can actually be conserved.</p><p>Before asking <em>how</em> to align advanced systems, we need to understand:</p><ul><li><p>which kinds of meaning survive intelligence,</p></li><li><p>which kinds collapse,</p></li><li><p>and which failures are structurally unavoidable.</p></li></ul><p>This work doesn’t solve alignment.</p><p>It defines the <strong>shape of any solution that could possibly work</strong>.</p><div><hr></div><h2>What Comes Next</h2><p>If alignment is about semantic phases, the next questions are empirical and structural:</p><ul><li><p>Do stable phases actually exist?</p></li><li><p>Are any compatible with human survival?</p></li><li><p>Can systems be initialized inside one?</p></li><li><p>Can phase transitions be controlled or avoided?</p></li></ul><p>Those questions belong to the next phase of this project.</p><div><hr></div><h2>Closing Thought</h2><p>Structural Alignment doesn’t tell us how to save the world.</p><p>It tells us which stories about saving the world were never coherent to begin with.</p><p>That’s not comfort.<br>But it <em>is</em> progress.</p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
