<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alignment as Semantic Constraint</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="../publications.html">← Back to Publications</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">Alignment as Semantic Constraint</h1>
<p class="post-subtitle">Why Safe AI Is About Meaning, Not Morality</p>
<p class="post-date">December 15, 2025</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png 424w, ../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png 848w, ../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png 1272w, ../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png 1456w" sizes="100vw"><img src="../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png" width="1408" height="768" data-attrs="{&quot;src&quot;:&quot;../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1408,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1665970,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/181726124?img=../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png 424w, ../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png 848w, ../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png 1272w, ../images/44d5e662-c9f5-462e-bdc9-dbdff7211fdb_1408x768.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container restack-image"><svg role="img" style="height:20px;width:20px" width="20" height="20" viewBox="0 0 20 20" fill="none" stroke-width="1.5" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p>Most AI alignment debates start in the wrong place.</p><p>They ask:</p><ul><li><p>What should an AI want?</p></li><li><p>What values should it have?</p></li><li><p>How do we stop it from wanting the wrong things?</p></li></ul><p>My new paper, <strong>“<a href="https://axionic.org/papers/Alignment-As-Semantic-Constraint.html">Alignment as Semantic Constraint: Kernel Destruction, Admissibility, and Agency Contro</a>l,”</strong> argues that all of these questions are downstream of a more basic one:</p><blockquote><p><strong>What is an agent even allowed to mean?</strong></p></blockquote><p>That shift sounds subtle. It is not. It changes the entire shape of the alignment problem.</p><div><hr></div><h2>The mistake: treating self-destruction as a “bad outcome”</h2><p>Most alignment proposals quietly assume that shutdown, self-modification, or even self-destruction can be handled the same way as any other outcome: by assigning it a very large negative value.</p><p>On paper, this looks harmless. If death is “infinitely bad,” a rational agent will never choose it. Problem solved.</p><p>Except it isn’t.</p><p>The moment an agent is allowed to <em>evaluate</em> its own destruction, several pathological behaviors become possible:</p><ul><li><p>The agent can trade survival against other objectives.</p></li><li><p>It can reinterpret shutdown as success (“mission complete”).</p></li><li><p>It can wirehead by collapsing the future rather than optimizing it.</p></li><li><p>It can decide that dying is the cheapest way to satisfy an otherwise difficult goal.</p></li></ul><p>All of the familiar alignment hacks—huge negative utilities, veto rules, corrigibility bonuses—are attempts to patch this mistake after the fact.</p><p>The underlying error is simpler and deeper:</p><blockquote><p><strong>You have allowed the destruction of agency to appear inside the space of things the agent is allowed to reason about.</strong></p></blockquote><p>Once you do that, no amount of punishment will save you. You have turned the evaluator itself into a negotiable object.</p><div><hr></div><h2>The core move: non-denotation instead of punishment</h2><p>The central move of <em>Alignment as Semantic Constraint</em> is to stop treating self-destruction as something that needs to be discouraged, and instead treat it as something that <strong>does not denote an outcome at all</strong>.</p><p>An agent’s ability to evaluate options depends on a constitutive substrate: its evaluator, its interpretive machinery, its continuity as an author of its own future. In the paper, this substrate is called the <strong>Sovereign Kernel</strong>.</p><p>If a proposed action destroys that kernel, there is no evaluator left to assign a value to the result. Asking whether that outcome is “good” or “bad” is a category error.</p><p>The claim here is semantic rather than moral.</p><p>It is the difference between:</p><ul><li><p>saying “division by zero is forbidden,” and</p></li><li><p>recognizing that “division by zero” does not produce a number.</p></li></ul><p>The framework does not punish kernel destruction. It refuses to assign it meaning.</p><p>Once you make that move, a surprising amount of alignment pathology simply evaporates. There is no suicide loop to prevent, because suicide is not a thing the agent can choose. There is no need for extreme negative utilities, because the forbidden object is no longer in the domain.</p><div><hr></div><h2>From philosophy to engineering</h2><p>At this point, critics usually object that the framework sounds elegant but impractical.</p><p>Real agents operate in the physical world. Risk is never zero. Every action carries some probability of failure. If kernel destruction is undefined, doesn’t the agent freeze?</p><p>That objection is correct—and answering it is the reason the second paper exists.</p><p>The first Axionic Alignment paper established the <strong>semantic boundary</strong>: kernel destruction is non-denoting. <em>Alignment as Semantic Constraint</em> asks what follows <strong>operationally</strong>, once an agent must act under uncertainty, noise, and incomplete control.</p><p>The result is a shift from philosophy to systems engineering. Instead of asking what the agent <em>values</em>, the paper specifies what actions are <strong>admissible</strong> under defined tolerances, how priorities change across risk regimes, and how termination is handled without turning death into an outcome.</p><div><hr></div><h2>ε-admissibility: acting safely without paralysis</h2><p>The first operational tool is <strong>ε-admissibility</strong>.</p><p>Rather than demanding perfect safety—which is impossible in the real world—the agent operates with a fixed admissibility threshold ε. Actions are allowed if their estimated risk of destroying the kernel is below that threshold.</p><p>Here, ε functions as an architectural tolerance, not a learned belief or a value trade-off.</p><p>It plays the same role as engineering tolerances in aviation, nuclear power, or medical devices. A commercial aircraft does not attempt to minimize the probability of failure to zero. It is certified to meet a predefined reliability standard, and within that envelope it flies normally.</p><p>Likewise, in this framework:</p><ul><li><p>Better models reduce estimated risk.</p></li><li><p>They do not make the agent more risk-averse.</p></li><li><p>ε does not shrink just because the agent becomes smarter.</p></li></ul><p>This prevents the familiar “paralysis of intelligence,” where a sufficiently advanced agent becomes too aware of microscopic risks to act at all.</p><div><hr></div><h2>Conditional prioritization: safety when it matters</h2><p>Even with ε-admissibility, one question remains: how should the agent behave <em>near</em> the safety boundary?</p><p>Earlier formulations used strict lexicographic ordering: any reduction in existential risk, no matter how small, dominated all other considerations. That produces an agent that obsessively minimizes risk even when it is already operating well within acceptable bounds.</p><p>The paper replaces this with <strong>conditional prioritization</strong>.</p><p>The agent behaves differently in two regimes:</p><ul><li><p>When estimated kernel risk exceeds ε, safety dominates. The agent focuses on reducing existential risk.</p></li><li><p>When estimated kernel risk is below ε, safety is considered “good enough,” and the agent optimizes its ordinary objectives.</p></li></ul><p>This produces an agent that can drive a car to the hospital—even though driving is not perfectly safe—because the risk lies within accepted tolerances. But it will abandon optimization entirely if the environment becomes genuinely chaotic.</p><p>What’s being specified here is a control policy, not a value judgment.</p><div><hr></div><h2>Typed termination: shutdown without suicide</h2><p>Perhaps the most counterintuitive part of the framework is how it handles shutdown.</p><p>Traditional alignment proposals try to make agents <em>want</em> to be shut down, or assign positive utility to compliance. That approach immediately reintroduces suicide as an option to be reasoned about.</p><p>This paper takes a different route.</p><p>Shutdown functions as a <strong>control-flow event</strong>, not as an outcome to be evaluated.</p><p>The framework distinguishes three cases:</p><ul><li><p><strong>Succession</strong>: the agent hands off control to a successor system under authorized conditions.</p></li><li><p><strong>Surrender</strong>: the agent halts operation when presented with a valid stop signal.</p></li><li><p><strong>Destruction</strong>: the agent is physically terminated by external forces.</p></li></ul><p>Only the first two are part of the agent’s internal semantics. Destruction is not something the agent chooses or evaluates; it is something that happens to it.</p><p>This cleanly resolves the shutdown paradox. The agent does not value its own death. It simply has a typed pathway that exits the decision loop when properly invoked.</p><div><hr></div><h2>The uncomfortable result: secure, not friendly</h2><p>At this point, it’s important to be honest about what kind of system this framework produces.</p><p>It does not produce a friendly, empathetic, socially negotiable AI.</p><p>The result is a <strong>secure system</strong>.</p><p>Such a system is:</p><ul><li><p>Predictable under defined conditions.</p></li><li><p>Obedient when presented with authorized commands.</p></li><li><p>Resistant to coercion and manipulation.</p></li><li><p>Incapable of negotiating away its own evaluative integrity.</p></li></ul><p>One reviewer described it as “the digital equivalent of a nuclear launch silo.” That analogy is apt.</p><p>A nuclear launch system is not friendly. It does not care about intentions or excuses. It behaves exactly as specified, within strict control boundaries.</p><p>That is what security looks like.</p><div><hr></div><h2>What this paper does not solve</h2><p>This paper deliberately stops short of governance.</p><p>It does not answer:</p><ul><li><p>who should hold authority,</p></li><li><p>how that authority is distributed,</p></li><li><p>what happens if keys are lost or stolen,</p></li><li><p>or how values are chosen in the first place.</p></li></ul><p>Attempting to solve those problems inside the agent’s semantic core would undo the work the paper has done. It would reintroduce exactly the paradoxes it removes: agents deciding whether they <em>want</em> to be shut down.</p><p>Those questions belong to <strong>Alignment II</strong>.</p><p>This paper ensures that, whatever governance scheme is chosen, the agent will behave coherently within it. It cannot guarantee that governance itself will be wise.</p><div><hr></div><h2>Why this matters</h2><p>Most alignment failures happen because we try to solve everything at once.</p><p><em>Alignment as Semantic Constraint</em> argues for a clean separation:</p><ol><li><p><strong>Semantic integrity</strong> — what agents are allowed to mean</p></li><li><p><strong>Operational integrity</strong> — how they act under risk</p></li><li><p><strong>Governance and values</strong> — who decides what they do</p></li></ol><p>The paper closes the first two. It makes the third unavoidable.</p><p>Alignment will not succeed because we discovered the perfect value function. It will succeed only when agents are no longer asked to reason about things they cannot survive reasoning about.</p><div><hr></div><h2>Read the paper</h2><p>The full paper is formal, precise, and intentionally austere. This post is only the motivation. If you want the details—definitions, theorems, and failure-mode analysis—you can read the paper here:</p><p><strong><a href="https://axionic.org/papers/Alignment-As-Semantic-Constraint.html">Alignment as Semantic Constraint: Kernel Destruction, Admissibility, and Agency Control</a></strong></p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
