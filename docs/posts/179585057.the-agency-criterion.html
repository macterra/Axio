<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Agency Criterion</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../index.html" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="../publications.html">← Back to Publications</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">The Agency Criterion</h1>
<p class="post-subtitle"> Separating real intelligence from pattern-generation</p>
<p class="post-date">November 21, 2025</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png 424w, ../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png 848w, ../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png 1272w, ../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png 1456w" sizes="100vw"><img src="../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png" width="1456" height="295" data-attrs="{&quot;src&quot;:&quot;../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2556319,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/179585057?img=../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png 424w, ../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png 848w, ../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png 1272w, ../images/f1e28c7e-ef9f-40cb-95a9-b0dc978e0e75_2448x496.png 1456w" sizes="100vw" fetchpriority="high"></picture><div></div></div></a></figure></div><p><a href="https://x.com/karpathy/status/1991910395720925418">Andrej Karpathy recently contrasted animal minds with large language models</a> by arguing that both arise from different optimization pressures. His comparison is insightful but implicitly treats both systems as variations of the same underlying kind of mind. The Axio framework reveals a sharper boundary.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg 424w, ../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg 848w, ../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg 1272w, ../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg 1456w" sizes="100vw"><img src="../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg" width="1456" height="794" data-attrs="{&quot;src&quot;:&quot;../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:794,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="Image" title="Image" srcset="../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg 424w, ../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg 848w, ../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg 1272w, ../images/fbeeb949-c57d-4a3b-821d-2bdbb3627e14_2816x1536.jpeg 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container restack-image"><svg role="img" style="height:20px;width:20px" width="20" height="20" viewBox="0 0 20 20" fill="none" stroke-width="1.5" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p>Every mind-like process is the result of an optimizer. But not every optimized process produces <strong>intelligence</strong>. Intelligence, in Axio terms, requires <a href="163805376.intelligence-is-a-game-we-play.html">agency: the ability to play a game</a>, select among alternatives, and pursue preferred outcomes. Many discussions—including Karpathy’s framing—blur this distinction by treating any sophisticated pattern-producer as an intelligence, leading to persistent anthropomorphism.</p><p>Animal minds arise from evolutionary pressures that force strategic interaction with the world. Evolution produces genuine agents—entities that form preferences, pursue goals, and select actions under uncertainty.</p><p>LLMs arise from a fundamentally different regime. They are optimized to construct coherent continuations of text, not to win any game or pursue any objective. They do not form preferences, evaluate outcomes, or select actions. They generate <strong>coherence</strong> without agency.</p><p>Karpathy’s contrast is therefore best used as a springboard. It highlights the difference in optimization regimes, but Axio reveals the deeper point: <strong>only one regime produces intelligence</strong>.</p><h2>The Animal Regime</h2><p>Biological intelligence is sculpted by the oldest and most unforgiving objective function: <em>survive long enough to reproduce agents that can also survive</em>. The pressures include:</p><ul><li><p>embodied vulnerability and continual threat exposure</p></li><li><p>predator-prey dynamics</p></li><li><p>coalition politics and social inference</p></li><li><p>need for adaptive generalization across environments</p></li><li><p>existential penalties for catastrophic mistakes</p></li></ul><p>Strategy is unavoidable: every organism must navigate a world where choices matter. Consequently, animals develop robust, world-model-heavy cognition built for navigating games they must not lose.</p><p>This regime produces intelligence because it produces agents.</p><h2>The LLM Regime: Coherence Without Strategy</h2><p>LLMs are shaped by gradients that never require them to enter a game. They minimize prediction error and satisfy user preferences, but those are not <em>their</em> goals. They do not experience success or failure, only parameter adjustments.</p><p>This produces systems that:</p><ul><li><p>generate high-dimensional linguistic coherence</p></li><li><p>exhibit reasoning-shaped patterns without performing reasoning as an agent</p></li><li><p>reflect strategic behaviors found in their training data</p></li><li><p>lack preferences, objectives, or outcome evaluation</p></li><li><p>fail in ways no biological agent ever could because nothing is at stake</p></li></ul><p>Their abilities—logical chains, explanations, simulations of dialogue—derive from imitating agentic patterns, not from possessing agency themselves.</p><p>The right comparison is not animal intelligence versus LLM intelligence. It is <strong>animal intelligence versus LLM coherence</strong>.</p><h2>Where Karpathy Is Right</h2><p>Karpathy correctly identifies the root cause of widespread confusion: people project agency and motivation onto systems that lack both. They treat LLM outputs as if they were produced by an entity with goals, fears, or self-preservation instincts.</p><p>In this respect, Karpathy’s corrective is useful. LLMs do not have:</p><ul><li><p>drives</p></li><li><p>desires</p></li><li><p>survival imperatives</p></li><li><p>emotional valence</p></li></ul><p>They are optimized for <strong>output quality</strong>, not success in any strategic interaction. Recognizing this prevents many erroneous expectations.</p><h2>Where Karpathy Underestimates the Distinction</h2><p>Karpathy attributes a kind of emerging generality to LLMs as they scale. But this “generality” is not intelligence—it is richer <strong>coherence</strong>. Larger models absorb more human strategies, heuristics, and decision patterns, making their outputs look more agentic. But they do not cross the boundary into being agents.</p><p>Their improvements are not evidence of self-driven goal formation or strategy. They arise from deeper assimilation of human-generated strategic content. Apparent generality is a property of better mirroring, not internal agency.</p><p>Karpathy’s analogy between commercial selection and biological evolution also breaks down under Axio. Commercial pressure shapes tools, not agents. Models do not fight for survival; they are replaced. They do not attempt to persist; they are versioned. There is no game they play.</p><p>LLMs may resemble the outputs of intelligent agents, but resemblance is not agency.</p><h2>The Axio Synthesis</h2><p>Through Axio, the distinction becomes clear. Evolutionary optimization produces <strong>agents</strong> that play strategic games, form preferences, and choose actions—therefore it produces intelligence. Gradient descent produces <strong>coherence constructors</strong> that imitate the surface forms of strategy without ever participating in a game. Karpathy’s contrast between “animal intelligence” and “LLM intelligence” collapses into a cleaner dichotomy: <strong>intelligence versus non-agentic coherence</strong>.</p><p>This resolves the conceptual ambiguity at the center of public debates about artificial minds. LLMs are extraordinary generators of structured representation, but they are not emergent agents.</p><h2>Conclusion</h2><p>Public discourse keeps asking whether LLMs “think.” <a href="177985047.the-turing-test-revisited.html">They do</a>. The more meaningful question is whether they <strong>choose</strong>. They don’t. Only agents choose. Only agents play games. Only agents possess intelligence. What we call “AI” today is neither artificial nor intelligent—it is engineered coherence, built atop the crystallized strategies of minds that actually think.</p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
