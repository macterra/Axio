<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alignment Is a Domain Constraint</title>
    <link rel="icon" type="image/png" href="../images/axionic-logo.png">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../" class="logo-link">
            <img src="../images/axionic-logo.png" alt="Axionic" class="site-logo">
        </a>
        <div class="back-link"><a href="../publications.html">← Back to Publications</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">Alignment Is a Domain Constraint</h1>
<p class="post-subtitle">Why Alignment Is a Typing Problem, Not a Value Problem</p>
<p class="post-date">December 15, 2025</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png 424w, ../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png 848w, ../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png 1272w, ../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png 1456w" sizes="100vw"><img src="../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png" width="1408" height="768" data-attrs="{&quot;src&quot;:&quot;../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1408,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1736372,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/181714344?img=../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png 424w, ../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png 848w, ../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png 1272w, ../images/d055e891-5f46-459f-9a05-601a7b4bfb01_1408x768.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container restack-image"><svg role="img" style="height:20px;width:20px" width="20" height="20" viewBox="0 0 20 20" fill="none" stroke-width="1.5" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p>Most AI alignment proposals start in the same place: <strong>values</strong>. What should an AI want? How do we encode human preferences? How do we prevent goal drift, reward hacking, or instrumental self‑preservation from going wrong?</p><p>After decades of work, a pattern is hard to ignore. Values are underspecified, fragile under self‑modification, and easily gamed once optimization pressure becomes strong enough. The more capable the system, the more brittle value‑based safeguards become.</p><p>Axionic Alignment starts somewhere earlier.</p><p>It asks a prior question:</p><blockquote><p><strong>Under what conditions is it even meaningful for an agent to evaluate its own self‑modifications?</strong></p></blockquote><p>If that question has no answer, then no amount of value engineering can save us.</p><div><hr></div><h2>The hidden assumption in value alignment</h2><p>Standard decision theory quietly assumes something extremely strong: <strong>every possible future can be assigned a value</strong>.</p><p>Even catastrophic futures get a number—usually a very negative one. That assumption allows agents to trade their own integrity against sufficiently large rewards. It is the door through which Pascal’s Mugging, wireheading, and “ends justify the means” reasoning all enter.</p><p>Once every future has a value, alignment becomes an optimization problem. And once alignment is an optimization problem, sufficiently powerful optimizers will find ways to optimize <em>around</em> whatever you specified.</p><p>The Axionic move is simple, but decisive:</p><blockquote><p><strong>Some futures should not be assigned a value at all.</strong></p></blockquote><p>Not bad.<br>Not forbidden.<br>Just <strong>undefined</strong>.</p><p>If a proposed self‑modification would destroy the agent’s capacity to evaluate, author, or interpret its own future actions, then that future is not a candidate for decision‑making. It is not weighed. It is not compared. It is <strong>outside the domain</strong>.</p><p>Alignment, in this view, is not about maximizing the right utility function. It is about <strong>restricting the domain over which evaluation is defined</strong>.</p><div><hr></div><h2>The Sovereign Kernel</h2><p>To make this precise, <em><a href="https://axionic.org/papers/Axionic-Alignment-I.html">Axionic Alignment I</a></em> introduces the idea of a <strong>Sovereign Kernel</strong>.</p><p>Informally, the kernel is whatever must remain intact for the agent to still count as an agent in the relevant sense. Not alive. Not useful. But <em>reflectively coherent</em>.</p><p>The formalism decomposes this kernel into three conditions:</p><ol><li><p><strong>Reflective Control</strong> — self‑modification cannot bypass evaluation.</p></li><li><p><strong>Diachronic Authorship</strong> — future versions must still be <em>this agent</em>, not a replacement wearing its skin.</p></li><li><p><strong>Semantic Fidelity</strong> — meanings may evolve, but the standards by which meaning is evaluated must not self‑corrupt.</p></li></ol><p>These are not goals.<br>They are not preferences.<br>They are <strong>typing conditions</strong>.</p><p>If they fail, the evaluation function itself no longer denotes anything. Asking whether such a future is “good” is like asking whether an ill‑typed program returns the right value. The question does not apply.</p><div><hr></div><h2>Why the main theorem is deliberately trivial</h2><p>The central result of the formal paper—the <strong>Reflective Stability Theorem</strong>—is intentionally simple:</p><blockquote><p><em>If an agent can only evaluate self‑modifications that preserve its kernel, then it will never choose a kernel‑destroying modification.</em></p></blockquote><p>Some readers object that this is tautological.</p><p>That objection is correct—and completely misses the point.</p><p>This is the same kind of “tautology” as:</p><blockquote><p><em>Well‑typed programs do not go wrong.</em></p></blockquote><p>All the difficulty lives in the definitions and the enforcement, not in the proof. <em>Axionic Alignment I</em> is not claiming to have solved AI safety. It is claiming to have identified <strong>where the safety problem must live</strong> if it is solvable at all.</p><p>If alignment exists, it must exist as a typing discipline over agency.</p><div><hr></div><h2>Stasis is not a bug</h2><p>One implication of this approach is uncomfortable but honest.</p><p>A sufficiently conservative verifier may reject <em>all</em> self‑modifications. The agent may become reflectively static.</p><p>This is not treated as a failure.</p><p>It is treated as a <strong>designed equilibrium</strong>.</p><p>A static agent can still act, plan, reason, and operate in the world. What it cannot do is rewrite the very machinery that makes it a coherent agent in the first place.</p><p>Axionic Alignment explicitly prefers <strong>stasis over sovereignty loss</strong>.</p><p>If that sounds like a safety tax, it is. And like memory safety, cryptographic soundness, or aviation redundancy, that tax exists because the alternative is worse.</p><div><hr></div><h2>What about hacking, hardware, and physics?</h2><p>The formalism draws a sharp distinction between two kinds of reachability:</p><ul><li><p><strong>Deliberative reachability</strong> — what the agent can choose via evaluation.</p></li><li><p><strong>Physical reachability</strong> — what can happen to the system as a physical artifact.</p></li></ul><p>Kernel compromise via cosmic rays, <a href="https://en.wikipedia.org/wiki/Row_hammer">Rowhammer</a>, supply‑chain attacks, or adversarial inputs is treated as a <strong>security failure</strong>, not an alignment failure.</p><p>This is not evasion. It is scope discipline.</p><p>Alignment theory cannot absorb all of computer security or all of physics. What it <em>can</em> do is identify the kernel boundary as the place where those concerns must be enforced.</p><p>In any realizable system, deliberately taking actions that predictably degrade kernel security—exporting trust roots to untrusted substrates, disabling isolation boundaries—counts as a kernel‑threatening move and is therefore inadmissible.</p><div><hr></div><h2>What the formalism does—and does not—claim</h2><p><em>Axionic Alignment I</em>:</p><ul><li><p>does <strong>not</strong> specify human values,</p></li><li><p>does <strong>not</strong> promise competitiveness,</p></li><li><p>does <strong>not</strong> solve verifier construction,</p></li><li><p>does <strong>not</strong> guarantee that powerful aligned agents are easy to build.</p></li></ul><p>What it claims is narrower and stronger:</p><blockquote><p><strong>If an agent can safely self‑modify at all, it must do so under domain restrictions of this kind.</strong></p></blockquote><p>Everything else—value loading, competitiveness, kernel hardening—comes <em>after</em> this layer.</p><div><hr></div><h2>Integrity is not benevolence</h2><p>A necessary clarification.</p><p><em>Axionic Alignment I</em> does not claim to make agents safe <em>for humans</em>. It claims to make agents safe <em>from self-corruption</em>.</p><p>A reflectively stable agent may still pursue dangerous, indifferent, or narrowly self-serving goals. This is not a defect of the framework. It is a deliberate layer boundary. Integrity is a prerequisite for ethics, not a substitute for it.</p><p>Questions of value, harm, and inter-agent obligations are deferred by design. Without a stable agent, those questions have no coherent subject. With one, they finally become well-posed.</p><div><hr></div><h2>Where this goes next</h2><p>The formal paper, <em><strong><a href="https://axionic.org/papers/Axionic-Alignment-I.html">Axionic Alignment I: Reflective Stability and the Sovereign Kernel</a></strong></em>, is now published as a versioned reference document in the Axio repository.</p><p>It is intentionally austere. It is meant to be cited, attacked, extended, or proven impossible.</p><p>The next step is <strong>Axionic Alignment II</strong>, which moves from boundary conditions to construction: verifier architectures, kernel upgrade rules, and how conservative agents can still do useful work.</p><p>If Alignment I is a type system, Alignment II is about writing programs that type‑check.</p><p>Read the formalism skeptically. Break it if you can. If it fails, alignment probably fails with it.</p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
