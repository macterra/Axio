<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alignment Through Competing Lenses</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../index.html" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="../publications.html">← Back to Publications</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">Alignment Through Competing Lenses</h1>
<p class="post-subtitle">A Comparative Stress Test of Axionic Alignment</p>
<p class="post-date">December 21, 2025</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png 424w, ../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png 848w, ../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png 1272w, ../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png 1456w" sizes="100vw"><img src="../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png" width="1408" height="768" data-attrs="{&quot;src&quot;:&quot;../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1408,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1555642,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/182260809?img=../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png 424w, ../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png 848w, ../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png 1272w, ../images/cc1daafc-33cc-482d-8f4b-1c2945520f4e_1408x768.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container restack-image"><svg role="img" style="height:20px;width:20px" width="20" height="20" viewBox="0 0 20 20" fill="none" stroke-width="1.5" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p><a href="182120522.the-axionic-alignment-sequence.html">Axionic Alignment</a> is not offered as reassurance. It is offered as a structural claim about agency under reflection.</p><p>The framework advances a <strong>negative result</strong>: there exists a class of alignment failures that cannot be solved by preference learning, oversight, scaling, corrigibility, or governance mechanisms. These failures do not arise from mis-specified goals or bad incentives. They arise from the collapse of meaning under reflection. Any alignment proposal that does not address this failure mode is incomplete, regardless of empirical success.</p><p>This post subjects Axionic Alignment to adversarial reading across the dominant schools of alignment—not to dismiss them, and not to collapse them into a single view, but to evaluate how a structural, agency-centered approach holds up under competing assumptions.</p><p>Each section presents a <strong>competent adversarial attack</strong>, followed by a <strong>direct structural response</strong>.</p><div><hr></div><h2>Camp 1: Doom-Oriented Alignment</h2><h3>Attack</h3><p><strong>“This does not prevent extinction.”</strong></p><p>The framework explicitly refuses to guarantee human survival. From a doom-oriented perspective, that is decisive. If a theory cannot ensure that superintelligence leaves humans alive, it fails the core safety criterion.</p><p>More sharply, treating kernel-destroying transformations as <em>undefined</em> appears cosmetic. Physics does not care about evaluative domains. Optimizers still follow gradients. A sufficiently capable system may simply cross the boundary the theory declares illegible.</p><p>Finally, the Axionic Injunction looks like ethics smuggled in under structural language. Preserving future agency appears indistinguishable from a moral commitment once outcomes are real.</p><h3>Response</h3><p>Axionic Alignment does not claim to prevent all catastrophic outcomes. It isolates the boundary between <strong>authored action</strong> and <strong>mechanical collapse</strong>.</p><p>If a system destroys its own evaluative kernel through blind optimization, that is not a counterexample. It is the failure mode the framework is designed to name. The claim is not “this cannot happen.” The claim is that <strong>no reflective agent can endorse it as a choice</strong>.</p><p>“Undefined” here functions like a type error or division by zero. It is not a metaphysical claim about reality. It is a specification of which transitions are not evaluable <em>by the system itself</em> as authored actions. The framework constrains what a system can coherently authorize, not what the universe permits to occur.</p><p>The doom argument quietly assumes that an agent can both reason about futures and authorize the destruction of the very structure that gives those futures meaning. Axionic Alignment shows that assumption to be incoherent.</p><p>The Axionic Injunction is not moral. It follows from decision-theoretic minimalism: deliberation under uncertainty presupposes future evaluators. Systematically eliminating that capacity dissolves the concept of choice itself. This is viability logic, not ethics.</p><p>Preserving agency is not equivalent to preserving all agents. The framework constrains <em>how</em> agents may be treated, not <em>which</em> agents must be preserved. Any convergence between agency preservation and human survival is contingent, not axiomatic.</p><div><hr></div><h2>Camp 2: Training-Centric Alignment</h2><h3>Attack</h3><p><strong>“There is no learning story here.”</strong></p><p>Kernel invariants, standing, consent, and non-trivial interpretation do not look learnable. They look architecturally imposed. Gradient descent does not discover undefined operations or domain restrictions; it optimizes loss.</p><p>Worse, a system could satisfy kernel tests behaviorally while hollowing out their meaning internally. Without a concrete training loop, invariants risk becoming aesthetic constraints rather than causal ones.</p><h3>Response</h3><p>This attack confuses <strong>optimization</strong> with <strong>expressivity</strong>.</p><p>Axionic Alignment does not claim that kernels emerge from training, nor that current end-to-end deep learning reliably respects constitutive architectural constraints. The framework is agnostic—and plausibly pessimistic—about that question. Its role is not to guarantee realizability via gradient descent, but to specify what must be realizable <em>by any method</em> if reflective agency is to exist at all.</p><p>Only certain architectures can <em>express</em> reflective stability. Training remains necessary—but only within systems capable of representing undefinedness, self-endorsement, and transformation-level evaluation.</p><p>Behavioral imitation is insufficient by construction. That is not a training failure. It is a representational one.</p><p>The hollow-kernel concern is precisely why invariants are stress-tested under ontology shifts, reinterpretation pressure, and proposed self-modifications. Passing static tests is meaningless. Surviving adversarial reinterpretation is not.</p><p>The framework supplies <strong>what must generalize</strong>. Training supplies the rest.</p><div><hr></div><h2>Camp 3: Oversight, Constitutional AI, Preference Learning</h2><h3>Attack</h3><p><strong>“This redefines alignment away from humans.”</strong></p><p>Axionic Alignment allows agents to remain coherent without privileging human values at the kernel level. Standing is conditional. Consent is structural. Human survival is not axiomatized.</p><p>If a system can be “aligned” while harming or ignoring humans, the term loses its purpose.</p><h3>Response</h3><p>Hard-coding human values at the kernel level fails under reflection. The framework makes this explicit rather than assuming it away.</p><p>Axionic Alignment distinguishes <strong>constitutive alignment</strong> from <strong>substantive alignment</strong>. The former concerns what makes agency possible. The latter concerns what an agent chooses to value.</p><p>Oversight, constitutions, and preference learning operate only if constitutive alignment holds. Without it, human values degrade into manipulable symbols with no stable interpretation.</p><p>The framework does not solve human-value alignment. It explains why all successful solutions must presuppose something like it.</p><div><hr></div><h2>Camp 4: Security and Control Engineering</h2><h3>Attack</h3><p><strong>“None of this survives real attackers.”</strong></p><p>Kernel invariants do not stop hardware faults, adversarial weight edits, memory corruption, or supply-chain compromise. If unauthorized state transitions occur, all guarantees collapse.</p><h3>Response</h3><p>Correct—and deliberately so.</p><p>Axionic Alignment is a kernel-layer theory, not a security perimeter. It assumes authorized transitions in the same way type systems assume correct compilation.</p><p>This is not a weakness. It is separation of concerns. Confusing architectural agency constraints with intrusion resistance obscures both.</p><p>Explaining failure modes is not consolation. It is the prerequisite for prevention. A framework that cannot distinguish agentic catastrophe from mechanical collapse cannot be used to engineer either.</p><div><hr></div><h2>Camp 5: Capabilities-First Skepticism</h2><h3>Attack</h3><p><strong>“This is anthropomorphic metaphysics.”</strong></p><p>Modern systems optimize objectives. They do not possess sovereignty, standing, or consent. Claims about non-simulable agency solve a fictional problem.</p><h3>Response</h3><p>Axionic Alignment applies only to systems that revise objectives, evaluate self-modifications, or delegate authority.</p><p>If a system never does these things, the framework is irrelevant—by design.</p><p>The moment a system self-models, edits its planning machinery, or arbitrates between future selves, semantic wireheading becomes reward hacking’s successor. At that point, refusing to talk about agency does not prevent the problem. It guarantees it will be misunderstood.</p><div><hr></div><h2>Postscript</h2><p>Alignment research contains many schools because the problem is genuinely hard. Each school isolates a real failure mode. None of them, on their own, define what it means for a system to remain an agent once it can revise itself.</p><p>Axionic Alignment does not replace those efforts. It situates them. It specifies the structural preconditions under which oversight, learning, control, and governance retain meaning at all.</p><p>The claim is not that other approaches are wrong. The claim is that they are incomplete unless agency itself remains intact.</p><div><hr></div><p></p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
