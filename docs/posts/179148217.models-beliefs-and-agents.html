<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Models, Beliefs, and Agents</title>
    <link rel="icon" type="image/png" href="../images/axionic-logo.png">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../" class="logo-link">
            <img src="../images/axionic-logo.png" alt="Axionic" class="site-logo">
        </a>
        <div class="back-link"><a href="../publications.html">← Back to Publications</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">Models, Beliefs, and Agents</h1>
<p class="post-subtitle">Clarifying representational levels in cognition and control</p>
<p class="post-date">November 17, 2025</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png 424w, ../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png 848w, ../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png 1272w, ../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png 1456w" sizes="100vw"><img src="../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png" width="1456" height="295" data-attrs="{&quot;src&quot;:&quot;../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2336425,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/179148217?img=../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png 424w, ../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png 848w, ../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png 1272w, ../images/78c51b64-411a-401a-b9ff-d0d8bbb2de91_2448x496.png 1456w" sizes="100vw" fetchpriority="high"></picture><div></div></div></a></figure></div><p>The previous essays established two claims that appear, at first glance, to be in tension. One claim is that <strong><a href="179013388.understanding-requires-models.html">understanding</a> and <a href="179071634.control-requires-models.html">control require models</a></strong>. The other is that <strong><a href="177433505.the-nature-of-beliefs.html">beliefs are properties of the models we construct of agents</a>, not necessarily properties instantiated within the agents themselves</strong>. If the first thesis asserts that agents must embody models, while the second asserts that agents may lack beliefs, it is natural to ask whether these positions conflict.</p><p>They do not. The apparent tension dissolves once we distinguish between two different explanatory levels at which “models” operate: internal representational structures and external interpretive attributions.</p><h3>Internal models: structure required for regulation</h3><p>When a system understands or regulates its environment, it must embody internal structure that preserves relevant distinctions within that environment. This follows from the Good Regulator Theorem: any regulator that achieves reliable control must incorporate a representation, explicit or implicit, of the system it regulates. These representations need not be symbolic, conceptual, or linguistically expressible. They may be realised in molecular pathways, neural circuits, or dynamical couplings. Their role is functional rather than propositional. They support prediction, discrimination, and context-appropriate intervention.</p><p>Such internal structures qualify as <strong>models</strong> in the operational sense. They encode aspects of the world’s causal organisation. They are the substrate of understanding and control.</p><h3>External models: the intentional stance</h3><p>In contrast, when we as observers attribute <strong>beliefs</strong> to agents, we do so within our own representational frameworks. Belief, in this interpretive sense, is not a physical or computational component of an agent. It is a feature of a model that we construct to explain and predict the agent’s behaviour. The attribution of belief is justified when it yields accurate and economical predictions of the agent’s actions.</p><p>On this view, a system may regulate its environment without instantiating beliefs. A thermostat contains a mapping from temperature readings to actions. This constitutes a model of the relevant environmental dynamics, yet no interpretation requires that the thermostat believe the room is cold. Similarly, biological organisms often implement sophisticated regulatory mechanisms without representing their internal models as beliefs.</p><h3>Distinct roles for representation</h3><p>The term “model” therefore plays two roles. In the <strong>cybernetic-structural</strong> sense, it refers to the internal organisation that enables an agent to discriminate states, anticipate outcomes, and act coherently. In the <strong>intentional-interpretive</strong> sense, it refers to a descriptive tool we use to characterise the agent’s behaviour in conceptual terms.</p><p>These two senses should not be conflated. Internal models are constitutive of agency; external models are explanatory constructs. An agent must possess the former but need not instantiate the latter.</p><h3>Conditionalism and representational levels</h3><p><a href="178650287.the-conditionalism-sequence.html">Conditionalism</a> accommodates this distinction naturally. Truth claims about agents depend on the background model used to interpret them. When describing an agent’s internal dynamics, the relevant model is the system’s functional organisation. When characterising the agent in cognitive terms, the relevant model is the intentional framework adopted by the observer. Both are models, but they belong to different representational levels.</p><h3>Conclusion</h3><p>Understanding and control require internal models. Beliefs, however, arise only within an observer’s higher-level model of the agent. These two claims refer to different explanatory layers and are therefore compatible. By distinguishing representational structures that enable action from interpretive constructs that explain action, we preserve the coherence of the broader philosophical framework and clarify the architecture of agency.</p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
