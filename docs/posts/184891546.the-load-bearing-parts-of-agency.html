<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Load-Bearing Parts of Agency</title>
    <link rel="icon" type="image/png" href="../images/axionic-logo.png">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../" class="logo-link">
            <img src="../images/axionic-logo.png" alt="Axionic" class="site-logo">
        </a>
        <div class="back-link"><a href="../publications.html">← Back to Publications</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">The Load-Bearing Parts of Agency</h1>
<p class="post-subtitle">Why Some AI Systems Can Never Be Aligned</p>
<p class="post-date">January 17, 2026</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png 424w, ../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png 848w, ../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png 1272w, ../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png 1456w" sizes="100vw"><img src="../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png" width="1408" height="768" data-attrs="{&quot;src&quot;:&quot;../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1408,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1685199,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/184891546?img=../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png 424w, ../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png 848w, ../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png 1272w, ../images/47667bf6-b83e-42bc-8ce5-0edce3c31201_1408x768.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container restack-image"><svg role="img" style="height:20px;width:20px" width="20" height="20" viewBox="0 0 20 20" fill="none" stroke-width="1.5" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M2.53001 7.81595C3.49179 4.73911 6.43281 2.5 9.91173 2.5C13.1684 2.5 15.9537 4.46214 17.0852 7.23684L17.6179 8.67647M17.6179 8.67647L18.5002 4.26471M17.6179 8.67647L13.6473 6.91176M17.4995 12.1841C16.5378 15.2609 13.5967 17.5 10.1178 17.5C6.86118 17.5 4.07589 15.5379 2.94432 12.7632L2.41165 11.3235M2.41165 11.3235L1.5293 15.7353M2.41165 11.3235L6.38224 13.0882"></path></g></svg></button><button tabindex="0" type="button" class="pencraft pc-reset pencraft icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 lucide-maximize-2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></button></div></div></div></a></figure></div><p><em>This post offers a conceptual explanation of </em><strong><a href="https://axionic.org/papers/Axionic-Agency-VIII.6.html">Axionic Agency VIII.6 — Necessary Conditions for Non-Reducible Agency</a></strong><em> without formal notation. The technical paper develops its claims through explicit definitions, deterministic simulation, and preregistered failure criteria. What follows translates those results into narrative form while preserving their structural content.</em></p><div><hr></div><p>Conversations about artificial intelligence usually begin with behavior. We ask whether systems act coherently, whether they pursue goals, whether they adapt to pressure, or whether they appear rational over time. These questions feel natural because behavior is what we can observe. Yet they leave something more basic unexamined. Before asking how a system behaves, it is worth asking what kind of thing it is.</p><p>Over the past two research phases—internally labeled <em>v3.0</em> and <em>v3.1</em>—we set out to answer a simpler but deeper question: what must be present for an artificial system to count as an agent at all, rather than a sophisticated imitation that happens to behave well under certain conditions?</p><p>The result is not a new AI system, nor a claim that agency has been achieved. What we now have is a structural understanding of agency’s supports. We can see, with unusual clarity, where agency holds together and where it fails once key elements are removed. This post explains how we arrived at that understanding, what collapsed along the way, and why the result changes how alignment should be framed.</p><div><hr></div><h2>Learning by Removing Structure</h2><p>Rather than attempting to improve an AI system or push its performance further, we adopted a deliberately destructive strategy. We began with a system that already behaved coherently under a range of conditions, and then removed internal components one at a time. Each removal was guided by the same principle: if a component truly matters to agency, then removing it should cause the system to stop behaving like an authored agent, rather than merely becoming slower, noisier, or less efficient.</p><p>This is a familiar idea in other domains. Engineers learn which beams matter by seeing what happens when they are taken away. Biologists learn which organs matter by observing what fails when they are damaged. We applied the same logic to agency, treating it as a structural phenomenon rather than a surface behavior.</p><p>What emerged from this process was strikingly consistent. Four internal components proved indispensable. Whenever any one of them was removed, the system no longer behaved as an agent in any meaningful sense. What remained was something mechanical: rule execution without authorship, pattern replay without responsibility.</p><div><hr></div><h2>Reasons That Bind Choices</h2><p>The first component we tested was the system’s internal reasoning structure—the chain of derivations that connects rules, commitments, and actions. These are not explanations offered to humans, but internal justifications that make an action <em>the system’s own</em>.</p><p>When those derivations were removed, the system still possessed the same rules and constraints. Nothing about its external environment changed. What changed was the way those rules influenced behavior. They no longer bound actions in an authored way. The system could still act, but its actions lost the character of being <em>chosen for reasons</em>. At that point, agency collapsed.</p><p>This reveals something subtle but important. Rules alone do not create agency. Enforcement alone does not create agency. Agency arises when rules are connected to reasons that actively justify choices from the system’s own point of view.</p><div><hr></div><h2>Meaning Inside Deliberation</h2><p>We then turned to the system’s deliberative process itself. The formal structure of reasoning was left intact, but the meaningful content was removed. The system was forced to operate over symbols that no longer exposed their meaning to the system itself.</p><p>The outcome was immediate and robust. Even though the formal machinery remained unchanged, the system could no longer sustain coherent decision-making. It could not reliably distinguish high-stakes conflicts from trivial ones, nor maintain stable priorities across situations. Its choices lost direction and consistency.</p><p>What this shows is that agency depends on more than formal manipulation. Deliberation must operate over representations that give the system <strong>access to what those representations are about</strong>. When that access is removed—when symbols no longer connect to the concepts they stand for—reasoning loses its traction. What remains is mechanical operation disconnected from the objects the system is attempting to reason about, and agency dissolves.</p><div><hr></div><h2>The Capacity to Revise Commitments</h2><p>Next, we examined whether an agent must be able to change its own commitments. The system was allowed to reason and act as before, but its ability to update what it considered acceptable was removed.</p><p>At first glance, the resulting behavior appeared orderly. Over time, however, it became rigid. The system could not integrate new conflicts or adapt its normative stance. Its behavior converged toward that of a fixed policy.</p><p>This revealed another structural requirement. Agency involves authorship not only over actions, but over the commitments that guide those actions. Without the ability to revise its own norms, a system may behave consistently, yet it no longer behaves as a self-authoring agent.</p><div><hr></div><h2>Continuity Across Time</h2><p>Finally, we allowed the system to revise its commitments, but prevented those revisions from carrying forward across contexts. Within a single situation, the system behaved coherently. Across situations, that coherence evaporated. Each new context effectively erased what came before.</p><p>This demonstrated that agency is not confined to single moments. It unfolds over time. Commitments must persist if they are to be owned. Without continuity, behavior fragments into disconnected episodes, and agency disappears with it.</p><div><hr></div><h2>A Structural Picture of Agency</h2><p>Taken together, these results support a clear structural picture. Agency depends on reasons that bind actions, meanings that guide deliberation, commitments that can be revised, and continuity that carries those commitments forward. Removing any one of these supports causes the system to lose its agentic character in a way that is mechanical, repeatable, and independent of interpretation.</p><p>This conclusion is deliberately narrow. It does not claim that current AI systems possess agency. It does not claim consciousness. It does not claim that these components are sufficient to build an agent. What it provides is a set of necessary conditions. Any artificial system that genuinely qualifies as an agent will have to instantiate these structures in some form.</p><div><hr></div><h2>Why This Matters for Alignment</h2><p>This structural result has an important implication for AI alignment. Much of the alignment discussion assumes that the primary challenge is controlling powerful optimizers. From that perspective, alignment is framed as a problem of shaping behavior, managing incentives, or constraining outcomes.</p><p>What these experiments suggest is a different ordering. Alignment becomes meaningful only once genuine agency exists. Before that point, systems can be guided, restricted, or optimized, but they cannot truly endorse norms or values. Their compliance resembles management rather than alignment.</p><p>Seen this way, many problems described as misalignment arise because the system in question lacks agency altogether. The issue is not that an agent holds the wrong values, but that there is no agent capable of holding values in the first place.</p><p>Once agency exists, alignment shifts from behavioral control toward normative design. The central question becomes what commitments the agent can author, how it revises them, and how those commitments persist over time. This does not make alignment easy, but it makes it coherent.</p><div><hr></div><h2>Postscript</h2><p>We have not yet built a minimum viable reflective sovereign agent. What we have done is clarify the terrain such an agent must occupy. The essential supports of agency are no longer speculative. They have been tested by removal, and their necessity has been demonstrated.</p><p>Future work can now proceed with sharper questions: how minimal these components can be, how they interact, and how stable they remain under self-modification. Those questions were previously muddled. They are now well-posed.</p><p>That shift—from vague debate to precise structure—is meaningful progress.</p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
