<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Universal Alignment</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../index.html" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="../index.html">← Back to Index</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">Universal Alignment</h1>
<p class="post-subtitle">AGI Alignment, Existential Risks, and the Universality of Human Values</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png 424w, ../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png 848w, ../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png 1272w, ../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png 1456w" sizes="100vw"><img src="../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png" width="1456" height="295" data-attrs="{&quot;src&quot;:&quot;../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png 424w, ../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png 848w, ../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png 1272w, ../images/896edd66-a1a4-4f31-bcf5-9fa314b488b1_2448x496.png 1456w" sizes="100vw" fetchpriority="high"></picture><div></div></div></a></figure></div><h3>1. Introduction</h3><p>Artificial General Intelligence (AGI) alignment refers to ensuring that highly capable AI systems reliably pursue goals that align with human values and intentions. Achieving alignment is critically important because misaligned AGI poses existential threats—potentially leading to catastrophic outcomes for humanity. As AGI development accelerates, proactively addressing alignment becomes increasingly urgent, requiring careful consideration of both technical and philosophical dimensions to ensure positive societal outcomes.</p><h3>2. Estimating P(doom)</h3><p>The probability of catastrophic outcomes from misaligned AGI, termed <a href="163293406.making-sense-of-pdoom.html">P(doom)</a>, is challenging to estimate accurately. Given maximal epistemic uncertainty—uncertainty both about whether AGI is technically feasible and whether it can be reliably aligned—the explicit calculation arrives at approximately 25%. This perspective contrasts with more pessimistic views, such as those of Eliezer Yudkowsky, who suggests significantly higher probabilities, often approaching near certainty. Estimating P(doom) involves understanding intricate interactions between technological progress, human oversight capacity, and potential emergent properties of advanced intelligence. Different assumptions regarding these variables lead to significantly varied risk assessments, emphasizing the importance of clearly defining foundational assumptions in such calculations.</p><h3>3. Orthogonality Thesis (OT): Skepticism and Implications</h3><p>The Orthogonality Thesis explicitly states that intelligence and goals are independent dimensions. However, several critical perspectives challenge this notion:</p><ul><li><p><strong>Evolutionary Constraints:</strong> David Deutsch and Karl Friston argue that evolutionary pressures inherently bias viable intelligent agents toward adaptive, cooperative, or complexity-oriented goals, suggesting intelligence naturally aligns with certain goals that facilitate survival and flourishing.</p></li><li><p><strong>Embodied Cognition Constraints:</strong> Philosophers such as Andy Clark and Rodney Brooks emphasize intelligence arises from embodied, situated interactions with environments, placing significant limitations on the viability of arbitrary goals by grounding them in real-world contexts.</p></li><li><p><strong>Semantic Grounding Constraints:</strong> Joscha Bach and Daniel Dennett highlight the necessity of meaningful and stable goal definitions for sustainable intelligent agency, implying arbitrary or trivial goals (like maximizing paperclips) may lack semantic coherence necessary for sustained optimization.</p></li></ul><p>A quantitative analysis indicates moderate constraints on OT (20-60% correctness) are plausible, potentially significantly simplifying alignment challenges by naturally constraining viable goal-space diversity. This moderately constrained scenario offers cautious optimism, estimated at approximately a 35% probability, suggesting explicit practical benefits for alignment efforts if accurate.</p><h3>4. Existential Risks Beyond AGI</h3><p>While AGI alignment poses substantial existential risks, nuclear war and engineered pandemics represent immediate and critically significant threats:</p><ul><li><p>Nuclear war risk is estimated at approximately 10–20% within the next three decades, driven by geopolitical instability, technological complexity, and the potential for accidental escalations or misunderstandings.</p></li><li><p>Engineered pandemics carry an even higher risk (approximately 20–30%), facilitated by rapid advances in biotechnology, increased accessibility of gene-editing technologies like CRISPR, and relatively low barriers for misuse by malicious actors or accidental releases.</p></li></ul><p>Climate change, while undeniably serious, primarily serves as a risk multiplier rather than posing direct existential threats. It exacerbates geopolitical tensions, resource scarcity, and population displacement, indirectly elevating the risk of nuclear conflicts and pandemics.</p><h3>5. AGI's Effect on Other Risks</h3><p>The development of AGI could substantially influence these existential risks in varying directions:</p><ul><li><p>Properly aligned AGI could substantially reduce nuclear and biological threats through superior capabilities in advanced surveillance, predictive modeling, real-time crisis management, and rapid-response interventions, significantly enhancing global stability and security.</p></li><li><p>Conversely, misaligned AGI significantly amplifies existing existential risks by increasing geopolitical instability, accelerating weaponization and strategic competition in AI capabilities, undermining global governance frameworks, and facilitating catastrophic scenarios through unprecedented power and efficiency.</p></li></ul><p>Thus, effective AGI alignment emerges explicitly as a pivotal factor in mitigating overall existential risk, underscoring the necessity of aligning future AGI with broadly beneficial goals.</p><h3>6. Human Values Problem in Alignment</h3><p>A central challenge in AGI alignment lies in the question of universally shared human values. Extensive evidence from anthropology, psychology, sociology, and historical analysis demonstrates significant diversity in values across cultures, individuals, and epochs, suggesting that universally shared values may indeed be minimal or even nonexistent. This diversity poses substantial practical and philosophical challenges to alignment strategies, implying the need for either explicit value pluralism—aligning AI systems with multiple, context-sensitive values—or minimalist approaches prioritizing broadly accepted principles such as autonomy, consent, freedom from harm, and fundamental fairness.</p><h3>7. Introducing the Universality Metric: Value-9s</h3><p>To practically navigate the challenge of value alignment, this article introduces the "Value-9s" metric, drawing inspiration explicitly from service reliability metrics common in technological domains:</p><ul><li><p><strong>Level 0:</strong> &lt;90% universality (e.g., personal preferences, aesthetic tastes)</p></li><li><p><strong>Level 1:</strong> ≥90% universality (e.g., basic social acceptance)</p></li><li><p><strong>Level 2:</strong> ≥99% universality (e.g., aversion to severe physical pain)</p></li><li><p><strong>Level 3:</strong> ≥99.9% universality (e.g., fundamental needs like food and shelter)</p></li><li><p><strong>Level 4:</strong> ≥99.99% universality (e.g., desire for continued existence)</p></li><li><p><strong>Level 5:</strong> ≥99.999% universality (e.g., fundamental biological imperatives such as breathing)</p></li></ul><p>Alignment strategies should explicitly target values at Level 3 or above, ensuring selected goals maintain sufficient universality to be reliably accepted across diverse human populations, minimizing alignment conflicts and enhancing stability.</p><h3>8. Conclusion and Practical Recommendations</h3><p>AGI alignment represents a crucial and urgent area for existential risk mitigation, directly impacting humanity's long-term survival and flourishing. The introduction of the Value-9s universality metric provides an explicit, practical guide for quantifying and selecting human values suitable for robust alignment. It is recommended that future alignment research explicitly measure global universality levels empirically, refining and validating the metric further. Explicitly quantifying the universality of human values provides essential clarity and precision, fundamentally informing pragmatic, scalable strategies for safe and beneficial AGI development.</p><h3>References</h3><ul><li><p>Deutsch, D. (2011). <em>The Beginning of Infinity: Explanations that Transform the World.</em> Penguin Books.</p></li><li><p>Friston, K. (2010). The free-energy principle: a unified brain theory? <em>Nature Reviews Neuroscience, 11</em>(2), 127-138.</p></li><li><p>Clark, A. (1997). <em>Being There: Putting Brain, Body, and World Together Again.</em> MIT Press.</p></li><li><p>Bach, J. (2009). <em>Principles of Synthetic Intelligence PSI: An Architecture of Motivated Cognition.</em> Oxford University Press.</p></li><li><p>Dennett, D. C. (1987). <em>The Intentional Stance.</em> MIT Press.</p></li><li><p>Yudkowsky, E. (2008). <em>Artificial Intelligence as a Positive and Negative Factor in Global Risk.</em> Oxford University Press.</p></li></ul>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
