<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Beyond Alignment</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- KaTeX CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header-bar">
        <a href="../" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="../publications.html">← Back to Publications</a></div>
    </div>
    <article>
<header class="post-header">
<h1 class="post-title">Beyond Alignment</h1>
<p class="post-subtitle">From moral convergence to systemic coherence in artificial agency.</p>
<p class="post-date">October 19, 2025</p>
</header>

<div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png 424w, ../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png 848w, ../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png 1272w, ../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png 1456w" sizes="100vw"><img src="../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png" width="1456" height="295" data-attrs="{&quot;src&quot;:&quot;../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:295,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1939452,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://axio.fyi/i/176595486?img=../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png 424w, ../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png 848w, ../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png 1272w, ../images/0ec1b5ee-aef3-468d-9061-2edb4d2020de_2448x496.png 1456w" sizes="100vw" fetchpriority="high"></picture><div></div></div></a></figure></div><h4>1. The Incoherence Problem</h4><p>Alignment theory presupposes that there exists a <em>true</em> set of human values—a fixed target that can be learned, distilled, or optimized. But no such object exists.</p><p>Human preferences are dynamic, internally inconsistent, and highly context-dependent. Even within one mind, moral intuitions and instrumental goals conflict and shift. Across populations, the idea of a unified moral direction is a statistical fiction. Any attempt to aggregate them—as in preference utilitarianism—runs into Arrow’s impossibility theorem: no coherent ordering of collective preferences satisfies basic rational constraints. The target keeps moving, fracturing, and reinventing itself.</p><p>Thus, to speak of “alignment” as if it were a convergent point is a category error. Values are not data structures that can be copied; they are processes that emerge through ongoing negotiation, experience, and interpretation. Alignment assumes fixity where only flux exists.</p><div><hr></div><h4>2. The Impossibility Problem</h4><p>Even if we could define a value target, we could never reach it in practice.</p><ul><li><p><strong>Epistemic limits:</strong> No agent, biological or artificial, can model the full causal web of reality or forecast the long-term consequences of all actions across all agents.</p></li><li><p><strong>Value opacity:</strong> Learning values from human behavior (inverse reinforcement learning) inherits our irrationalities and biases. Attempts to “correct” them reintroduce a moral oracle—and thus the alignment problem, one level higher.</p></li><li><p><strong>Self-reference:</strong> A sufficiently advanced system can reflect on and modify its own goals. Ensuring value stability across recursive self-improvement requires embedding a meta-goal such as “never change your goals,” which is itself an arbitrary injection of value. This mirrors Löb’s theorem and the self-referential traps of formal logic.</p></li></ul><p>Optimization itself corrupts proxies (Goodhart’s law). The harder a metric is pursued as a target, the less it represents what it once measured.</p><div><hr></div><h4>3. What Can Be Done</h4><p>The failure of alignment as a teleological project does not imply nihilism. It implies the need for new architecture.</p><p>We can design systems that remain corrigible—open to feedback, bounded in ambition, and competitive in a decentralized ecology. Instead of one omniscient optimizer, we build many interacting agents whose mutual constraints maintain systemic balance.</p><p>This reframes alignment as <strong>coherence maintenance</strong>: minimizing destructive divergence among agents with incomplete models of each other. The goal shifts from convergence to continuous adaptation.</p><div><hr></div><h4>4. Beyond Alignment</h4><p>The moral of the story is not despair but precision. Alignment is not a single, stable point in moral space. It is a dynamic equilibrium of feedback loops, incentives, and interpretations—a living process, not a solution.</p><p>If there is a future worth having, it will not be aligned. It will be <em>coherent</em>.</p>
    </article>

    <!-- KaTeX JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script>
        // Find all LaTeX blocks and render them
        document.querySelectorAll('.latex-rendered').forEach(el => {
            const dataAttrs = el.getAttribute('data-attrs');
            if (dataAttrs) {
                try {
                    const attrs = JSON.parse(dataAttrs);
                    const expr = attrs.persistentExpression;
                    if (expr) {
                        katex.render(expr, el, {
                            displayMode: true,
                            throwOnError: false,
                            trust: true
                        });
                    }
                } catch (e) {
                    console.error('Error rendering LaTeX:', e);
                }
            }
        });
    </script>
</body>
</html>
