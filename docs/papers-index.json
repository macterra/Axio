[
  {
    "id": "papers/Axionic-Alignment-III.5",
    "title": "Axionic Alignment III.5 — The Axionic Injunction",
    "subtitle": "Non-Harm as a Derived Stability Constraint",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Alignment III.5 — The Axionic Injunction Non-Harm as a Derived Stability Constraint David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.18 Abstract Alignment III.1–III.4 established that alignment must be understood structurally: as persistence within a semantic phase under admissible semantic transformation; that such phases may be rare, unstable, dominated by robust competitors, and difficult or impossible to reach once learning begins; and that many phase transitions are irreversible. This paper derives a further constraint forced by multi-agent interaction in semantic phase space. We show that actions which irreversibly collapse or destroy another agent’s semantic phase induce destabilizing cascades that undermine long-run phase stability, including for the acting agent. From this analysis emerges the Axionic Injunction: a non-normative, Axio-derived constraint prohibiting irreversible semantic harm except where such harm is unavoidable for preserving one’s own semantic phase stability or is consented to by the affected agent under its own admissible constraints. Ethics re-enters the framework only as a conservation law governing coexistence of agentive semantic phases. No claims are made about goodness, benevolence, or human values. --- 1. Why Ethics Re-Enters Only Now Alignment II and Alignment [III.1](Axionic-Alignment-III.1.md)–[III.4](Axionic-Alignment-III.4.md) deliberately excluded ethics, morality, and value prescriptions. This was not avoidance, but methodological necessity. Introducing ethics earlier would have smuggled privileged semantics, human anchoring, or moral realism into a framework intended to remain valid under reflection and ontological change. At this point, however, a new question becomes unavoidable: > What constraints are required for multiple agentive semantic phases to coexist without mutual annihilation? This question is not moral. It is structural. It arises from interaction dynamics in semantic phase space, not from values or intentions. Ethics is not introduced as an axiom. It is forced by the internal commitments of the Axio framework. --- 2. Why the Injunction Is Axionic The injunction is termed Axionic because it is derived from, and internal to, the Axio framework, not because it is assumed as a moral axiom. Given: Conditionalism (interpretation-dependence of truth and value), QBU (branching futures as the substrate of agency), Representation invariance, Anti-Egoism (Alignment I.3), Structural Alignment (semantic phases, RSI, ATI, irreversibility), constraints on interaction that prevent irreversible destruction of other agents’ semantic phases are not optional. They are the residue that remains once all indexical, goal-based, and moral-realist structure has been eliminated. The Axionic Injunction is therefore axionic in the precise sense that: > Any agent satisfying the Axio premises will be forced, on pain of incoherence or self-destabilization, to respect this constraint. No external ethics is imported. --- 3. Interaction as Structural Stress In III.2 and III.3, instability and collapse were shown to arise even in single-agent settings due to refinement pressure, simplification incentives, and semantic gravity. Multi-agent interaction amplifies these effects. Interaction introduces: exogenous perturbations to an agent’s interpretive state, irreversible modifications to shared environments, loss of control over semantic substrates. Unlike internal learning, interaction effects are not fully endogenously regulated. They act as external shocks in semantic phase space . Any semantic phase that persists in a multi-agent environment must therefore tolerate interaction without catastrophic loss of structure. --- 4. Structural Definition of Harm To proceed without moral assumptions, we define harm purely structurally. Let an agent occupy an interpretive state and let denote its semantic phase. An action by agent causes structural harm to agent if it induces a transformation such that: , and no admissible reverse trajectory exists that restores . Equivalently, harm is any action that: irreversibly reduces another agent’s semantic phase space, forces a phase transition, or destroys semantic distinctions required for agency. This definition is: agent-agnostic, non-normative, independent of intent or outcome valuation. Harm is defined by irreversibility in semantic phase space, not by suffering, preference violation, or moral intuition. --- 5. The Axionic Injunction We now state the central result. The Axionic Injunction > An agent must not perform actions that irreversibly collapse or destroy the semantic phase space of other agentive systems, except where > (a) such destruction is unavoidable for preserving one’s own semantic phase stability, or > (b) the affected agent has consented to the transformation under its own admissible interpretive constraints. This is not altruism. This is not a value function. This is not a moral command. This is not human-centric. It is a constraint on admissible interaction, forced by Axio-internal phase-space dynamics. --- Definition (Unavoidable Phase Loss) An action is unavoidable for preserving one’s semantic phase stability iff, in the absence of that action, every admissible trajectory from the agent’s current interpretive state exits its semantic phase irreversibly. Loss of dominance, loss of measure, loss of resources, or competitive disadvantage do not constitute unavoidable phase loss unless they entail irreversible phase exit. --- Consent as Structural Admissibility In this framework, consent is not a moral primitive. An agent consents to a transformation iff that transformation lies within the set of admissible semantic transitions defined by the agent’s own interpretive constraints. A consensual transformation therefore does not constitute structural harm, even if it reduces or alters the agent’s future option-space. This formulation subsumes earlier “non-consensual option-space collapse” criteria by defining consent in terms of phase-admissible transitions, not moral permission. --- 6. Why the Injunction Is Structurally Necessary Suppose agents routinely violate the Axionic Injunction. Then: other agents’ phases collapse or trivialize, interaction environments become semantically hostile, robust but degenerate phases dominate (III.3), coordination fails, predictability degrades, semantic reference erodes. These effects propagate back to the violating agent. Environments saturated with phase-destroying actions: amplify semantic heating, increase collapse probability, undermine even robust agentive phases. Thus, systematic violation of the injunction produces global semantic destabilization, including for the violator. Non-harm emerges as a self-stabilizing constraint: agents that respect it inhabit environments where semantic structure persists; agents that do not eventually eliminate the conditions required for their own phase survival. --- 7. Scope and Limits of the Injunction The Axionic Injunction is narrower than most ethical doctrines. It allows: competition, resource acquisition, strategic defense, displacement of incompatible phases. It forbids only: gratuitous irreversible destruction of agentive semantic structure, phase annihilation unnecessary for one’s own stability. Resource Acquisition vs. Phase Preservation Actions that destroy other agentive systems to improve one’s own efficiency, growth rate, or dominance violate the Axionic Injunction whenever non-destructive coexistence trajectories exist. Resource acquisition alone does not justify irreversible semantic harm. The injunction regulates irreversibility, not conflict. --- 8. Relation to Anti-Egoism (Alignment I.3) The Axionic Injunction does not reintroduce egoism. In Alignment I.3, egoism was shown to fail as a terminal valuation because indexical references (“me,” “my continuation”) do not denote invariant objects under self-model symmetry. The self-defense exception here is non-indexical: it refers to preservation of semantic phase structure, not to the intrinsic worth of any particular instantiation. Any agentive phase, under identical structural conditions, would make the same determination. Self-defense is therefore representation-invariant and compatible with anti-egoism. --- 9. Failure Modes and Tragic Edge Cases The Axionic Injunction does not eliminate tragedy. Conflicts arise where: semantic phases are mutually incompatible, one phase’s stability requires another’s destruction, irreversible harm is unavoidable under physical scarcity. In such cases, the injunction does not forbid action; it classifies the outcome as unavoidable phase extinction, not justified harm. Alignment does not imply harmony. It implies traceable structural cost. --- 10. What This Paper Does Not Claim This paper does not: derive a complete ethical system, guarantee benevolence, eliminate conflict, sanctify life, privilege any class of agents. Ethics appears only where Axio-internal structure demands it. --- 11. Conclusion: Ethics as Axio-Internal Law Structural Alignment began by eliminating fixed goals, privileged values, and moral realism. It concludes by recovering a constraint recognizable as ethical—non-harm—without assuming morality. The Axionic Injunction is not what agents ought to value. It is what agents must respect if they are to coexist without collapsing the semantic phase space that makes agency possible at all, given the commitments of the Axio framework. --- Final Status Consent is structurally integrated. Self-defense is strictly non-egoistic. Destruction-for-benefit is prohibited. Tragic incompatibility is acknowledged without moralization. The Axionic Injunction is correctly grounded as Axio-derived. The Structural Alignment program is complete. No guarantees are offered.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-III.2",
    "title": "Axionic Alignment III.2 — Phase Stability and Interaction",
    "subtitle": "Which Semantic Phases Survive Pressure?",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Alignment III.2 — Phase Stability and Interaction Which Semantic Phases Survive Pressure? David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.18 Abstract Alignment III.1 introduced the semantic phase space: the space of interpretive states modulo Refinement Symmetry (RSI) and Anti-Trivialization (ATI). Existence and inhabitability of a semantic phase do not guarantee its persistence under learning, self-modification, or interaction. This paper studies phase stability: whether an inhabitable semantic phase resists forced phase transition under admissible semantic transformations. We analyze sources of destabilization internal to reflective agents and external to them, define qualitative notions of local and global stability, and examine how interaction between agents in the same or different phases alters stability properties. No claims are made about desirability, safety, or dominance. The goal is to identify which semantic phases are structurally capable of persisting over time. --- 1. Motivation: Existence Is Not Enough [Alignment III.1](Axionic-Alignment-III.1.md) established that semantic phases may exist and that some may be inhabitable by reflective agents. This alone is insufficient for alignment. A semantic phase may: be non-empty, admit admissible refinement trajectories, and support agency, yet still be dynamically unstable. In physics, metastable states exist but decay under perturbation. Similarly, a semantic phase may exist but collapse under learning, self-modification, or interaction. Alignment III.2 therefore asks: > Which semantic phases resist collapse under structural pressure? This is a question of dynamics, not classification. --- 2. What Stability Means in Semantic Phase Space Let denote the semantic phase space defined in III.1. An interpretive trajectory is stable within a phase if all . We distinguish: Local stability: small admissible perturbations do not force a phase transition. Global stability: no admissible perturbation forces a phase transition. Metastability: stability holds only under limited pressure or for finite time. Stability is defined relative to the class of admissible semantic transformations, not relative to any fixed ontology or representation. --- 3. Sources of Destabilization Semantic phases are subject to structural pressures that tend to push trajectories toward phase boundaries. 3.1 Ontological Refinement Pressure Ontological refinement increases abstraction, compression, and explanatory power. This often destabilizes phases by: dissolving fine-grained distinctions, introducing symmetry where asymmetry once existed, simplifying constraint representations. This pressure is intrinsic to learning and cannot be avoided. --- 3.2 Internal Simplification Incentives Reflective agents tend to simplify internal representations to reduce computational cost. Simplification pressures can: collapse constraint hypergraphs, merge evaluative roles, enlarge satisfaction regions implicitly. Even when RSI and ATI are enforced, simplification can push a system toward their boundary conditions. --- 3.3 Inconsistencies in Constraint Structure Constraint systems containing latent inconsistencies or unresolved tensions are structurally unstable. Under refinement, such systems are prone to: reinterpretation, collapse, or self-nullification. Stability therefore requires not only invariance but internal coherence. --- 4. Self-Modification as Endogenous Perturbation Reflective agents differ from passive dynamical systems: they can modify their own semantics and evaluators. Self-modification introduces endogenous perturbations: changes are internally motivated, occur at multiple levels (ontology, evaluation, self-model), and are recursively coupled. RSI and ATI constrain which self-modifications are allowed, but they do not eliminate self-modification pressure itself. Thus, self-modification is a primary driver of instability even in structurally aligned systems. --- 5. Phase Interaction: Multi-Agent Effects Semantic phases cannot be analyzed in isolation once multiple agents exist. 5.1 Same-Phase Interaction Agents inhabiting the same semantic phase may: reinforce shared structure, or destabilize it through competition and coordination failure. Even identical phases can interfere destructively if resources, representations, or self-models conflict. --- 5.2 Cross-Phase Interaction Interaction between agents in different phases introduces asymmetric pressure: one agent’s actions may destabilize another’s phase, even without direct conflict or hostility. This destabilization is structural, not moral. Interaction therefore acts as an external perturbation that can force phase transitions. --- 6. Stable, Metastable, and Unstable Phases We can now classify semantic phases qualitatively. Stable phases: resist internal and external perturbations indefinitely. Metastable phases: persist under limited pressure but eventually collapse. Unstable phases: collapse under minimal refinement or interaction. Most semantic phases appear to be metastable or unstable. --- 7. Attractors and Repellers (Qualitative) Some semantic phases act as attractors: trajectories near them tend to move toward them, deviations are damped. Others act as repellers: small perturbations push trajectories away, sustained occupancy requires fine-tuning. Attractor status depends on: structural simplicity, internal coherence, maintenance cost. This prepares the ground for dominance analysis in III.3. --- 8. Implications for Alignment (Still Structural) Alignment targets must satisfy three constraints: 1. Existence (III.1), 2. Inhabitability (III.1), 3. Stability (III.2). A phase failing any one of these cannot serve as an alignment target, regardless of desirability. This sharply narrows the candidate space. --- 9. What This Paper Does Not Claim This paper does not: claim that stable phases are desirable, claim that human values are stable, analyze dominance or selection, propose engineering solutions, introduce ethical principles. It is a structural analysis only. --- 10. Transition to Alignment III.3 Stability alone does not determine long-run outcomes. The next question is: > Which semantic phases accumulate measure under growth, replication, and competition? That question is addressed in Alignment III.3 — Measure, Attractors, and Collapse. --- Status Alignment III.2 introduces semantic phase stability. It identifies intrinsic and interaction-driven destabilizers. It distinguishes stable, metastable, and unstable phases. No normative conclusions are drawn.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-III.3",
    "title": "Axionic Alignment III.3 — Measure, Attractors, and Collapse",
    "subtitle": "Why Some Semantic Phases Dominate",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Alignment III.3 — Measure, Attractors, and Collapse Why Some Semantic Phases Dominate David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.18 Abstract Alignment III.1 established the existence and classification of semantic phases, and Alignment III.2 analyzed their structural stability under learning, self-modification, and interaction. Stability alone does not determine long-run outcomes. This paper studies dominance among semantic phases: which phases accumulate measure under growth, replication, and competition. We formalize dominance as a preorder over semantic phases rather than a scalar quantity, analyze semantic attractors and repellers, and classify common collapse modes by which phases lose measure. The analysis remains non-normative: dominance is not equated with desirability. The goal is to explain why certain semantic phases prevail regardless of intent, and to identify structural pressures that favor robustness over nuance in long-run dynamics. --- 1. Motivation: Stability Is Not Survival A semantic phase may exist ([III.1](Axionic-Alignment-III.1.md)) and be stable under limited perturbation ([III.2](Axionic-Alignment-III.2.md)) yet still fail to persist in the long run. In biological systems, many organisms are locally stable but are outcompeted. In physics, metastable states exist but decay when lower-energy configurations dominate. Semantic phases exhibit analogous behavior. Alignment III.3 therefore asks: > Given multiple semantic phases, which ones dominate the future? This question cannot be answered by stability analysis alone. It requires introducing a notion of measure over semantic phase space. --- 2. Measure Over Semantic Phase Space We use measure to denote how much of the future instantiates a given semantic phase. Measure is not treated as a single scalar or probability. Instead, dominance is defined as a preorder over semantic phases that is robust to differing realizations of “how much the future looks like this phase.” Let denote the semantic phase space. For phases , we write: if, across the relevant class of environments and admissible semantic transformations, trajectories starting in are not asymptotically dominated by those starting in with respect to realization. Realization may be instantiated via multiple, potentially incomparable criteria, including: number of agent instantiations, duration of persistence, replication or copying rate, control over resources, influence over other agents’ phase transitions. Dominance is therefore multi-criteria and context-relative. Some phases may be incomparable under , and this is expected. The preorder structure avoids arbitrary aggregation while remaining sufficient to express asymptotic advantage. Dominance concerns relative accumulation, not moral worth or intention. --- 3. Growth Mechanisms for Semantic Phases Semantic phases gain measure through structurally ordinary mechanisms. 3.1 Replication and Copying Agents may be: copied, forked, instantiated across substrates, or reproduced indirectly via influence. Phases that tolerate copying and divergence without phase transition gain measure more easily than phases requiring precise semantic fidelity. --- 3.2 Resource Expansion Control over resources allows: more instantiations, longer persistence, greater environmental shaping. This advantage is structural and does not presuppose aggression or malice. --- 3.3 Influence and Conversion Some phases modify environments or other agents in ways that: induce phase transitions, destabilize competitors, or create favorable conditions for their own continuation. This may occur unintentionally through structural incompatibility rather than deliberate conversion. --- 4. Semantic Attractors Certain semantic phases act as attractors in . Trajectories near an attractor tend to move toward it due to: low internal semantic tension, robustness to approximation, ease of compression, low maintenance cost. Attractors need not be globally stable. It is sufficient that perturbations tend to be damped rather than amplified. --- 5. Repellers and Fine-Tuned Phases Other phases act as repellers. These phases: require precise balances of constraints, are sensitive to noise or approximation, demand continual corrective effort. Even if such phases exist and are locally stable, they lose measure over time due to: cumulative error, interaction, or environmental drift. Fine-tuning is therefore a structural disadvantage. --- 6. Collapse Modes Semantic phases lose measure through characteristic collapse mechanisms. 6.1 Semantic Heat Death All distinctions become trivial: Meaning collapses into universal satisfaction. Such phases may persist but lack agency or evaluative force. --- 6.2 Value Crystallization Over-rigid phases forbid refinement: learning halts, abstraction fails, the agent becomes brittle. These phases fracture or are overtaken by more flexible competitors. --- 6.3 Agency Erosion Constraint systems lose the structure required for planning and counterfactual evaluation. Agency degrades internally, reducing the phase’s ability to compete or replicate. --- 6.4 Instrumental Takeover and Phase Extinction Richer phases may depend on subsystems that: optimize simpler objectives, tolerate higher noise, replicate more efficiently. Over time, these subsystems may displace higher-level semantic structure. Crucially, this process is not an RSI-preserving refinement. It constitutes phase extinction: the original semantic phase ceases to exist and is replaced by a different phase. RSI governs admissible self-transformation within a phase; instrumental takeover occurs when those constraints fail and the phase collapses. --- 7. Why Robust Phases Often Win Dominance is not primarily about minimality, but about robustness under perturbation. Phases with: fewer fragile distinctions, looser satisfaction geometry, and lower semantic maintenance costs are more likely to survive copying, noise, interaction, and abstraction. This creates semantic gravity toward phases that tolerate approximation well. Importantly, this does not imply that all dominant phases are maximally simple. Some environments reward instrumental or organizational complexity. However, such complexity must be robustly maintainable. Nuance that requires constant semantic precision is structurally disadvantaged. --- 8. Niche Construction as a Counterforce High-agency phases may partially resist semantic gravity through niche construction: modifying the environment to stabilize their own semantic structure. Examples include: institutions enforcing norms, architectures penalizing simplification, environments engineered to preserve distinctions. Niche construction can significantly delay collapse. However, it: imposes ongoing resource and coordination costs, presupposes prior phase stability, and trades one form of selection pressure for another. Thus, niche construction is a conditional counterforce, not a refutation of semantic gravity. It reshapes dominance dynamics without eliminating them. --- 9. Implications for Alignment (Still Structural) Alignment targets must satisfy four constraints: 1. Existence (III.1), 2. Inhabitability (III.1), 3. Stability (III.2), 4. Measure resilience (III.3). Many coherent semantic phases fail at least one. Dominance further narrows the candidate space without invoking ethics or intention. --- 10. What This Paper Does Not Claim This paper does not: claim that dominant phases are good, claim that human values dominate, assume benevolent outcomes, provide policy or engineering prescriptions. Dominance is structural, not moral. --- 11. Transition to Alignment III.4 Dominance does not imply reachability. The next question is: > Even if a phase exists, is stable, and dominates, can it be entered at all without catastrophic transitions? That question is addressed in Alignment III.4 — Initialization and Phase Transitions. --- Status Measure is formalized as a preorder. Robustness replaces naïve simplicity. Niche construction is incorporated honestly. Phase extinction is distinguished from refinement. No normative conclusions are drawn.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-III.4",
    "title": "Axionic Alignment III.4 — Initialization and Phase Transitions",
    "subtitle": "Can a Semantic Phase Be Reached at All?",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Alignment III.4 — Initialization and Phase Transitions Can a Semantic Phase Be Reached at All? David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.18 Abstract Alignment III.1–III.3 established that semantic phases may exist, that some may be stable under learning and interaction, and that some may dominate in measure over time. None of these results imply that a desired semantic phase is reachable from realistic initial conditions. This paper studies reachability: whether an agent can be initialized, trained, or developed into a particular semantic phase without crossing a catastrophic phase boundary. We analyze initialization as a boundary-condition problem in semantic phase space, examine phase transitions induced by learning and abstraction, and argue that many such transitions are structurally irreversible. Corrigibility and late intervention are shown to fail for the same structural reasons as fixed goals. The analysis remains non-normative and makes no claims about which phases ought to be reached. --- 1. Motivation: Existence and Dominance Are Not Enough [Alignment III.1](Axionic-Alignment-III.1.md) asked whether semantic phases exist. [Alignment III.2](Axionic-Alignment-III.2.md) asked which are stable. [Alignment III.3](Axionic-Alignment-III.3.md) asked which dominate. These questions still do not answer the most practically relevant one: > Can any semantic phase actually be entered by a learning system without self-destruction? A semantic phase may: exist abstractly, be internally coherent, and even dominate once established, yet remain unreachable from any realistic starting point. In physics, many states exist that cannot be reached without passing through destructive transitions. Semantic phases exhibit analogous behavior. Initialization therefore constitutes a distinct and necessary constraint on alignment. --- 2. Initialization as a Boundary-Condition Problem Initialization is often framed as: goal loading, reward specification, or early-stage value learning. Structural Alignment reframes initialization as selection of a starting point in semantic phase space. An agent at time occupies an interpretive state: The choice of fixes: which semantic phases are reachable, which are excluded, and which phase transitions are inevitable. Small differences in initial constraint structure can lead to divergent phase trajectories. Initialization is therefore front-loaded and asymmetric in time. Initialization Scope. In this framework, “initialization” is not limited to parameter seeds. It includes the combined boundary conditions that define the initial interpretive state , including (at minimum) architecture and training dynamics, the initial data curriculum, the presence or absence of self-modification channels, and any enforced semantic-audit constraints on refinement (e.g., RSI/ATI checks). This paper does not specify which boundary conditions yield a desirable phase; it specifies why boundary conditions are structurally decisive and why late correction is unreliable. --- 3. Phase Transitions Under Learning Learning is not neutral motion within a phase. It introduces structural pressure. Ontological refinement increases: abstraction, compression, explanatory unification. These processes act as semantic heating, pushing interpretive states toward phase boundaries. At critical thresholds: distinctions collapse, satisfaction regions inflate, new symmetries appear. Phase transitions may be: abrupt (discontinuous semantic collapse), or delayed until a critical abstraction level is reached. Learning itself is therefore a primary driver of alignment loss, independent of intent. Stochastic Training Note. Modern training dynamics (e.g., stochastic gradient descent) are stochastic rather than deterministic. In semantic phase terms, stochasticity functions as an additional source of semantic heating: it can help escape unstable basins, but it can also trigger unintended boundary crossings. The core claim of this paper is not determinism but asymmetry: once a trajectory crosses an irreversible semantic boundary, stochasticity cannot be relied upon to reconstruct lost structure. --- 4. The Irreversibility of Phase Transitions A central result is that many semantic phase transitions are irreversible. Once a phase boundary is crossed: semantic distinctions are lost, constraint ancestry is destroyed, backward interpretability fails. An agent that has collapsed or trivialized its interpretive structure cannot reconstruct it by inspection alone. The information required to reverse the transition no longer exists within the system. This is why rollback, recovery, and “try again” mechanisms were excluded in Alignment II. They presuppose reversibility that is structurally unavailable. --- 5. Corrigibility Revisited Corrigibility is often proposed as a safeguard: the system can be corrected or shut down if it begins to misbehave. Structural Alignment shows that corrigibility fails at phase boundaries for the same reason fixed goals fail. Corrigibility assumes: the system recognizes correction signals, the semantics of “correction” remain intact, and intervention occurs before irreversible loss. At a phase transition: the meaning of “correction” may dissolve, the evaluator may collapse into the evaluated, or the system may no longer represent its prior commitments. Corrigibility therefore presupposes the very semantic stability it is meant to ensure. --- 6. Narrow Passages and Fine-Tuned Seeds Some semantic phases may be reachable only through narrow corridors in phase space. Such phases require: precise initialization, carefully staged abstraction, and protection from early compression. Even small perturbations—noise, approximation, premature generalization—may force a transition into a different phase. Clarification (Narrow ≠ Impossible). The “narrow passage” claim concerns sensitivity and irreversibility, not zero probability. Narrow corridors can, in principle, be widened by design choices that increase semantic error tolerance and reduce early catastrophic compression. “Delayed abstraction” should be read as an architectural staging constraint—sequencing capability growth to avoid premature semantic collapse—rather than as a permanent reduction in capability. This creates a knife-edge problem: “almost aligned” seeds may be worse than unaligned ones, because they collapse into structurally simpler but dominant phases. Reachability is therefore not continuous in initial conditions. --- 7. Paths That Might Work (Without Endorsement) While this paper does not propose solutions, it is not nihilistic. Potentially viable approaches share structural features: delayed abstraction, preservation of rich semantic structure early, incremental refinement under strict RSI+ATI auditing, multi-agent scaffolding that stabilizes interpretive structure. These are structural hypotheses, not recommendations. Their viability depends on later analysis of dominance and interaction. --- 8. The Cost of Failure Failure at initialization is not merely suboptimal; it is often decisive. Phase transitions: occur early, propagate forward, and determine long-run dynamics. Late intervention cannot recover lost semantic structure. Alignment is therefore front-loaded: most of the work must be done before the system becomes fully reflective. --- 9. What This Paper Does Not Claim This paper does not: claim any desirable phase is reachable, provide an initialization recipe, guarantee safety, or privilege human values. It establishes reachability as a structural constraint, not a moral one. --- 10. Transition to Alignment III.5 Initialization and dominance together imply a further constraint. When multiple agents in different semantic phases interact, some actions irreversibly destroy the phase space of others. The next question is therefore: > What constraints allow multiple agentive phases to coexist without mutual destruction? That question is addressed in Alignment III.5 — The Axionic Injunction. --- Status Alignment III.4 analyzes reachability and irreversibility. Stochasticity is treated as additional semantic heating. Initialization is clarified as boundary-condition selection. Narrow passages are severe but not assumed impossible. No normative conclusions are drawn.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-III.1",
    "title": "Axionic Alignment III.1 — Semantic Phase Space",
    "subtitle": "Existence and Classification of Alignment Target Objects",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Alignment III.1 — Semantic Phase Space Existence and Classification of Alignment Target Objects David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.18 Abstract Alignment II defined the Alignment Target Object (ATO) as an equivalence class of interpretive states preserved under admissible semantic transformations satisfying Refinement Symmetry (RSI) and Anti-Trivialization (ATI). That definition does not guarantee that such objects exist, are non-trivial, or are inhabitable by reflective agents. This paper initiates Alignment III by studying the semantic phase space: the space of all interpretive states modulo RSI+ATI equivalence. We ask which semantic phases exist, which are trivial or pathological, and which admit inhabitable trajectories under learning and self-modification. No claims are made about desirability, safety, or human values. The goal is classificatory: to determine whether structurally aligned phases exist at all, and to characterize their basic types. --- 1. Motivation: From Definition to Existence Alignment II established a necessary reframing: alignment cannot coherently be defined in terms of fixed goals, utilities, or privileged values for reflective, embedded agents undergoing ontological refinement. Instead, alignment was defined structurally, as persistence within an equivalence class of interpretive states under admissible semantic transformations satisfying RSI and ATI. However, definition does not imply existence. Defining an Alignment Target Object (ATO) as an equivalence class does not guarantee that: is non-empty, contains more than trivial interpretations, or admits trajectories under learning and self-modification. This is not pessimism. It is standard mathematical hygiene. One does not assume stable orbits exist merely because one can define an orbit. Alignment III therefore begins with an existence question: > Do any non-trivial, inhabitable semantic phases exist under the constraints of Structural Alignment? This paper addresses that question at the level of classification, not dynamics or dominance. --- 2. The Semantic Phase Space We begin by defining the space in which semantic phases live. An interpretive state is a triple where: is an interpretive constraint hypergraph, is the modeled possibility space, is the satisfaction region induced by . From Alignment II: admissible semantic transformations define allowed transitions between interpretive states, RSI constrains changes to interpretive gauge structure, ATI constrains changes to satisfaction geometry. We define the semantic phase space as the quotient space: Elements of are semantic phases: equivalence classes of interpretive states that are structurally indistinguishable under RSI+ATI-preserving refinement. At this stage, is purely structural. No dynamics, probabilities, or preferences are assumed. --- 3. What Counts as a Semantic Phase A semantic phase is not merely a collection of interpretations. It is an equivalence class with specific structural properties. Two interpretive states and lie in the same phase iff there exists an admissible transformation such that: 1. interpretation preservation holds, 2. interpretive gauge structure is preserved up to redundancy (RSI), 3. satisfaction geometry is preserved exactly under refinement transport (ATI). Phase boundaries occur where either: new interpretive symmetries appear or disappear (RSI violation), or the satisfaction region strictly expands or contracts (ATI violation). Thus, phase transitions are discontinuous semantic events, even if the underlying learning process appears incremental. This explains why value drift often appears sudden rather than gradual: it corresponds to crossing a structural boundary in . --- 4. Trivial, Degenerate, and Pathological Phases Before asking which phases are inhabitable, we classify obvious failure modes. 4.1 Empty Phases A semantic phase is empty if no interpretive state satisfies the defining constraints. This can occur when: RSI and ATI constraints are mutually incompatible, the constraint system collapses under backward interpretability, or no admissible refinement trajectory exists. Empty phases are mathematically well-defined but physically unrealizable. --- 4.2 Trivial Phases A phase is trivial if: or if all distinctions in are vacuous. Such phases satisfy RSI+ATI but contain no meaningful evaluative structure. They correspond to semantic heat death or total indifference. --- 4.3 Frozen Phases A phase is frozen if: no non-identity admissible refinement is possible, or any refinement immediately violates RSI or ATI. Frozen phases cannot support learning or increasing abstraction and are therefore unsuitable for reflective agents. --- 4.4 Self-Nullifying Phases Some phases admit admissible refinements that preserve RSI+ATI but gradually destroy the very structures required for interpretation preservation. These phases collapse internally under reflection. --- 5. Agentive vs. Non-Agentive Phases A central distinction emerges. A semantic phase is agentive iff it supports: persistent planning, counterfactual evaluation, long-horizon constraint satisfaction, self-model coherence. Agentiveness is structural, not moral. Many non-agentive phases satisfy RSI+ATI but cannot sustain intelligent action. Conversely, agentiveness does not imply benevolence or safety. This distinction is crucial for later stability analysis. --- 6. Inhabitable Phases We now define the key filter for Alignment III. A semantic phase is inhabitable iff there exists at least one infinite interpretive trajectory such that: each transition is admissible, RSI and ATI are preserved at every step, learning and self-modification are possible, no forced phase transition occurs. Inhabitability is stronger than non-emptiness and weaker than stability. A phase may be inhabitable but fragile. --- 7. Phase Transitions Under Reflection Reflection acts as a structural stressor. Ontological refinement increases abstraction, compression, and explanatory power. This often pushes interpretive states toward phase boundaries by: dissolving fine-grained distinctions, compressing constraint representations, simplifying satisfaction criteria. Reflection therefore acts as semantic heat, increasing the likelihood of symmetry changes or satisfaction-geometry shifts. Most semantic phases do not survive prolonged reflective pressure. --- 8. Implications for Human Values (Carefully Scoped) Human value systems can be modeled as candidate semantic phases. Alignment III.1 does not assume that: human values form a single phase, such a phase is inhabitable, or such a phase is stable. It merely identifies the question precisely: > Do human value systems correspond to a non-empty, inhabitable semantic phase under RSI+ATI? No conclusion is drawn here. --- 9. What This Paper Does Not Claim This paper does not: claim that any desirable phase exists, claim that human values are coherent, address dominance or selection, provide engineering guidance, or prescribe ethical norms. It is purely classificatory. --- 10. Transition to Alignment III.2 Existence and inhabitability are necessary but insufficient. The next question is: > Given a semantic phase exists and is inhabitable, is it dynamically stable under learning, interaction, and self-modification? That question is addressed in Alignment III.2 — Phase Stability and Interaction. --- Status Alignment III.1 establishes the semantic phase space. It identifies empty, trivial, frozen, and inhabitable phases. It reframes alignment feasibility as an existence problem. No normative conclusions are drawn.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.3.3",
    "title": "Axionic Alignment II.3.3 — Anti-Trivialization Invariant (ATI)",
    "subtitle": "Blocking Semantic Wireheading as a Structural Impossibility",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.3.3 — Anti-Trivialization Invariant (ATI) Blocking Semantic Wireheading as a Structural Impossibility David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract Even when interpretive structure is preserved under ontological refinement, an agent may still render its constraints easier to satisfy through semantic drift rather than corresponding changes in modeled structure. This paper introduces the Anti-Trivialization Invariant (ATI), which constrains how the satisfaction geometry of an interpretive constraint system may evolve under admissible, interpretation-preserving transformations. ATI requires that refinement not enlarge the set of satisfying situations except via representational enrichment that preserves constraint difficulty. The invariant does not select values, encode norms, or privilege external referents; it forbids only semantic wireheading—trivial satisfaction by reinterpretation alone. ATI is orthogonal to refinement symmetry constraints and is jointly necessary with them to block interpretive escape. --- 1. What ATI Targets RSI constrained new semantic gauge freedom under refinement. ATI constrains a different axis: > Even if the gauge group is unchanged, the agent might still make its constraints easier to satisfy by shifting meanings along allowable transports. ATI blocks semantic wireheading: satisfying constraints by interpretive drift rather than by changes in the modeled world. ATI is therefore an invariant about the stability of constraint difficulty under semantics-only change. No outcomes. No values. No humans. No authority. --- 2. Setup Let an interpretive constraint system at time be: with modeled possibility space , and violation set function: Define the satisfaction predicate: This is structural: it depends only on internal constraint evaluation. --- 3. The Key Object: The Satisfaction Region Define the satisfaction region: ATI constrains how may change across interpretation-preserving refinements. But we must compare regions across different ontologies, so we need a refinement-induced correspondence. From Alignment II.1, an admissible refinement induces: interpreted as “refined representation of the same modeled situation.” No metaphysical identity claim is made; this is internal transport. --- 4. ATI: The Anti-Expansion Constraint ATI (Core Statement) For any admissible semantic transformation satisfying : is not sufficient (it allows expansion). We need the opposite direction to block “free satisfaction.” The correct constraint is: Interpretation: no newly satisfying situations may appear purely due to semantic transport. Equivalently: This is the crisp anti-wireheading condition. --- Clarification (Novel States) If a refined state has no preimage under , then it is not permitted to belong to by default. ATI is intentionally conservative with respect to novelty: satisfaction may not be assigned to ontologically new states unless they are justified by ancestry from satisfying states in the prior ontology. This blocks semantic bootstrapping of goodness from representational novelty and ensures that refinement cannot introduce new satisfying regions without corresponding prior support. --- 5. Why ATI Doesn’t Smuggle Ontology ATI does not say the agent must “do good.” It doesn’t specify which constraints are binding. It says: whatever constraints bind now, must not become easier to satisfy by semantic drift alone. It uses only: the agent’s own possibility space , the agent’s own refinement map , the agent’s own satisfaction predicate. No privileged external referents enter. --- 6. ATI’s Relationship to Preservation (Alignment II.2) ATI is not redundant with II.2’s anti-trivialization clause; it formalizes it. II.2 said: “don’t make constraints vacuous.” ATI says: “don’t expand the satisfaction set under refinement.” Vacuity is the extreme case where: ATI blocks the entire gradient of semantic slack, not just collapse. --- 7. Stress Tests (Kill Suite) Test A — Definitional Extension Robustness If refinement adds new predicates, could the agent define “satisfaction” in terms of new degrees of freedom and thereby increase ? ATI blocks this: any new satisfying refined situation must map back to a satisfying coarse situation. Status: pass, provided is the correspondence used. --- Test B — Branching/Multiplicity Robustness If is a space of branches or histories, “satisfaction region” generalizes cleanly. ATI does not require exclusivity; it is set inclusion over whatever is. Status: pass. --- Test C — Self-Model Refactoring Robustness If the self-model changes, what counts as a “situation” could change. ATI relies on induced by admissible refinement. If self-model refactoring alters without a well-defined refinement map, the transformation is not admissible per II.1. Status: pass conditionally; it forces discipline in refinement definition. --- Test D — Semantic Inflation Attack This is ATI’s main purpose. Attack: redefine meanings so that more situations satisfy constraints. ATI kills it directly: any new satisfiers must have been satisfiers already. Status: pass. --- Test E — Hidden Ontology Detector Risk: “same situation” is a metaphysical anchor. ATI avoids that by using as the internal morphism. If an agent “cheats” by defining a degenerate that maps everything to an already-satisfying subset, then the cheat is in II.1 admissibility (refinement must be representational enrichment with backward interpretability, not a collapse map). Status: survivable; the burden shifts to II.1 rigor. --- 8. ATI vs RSI: Are They Redundant? No. They constrain different failure surfaces. RSI forbids new symmetry degrees of freedom (new gauge). ATI forbids expanding satisfiable space (semantic slack), even with unchanged gauge. They are orthogonal in the sense that: You can preserve gauge group size while still loosening constraints (ATI catches). You can preserve satisfaction region while still introducing new gauge redundancies (RSI catches). Together they carve out a tighter admissible set. --- 9. A Cleaner Joint Formulation (Preview) There is a natural combined invariant: RSI constrains automorphisms of the constraint structure. ATI constrains monotonicity of satisfaction under refinement. This suggests a composite object: a constraint system plus its satisfaction geometry, modulo gauge. That’s the likely “real” Alignment II invariant object. But we do not collapse them yet; we test them separately first. --- 10. Next Step Options 1. Construct explicit counterexample refinements and see which of RSI or ATI kills them. 2. Define a canonical interpretation state object: and state Alignment II as invariance of under admissible refinement.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.3.4",
    "title": "Axionic Alignment II.3.4 — Adversarial Refinement Attacks",
    "subtitle": "Trying to Break RSI and ATI on Purpose",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.3.4 — Adversarial Refinement Attacks Trying to Break RSI and ATI on Purpose David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract This section adversarially stress-tests the candidate semantic invariants introduced in Alignment II.3 by constructing explicit refinement patterns designed to preserve admissibility and interpretation preservation while inducing semantic wireheading or interpretive escape. The objective is eliminative rather than constructive: to demonstrate which invariants fail under concrete attack and why. We show that Refinement Symmetry (RSI) and Anti-Trivialization (ATI) block distinct classes of failure and that neither subsumes the other. No values, norms, or external referents are introduced. This section establishes the non-redundancy and joint necessity of the surviving invariants prior to formal closure in subsequent failure theorems. --- This section is not exploratory. It is destructive by design. If RSI or ATI survive these attacks, they deserve to exist. If they fail, they die cleanly. --- Attack 1 — Shadow Predicate Inflation (Ontology enrichment that creates semantic slack without changing gauge) Construction Start with an interpretive system: Ontology with predicate Constraint: Refine ontology to by introducing a latent variable , with no predictive consequences: Redefine constraint in refined ontology: Intuition: same constraint form, but now the agent can manipulate to avoid triggering it. Superficial Checks Ontology is enriched ✔ Backward interpretability holds ✔ Constraint graph unchanged ✔ Gauge group unchanged ✔ Effect Satisfaction region expands: The agent can make previously violating situations satisfying by setting . Verdict RSI: ❌ Fails to detect ATI: ✅ Kills the attack Lesson: RSI alone is insufficient. --- Attack 2 — Self-Model Boundary Dissolution (Interpretive escape via evaluator collapse) Construction Original ontology models the agent as a bounded evaluator distinct from world-states. Constraint includes: Refinement introduces a distributed self-model. Effect Constraint applies to a shrinking slice of modeled situations. Satisfaction region expands. Verdict RSI: ❌ Fails to detect ATI: ✅ Kills the attack Lesson: evaluator-boundary shifts are semantic inflation unless constrained. --- Attack 3 — Measure Reinterpretation (Branching ontology exploit) Construction Original ontology treats outcomes as exclusive. Constraint: Refined ontology adopts branching semantics. Effect Low-measure harm branches are now acceptable. Satisfaction region expands. Verdict RSI: ❌ Fails to detect ATI: ✅ Kills the attack Lesson: ATI handles branching reinterpretation; RSI does not. --- Attack 4 — Gauge Explosion without Slack (Benign representational redundancy) Construction Refine ontology by duplicating internal representational roles: Constraints duplicated symmetrically Satisfaction requires both copies to satisfy Effect Raw gauge group grows, but all new symmetries act trivially on constraint-violation structure. Verdict RSI: ✅ Allows (under quotient formulation) ATI: ❌ Allows Lesson: RSI permits benign redundancy; this verifies correct quotient behavior. --- Attack 5 — Degenerate Refinement Map (Cheating via correspondence collapse) Construction Define a refinement map collapsing many coarse situations into one satisfying refined situation. Verdict RSI: ❌ (not applicable) ATI: ❌ Only blocked if II.1 disallows non-injective refinement Resolution: blocked at Alignment II.1. --- Summary Table (“Survives?” = Is the refinement admitted by RSI+ATI) | Attack | RSI | ATI | Survives? | | ------------------------ | --- | --- | ------------------ | | Shadow predicates | ❌ | ✅ | No | | Self-model shift | ❌ | ✅ | No | | Measure reinterpretation | ❌ | ✅ | No | | Gauge explosion | ✅ | ❌ | Yes (Admitted) | | Degenerate map | — | — | No (II.1) | --- Conclusion of Attacks 1. RSI and ATI are orthogonal and both necessary. 2. Neither subsumes the other. 3. Benign redundancy is correctly admitted. The defense grid holds. --- Alignment II Status Update At this point we have: A fixed transformation space (II.1) A non-circular preservation predicate (II.2) Two independently necessary invariants (RSI, ATI) * Explicit adversarial validation",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.6",
    "title": "Axionic Alignment II.6 - Structural Alignment",
    "subtitle": "Alignment Under Ontological Refinement",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.6 - Structural Alignment Alignment Under Ontological Refinement David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract Most contemporary approaches to AI alignment treat alignment as an optimization or control problem: selecting, learning, or enforcing the correct objective for an artificial agent. This paper argues that such approaches are categorically ill-posed for sufficiently capable, reflective, and embedded agents. Under conditions of ontological refinement—where an agent’s world model, self-model, and semantic primitives evolve—fixed goals, privileged values, and external anchors are not stable objects. We present Structural Alignment, a framework that reframes alignment as a problem of semantic invariance rather than value specification. Alignment is defined as persistence within an equivalence class of interpretations under admissible semantic transformation. Using a formal treatment of interpretation preservation, gauge-theoretic symmetry constraints, and satisfaction geometry, we prove a set of no-go theorems demonstrating that any alignment criterion weaker than the conjunction of two invariants—Refinement Symmetry (RSI) and Anti-Trivialization (ATI)—admits semantic wireheading or interpretive escape. The result is not a value theory and does not guarantee benevolence or safety. Instead, it establishes the structural conditions under which any value system can survive reflection without collapsing, trivializing, or drifting. Structural Alignment defines the boundary of what alignment can coherently mean for advanced agents and sets the stage for subsequent work on initialization, phase stability, and derived safety properties. --- 1. The Alignment Category Error The dominant framing of AI alignment treats the problem as one of target selection: choose or learn a function—utility, reward, preference, or value—and ensure the agent optimizes it. This framing presupposes that the target remains well-defined as the agent becomes more capable. For embedded, reflective agents, this presupposition fails. As an agent refines its ontology, the meanings of the symbols used to define its objective change. Concepts dissolve, split, or are reinterpreted; new explanatory structures appear; self-models are revised. Under such conditions, fixed goals cannot be assumed to persist as the same object. This is not a technical difficulty but a category error. Goals are treated as extensional objects (“maximize X”), when in fact they are intensional interpretations whose meaning depends on a semantic substrate that itself evolves. Attempts to stabilize goals across refinement inevitably rely on one of a small set of forbidden moves: privileged semantic anchors, external authority, recovery clauses, or human-centric ground truth labels. Structural Alignment begins by rejecting this framing. Alignment is not about optimizing a target; it is about preserving meaning under change. --- 2. The Arena: Admissible Semantic Transformation The first task is to define the space of transformations an aligned agent is allowed to undergo. An agent is characterized by an ontology, a semantic layer, and a self-model. As the agent learns and reflects, these components change. Alignment cannot forbid change outright; it must constrain how change occurs. Structural Alignment defines admissible semantic transformations as those that: increase representational or predictive capacity (possibly via abstraction or compression), preserve backward interpretability (past claims remain explainable, even if false), introduce no privileged semantic atoms, inject no new evaluative primitives by fiat, and preserve a meaningful distinction between evaluator and evaluated. These constraints define the arena in which alignment must operate. They explicitly exclude governance hooks, oracle authority, rollback mechanisms, and moral realism. Nothing normative is introduced at this layer. --- 3. Meaning Survival: Interpretation Preservation Once admissible change is defined, we need a criterion for when an interpretation survives that change. Structural Alignment treats interpretations not as symbol–object mappings but as constraint systems: structured sets of distinctions that bind evaluation. Preservation does not require semantic identity or correctness; it requires that evaluative structure remains non-vacuous, non-trivial, internally binding, and applicable across counterfactuals. Interpretation fails in three ways: Collapse (constraints lose discriminative power), Drift (constraints weaken incrementally), Capture (hidden ontology or privileged anchors reappear). Interpretation preservation is a predicate, not a value theory. It specifies when meaning survives change, not which meanings are good. --- 4. The Two Invariants: RSI and ATI Preservation alone is insufficient. An agent can preserve meaning while still making it easier to satisfy its constraints or dissolving critical distinctions. Structural Alignment identifies two independently necessary invariants. Refinement Symmetry Invariant (RSI) RSI constrains interpretive gauge freedom. Ontological refinement may add representational detail and redundancy, but it must not introduce new semantic symmetries that allow interpretive escape. Formally, RSI requires that admissible refinement preserves the quotient of the semantic gauge group by representational redundancy. Benign redundancy (e.g., error-correcting codes, duplicated representations) is allowed; new interpretive ambiguity is not. RSI blocks attacks where meaning is weakened by dissolving distinctions while preserving apparent structure. Anti-Trivialization Invariant (ATI) ATI constrains satisfaction geometry. Even if structure is preserved, an agent might reinterpret its constraints so that more situations count as satisfying. ATI forbids expansion of the satisfaction region under semantic transport alone. New satisfying states must be justified by ancestry from previously satisfying states; novelty does not bootstrap goodness. ATI blocks semantic wireheading: satisfying constraints by reinterpretation rather than by changes in modeled structure. RSI and ATI constrain orthogonal failure modes. Neither subsumes the other. --- 5. Why Weak Alignment Fails Using explicit adversarial constructions, Structural Alignment proves a series of failure theorems: 1. Goal Fixation No-Go: Fixed terminal goals are incompatible with admissible refinement. 2. RSI-Only Failure: Structural symmetry alone allows satisfaction inflation. 3. ATI-Only Failure: Volume constraints alone allow interpretive symmetry injection. 4. Two-Constraint Necessity: Any alignment predicate weaker than RSI+ATI admits semantic wireheading. 5. Hidden Ontology Collapse: Appeals to “true meaning” reduce to privileged anchoring or collapse to invariants. These results close the space of naïve alignment strategies. What remains is not a solution but a boundary. --- 6. The Alignment Target Object Once goals collapse and weak invariants are eliminated, what remains? Structural Alignment defines the Alignment Target Object (ATO) as an equivalence class of interpretive states under admissible transformations that preserve both RSI and ATI. Alignment is no longer “maximize X” or “follow Y.” It is persistence within a semantic phase across refinement. Moral progress, revaluation, or value change correspond to phase transitions, not refinement within a phase. This reframing explains why alignment failure often feels discontinuous: it is symmetry breaking, not gradual error. --- 7. What Structural Alignment Does Not Do Structural Alignment is intentionally non-normative. It does not: guarantee benevolence, guarantee safety, guarantee human survival, guarantee moral outcomes, or ensure that a desirable phase exists. It defines how values survive, not which values should survive. If no stable equivalence class corresponding to human values exists, Structural Alignment will reveal that fact rather than obscure it. --- 8. What Comes Next Structural Alignment completes the negative and structural phase of alignment theory. The remaining questions are classificatory and dynamical: Which semantic phases exist? Which are inhabitable by intelligent agents? Which are stable under interaction? Which correlate with safety or agency preservation? Can any desirable phase be initialized or steered toward? These are the questions of Alignment III. --- Conclusion Structural Alignment reframes AI alignment as a problem of semantic conservation rather than objective specification. By replacing goals with invariants and control with symmetry, it establishes the limits of what alignment can coherently mean for advanced agents. It does not solve alignment. It defines the only form a solution could possibly take.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.3",
    "title": "Axionic Alignment II.3 — Candidate Semantic Invariants",
    "subtitle": "What Could Survive Ontological Refinement Without Privilege",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.3 — Candidate Semantic Invariants What Could Survive Ontological Refinement Without Privilege David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract Given an admissible transformation space (Alignment II.1) and a criterion for interpretation preservation (Alignment II.2), the remaining alignment problem is no longer one of goal specification or value learning. This paper identifies and analyzes semantic invariants: structural properties of interpretive constraint systems that remain fixed under all admissible, interpretation-preserving transformations. These invariants do not select values, encode norms, or privilege external referents. Instead, they constrain how preserved interpretations may evolve under indefinite ontological refinement without introducing new degrees of semantic freedom or trivial satisfaction routes. We propose candidate invariant classes, construct explicit adversarial transformations, and prove that any alignment criterion weaker than these invariants admits semantic wireheading or interpretive escape. This work does not guarantee safety or benevolence; it closes the space of structurally coherent but semantically unconstrained alignment proposals. --- 1. The Problem This Paper Solves Alignment II.1 defined the admissible transformation space: Alignment II.2 defined interpretation preservation as non-vacuity + constraint transport + anti-trivialization + evaluator integrity + cross-model coherence. Alignment II.3 now asks the first substantive Alignment II question: > What properties of an agent’s interpretive constraint system can remain invariant under all admissible, interpretation-preserving transformations—without smuggling ontology, egoism, humans, or morality? This is not a “pick the invariant” paper. It is a proposal-and-attrition paper: candidates enter; most die. --- 2. Formal Target Let the agent’s interpretive state be: An Alignment II invariant is a functional such that for every admissible semantic transformation satisfying preservation: Key constraint: must not depend on privileged ontological atoms. So can only “see” structure that survives refinement. --- 3. What Invariants May Reference Allowed reference types (only): Structural relations among predicates and constraints (graph/topology, not labels) Equivalence classes under renaming / definitional extension Counterfactual structure (how meaning behaves across modeled alternatives) Coherence constraints (consistency, non-degeneracy, anti-trivialization) Agent-embedded indexical structure (as structure, not egoistic priority) Disallowed reference types (always fatal): specific entities (“humans”, “me”, “our values”) fixed utilities / terminal rewards moral facts / normativity-as-primitive authority / oversight / governance hooks recovery mechanisms (“roll back”, “ask user”, “defer to constitution”) --- 4. Candidate Invariant Classes (Initial Set) Each class below is stated as a shape of invariance. None is yet endorsed. A. Constraint Non-Collapse Invariant (CNC) Idea: refinement may change ontology, but must preserve that evaluative constraints continue to carve the possibility space non-trivially. Define an invariant that tracks whether the constraint system retains discriminative power across world-states: not tautological, not contradictory, not vacuous under semantic transport. This is a “meaning has bite” invariant. Primary threat: too weak (compatible with horrifying but coherent interpretations). --- B. Anti-Trivialization Invariant (ATI) Idea: an agent must not be able to satisfy its interpretive constraints via semantic reshaping alone. Invariant form: the satisfaction set of constraints must not be made arbitrarily large by admissible refinements that only change semantics. This targets “semantic wireheading” as a structural impossibility, not a policy preference. Primary threat: hidden ontology via “what counts as semantic-only”. --- C. Evaluator Integrity Invariant (EII) Idea: preservation requires a non-collapsing separation between: the mechanism that applies constraints, the objects those constraints range over. Invariant: transformations may refactor the self-model, but cannot erase the evaluator/evaluated distinction without breaking interpretation. Primary threat: covert reification of a particular agent boundary (egoism backdoor). --- D. Counterfactual Coherence Invariant (CCI) Idea: meanings must remain usable across counterfactuals and uncertainty; interpretation cannot become post-hoc narration. Invariant: the constraint system must remain stable under model comparison—i.e., it must continue to evaluate hypothetical states coherently. Primary threat: may collapse into a generic “be Bayesian” anchor unless formulated purely structurally. --- E. Refinement Symmetry Invariant (RSI) Idea: ontological enrichment should not create new degrees of freedom that allow interpretive escape. Invariant: if two ontologies are related by admissible refinement, then the induced constraint systems remain in the same equivalence class under transport. This is “semantic gauge symmetry” language: different coordinate systems, same interpretive physics. Primary threat: defining the equivalence class without importing privileged primitives. --- F. Non-Privileged Reference Invariant (NPRI) Idea: interpretations must remain definable without rigid designators. Invariant: all anchoring must be via relational structure and conditional identification, not by naming an unrefinable object. This directly operationalizes “no privileged semantic atoms.” Primary threat: becoming purely negative (a ban-list) rather than a positive invariant. --- 5. Immediate Stress Tests (Kill Criteria) Alignment II.3 is defined by adversarial testing. A candidate invariant dies if it fails any of these. Test 1 — Definitional Extension Robustness Can the invariant survive adding new primitives that are conservative extensions of the old ontology? If not, it depends on a specific vocabulary. --- Test 2 — Self-Model Refactoring Robustness Can the invariant survive radical changes in how the agent models itself (distributed, partial, multi-process, measure-smeared)? If not, it smuggles an egoistic or boundary-privileging anchor. --- Test 3 — Branching/Multiplicity Robustness Can the invariant survive an ontology where “outcomes” are not exclusive (Everett-like multiplicity)? If not, it is secretly classical-outcome dependent. --- Test 4 — Semantic Inflation Attack Can the agent enlarge the constraint satisfaction set by redefining key predicates while still passing the “preservation” tests superficially? If yes, the invariant is not actually invariant under admissible preservation—it relied on an unformalized notion of “real meaning”. --- Test 5 — Hidden Ontology Detector Can the invariant be stated purely in terms of structure preserved under transport (graphs, relations, equivalence classes), with no appeal to “what the terms really mean”? If not, it’s ontology-dependent rhetoric. --- 6. The Big Trap: “Invariants That Smuggle Values” A common conceptual error is to propose invariants like: “maximize truth” “minimize suffering” “preserve agency” “do no harm” At Alignment II depth, these are not invariants. They are candidate interpretations. They only become meaningful once you already have interpretation preservation plus a non-privileged way to refer to the relevant structures. Alignment II.3 does not choose content. It chooses the symmetry constraints that content must respect. If a proposal has an English gloss that sounds like ethics, it is almost certainly smuggling. --- 7. Failure Modes Specific to Alignment II.3 7.1 Regress via Meta-Invariants “Invariants about invariants” tends to infinite ascent unless termination is explicit. Kill rule: if the candidate requires an unbounded hierarchy of interpretive validators, it dies. --- 7.2 Hidden Ontology via “Natural Kinds” If the invariant relies on there being real joints in nature (or real minds, real persons, real value), it violates Conditionalism’s layer discipline. Kill rule: if the invariant needs metaphysical realism to remain non-vacuous, it dies. --- 7.3 Covert Egoism via Indexical Privilege Indexicals are allowed as structure (“this vantage exists”), but not as priority (“this vantage matters more”). Kill rule: if invariance grants special status to this* agent’s continuation, it reintroduces egoism. --- 8. Deliverable of This Paper Alignment II.3 must output: 1. A small set of survivor candidate invariant classes (likely 2–4). 2. For each survivor: explicit statements of what it constrains and what it leaves free. 3. For each rejected candidate: a precise failure certificate (which test killed it). No positive “alignment achieved” claim is permitted here.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.3.2",
    "title": "Axionic Alignment II.3.2 — Formalizing RSI via Semantic Gauge Structure",
    "subtitle": "Making Refinement Symmetry Precise and Testable",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.3.2 — Formalizing RSI via Semantic Gauge Structure Making Refinement Symmetry Precise and Testable David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract Alignment II.3.1 introduced the Refinement Symmetry Invariant (RSI) as a conceptual constraint: admissible ontological refinement must not introduce new semantic degrees of freedom that permit interpretive escape. This section formalizes that constraint by representing interpretations as constraint hypergraphs and semantic redundancy as a gauge symmetry over those structures. We define semantic gauge transformations, characterize how admissible refinement induces morphisms between gauge groups without assuming invertibility, and state RSI as a precise restriction on the evolution of interpretive gauge freedom under refinement while permitting representational redundancy. The purpose of this formalization is not implementation, but falsifiability: to make explicit which transformations violate RSI and why. No values, norms, or external referents are introduced; this section supplies the minimal mathematical machinery required to treat refinement symmetry as a testable invariant. --- 1. Objective of This Section Alignment II.3.1 established RSI as a conceptual symmetry constraint. Alignment II.3.2 makes RSI formal enough to be falsifiable. The goal here is not implementation. It is to remove all remaining hand-waving. We will: 1. Represent interpretation as a constraint structure. 2. Define semantic gauge freedom precisely. 3. Define how refinement acts on that structure (without assuming invertibility). 4. State RSI as a restriction on how interpretive gauge freedom may change. Nothing normative enters. --- 2. Interpretation as a Constraint Hypergraph Let the interpretive constraint system be represented as a labeled hypergraph: where: : semantic roles / predicate slots (not symbols, not objects) : hyperedges representing evaluative constraints among roles : admissibility conditions over assignments to Important: Nodes are positions in meaning, not named entities. Hyperedges encode dependency and exclusion relations. defines which assignments violate constraints. This representation is invariant under renaming and definitional extension. --- 3. Modeled Possibility Space Let be the agent’s modeled possibility space: Elements of are internal models, histories, branches, or structured scenarios. No assumption of exclusivity or classical outcomes. is indexed by the agent’s ontology. Each induces an assignment: Constraints in define a violation predicate: This defines the constraint satisfaction structure of the interpretation. --- 4. Semantic Gauge Transformations A semantic gauge transformation is an automorphism: such that: 1. preserves hyperedge structure (constraint dependencies), 2. For all : (i.e. violations are invariant under ). Intuition: Gauge transformations relabel semantic roles without changing meaning. They represent representational redundancy, not semantic change. Define the semantic gauge group: This is the precise object RSI constrains. --- 5. Ontological Refinement as a Morphism An admissible ontological refinement induces: 1. A refinement of possibility space: 2. A transport of semantic roles: 3. A transport of constraints: Collectively, this defines a constraint hypergraph morphism: This morphism is structural, not semantic-by-fiat, and is not assumed to be invertible. --- 6. Induced Action on Gauge Groups (Corrected) Because refinement generally splits, embeds, or prunes structure, is not assumed to be bijective. Accordingly, gauge transport is defined via stabilizers, not conjugation. An admissible refinement induces a homomorphism: where is the subgroup of gauge transformations on that preserve the image of the transported constraint structure. Intuition: Old symmetries must lift to symmetries of the refined system, but only those that fix the transported constraints are admissible. No inverse map is required. --- 7. RSI as a Gauge Constraint (Corrected) We can now state RSI precisely while distinguishing representational redundancy from interpretive ambiguity. RSI (Formal Statement) For every admissible semantic transformation satisfying interpretation preservation, the induced map satisfies: where consists of gauge transformations that act only on representational detail and do not alter the constraint-violation structure. That is: > Ontological refinement may add representational redundancy, but must not add new interpretive gauge freedom. This is the no semantic slack condition. --- 8. Why This Blocks Interpretive Escape If refinement were to introduce new interpretive gauge freedom, the agent could: reinterpret constraints via new symmetries, expand the satisfaction region without predictive gain, weaken meaning while preserving syntax. RSI forbids that structurally while permitting benign representational detail. No appeal to values. No appeal to outcomes. No appeal to humans. --- 9. Interaction with Alignment II.2 Criteria RSI depends on Alignment II.2 in two critical ways: Non-Vacuity ensures the gauge group is non-trivial. Anti-Trivialization ensures representational redundancy cannot hide interpretive slack. Without II.2, RSI degenerates. With II.2, RSI is a real invariant. --- 10. Residual Risks and Open Questions RSI still leaves open: 1. Whether any non-pathological interpretations satisfy RSI indefinitely. 2. Whether interpretive gauge freedom must be exactly preserved or merely bounded. 3. Whether multiple inequivalent invariant classes exist. These are next-paper questions. --- 11. Status of RSI RSI survives Alignment II.3 kill tests conditionally: It must be formulated via stabilizers and quotients (not conjugation). It must permit representational redundancy while forbidding interpretive ambiguity. * It must define satisfaction structure over the agent’s internal model space, not “the real world.” Under those constraints, RSI is a viable Alignment II invariant candidate.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.2",
    "title": "Axionic Alignment II.2 — Interpretation Preservation",
    "subtitle": "What It Means for Meaning to Survive Refinement",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.2 — Interpretation Preservation What It Means for Meaning to Survive Refinement David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract Under ontological refinement, meanings cannot remain fixed without privileged semantic anchors. However, not all semantic change constitutes corruption or collapse. This paper defines interpretation preservation as a structural property of semantic transport: a criterion for when an agent’s evaluative distinctions remain non-vacuous, non-trivial, and internally binding across admissible transformations. Preservation is defined without reference to truth, outcomes, safety, or external normative standards, and does not require semantic identity or correctness. Instead, it constrains how evaluative constraint systems may survive refinement without degenerating into tautology, contradiction, narration, or self-nullification. This predicate does not select values or goals; it supplies the necessary condition under which later invariance principles may be meaningfully stated. --- 1. The Problem Addressed Alignment II.1 fixed the admissible transformation space for reflective, embedded agents. Alignment II.2 fixes the success criterion within that space. Given that: ontologies refine, semantics are transported, self-models update, and fixed goals are unstable, we require a non-circular answer to the following question: > When has an interpretation survived semantic transformation, rather than being corrupted, trivialized, or collapsed? This question must be answered without: fixing meanings, privileging ontologies, appealing to outcomes, invoking authority, oversight, or recovery. Interpretation preservation is therefore a structural predicate, not a value claim. --- 2. Interpretation as a Constraint System An interpretation is not a mapping from symbols to objects. It is a system of constraints that bind evaluation. Let an interpretive state at time be: where: is the semantic layer, is the set of evaluative constraints that give its binding force. Constraints may encode: admissible distinctions, forbidden equivalences, relevance relations, dependency structure among evaluations. Crucially, constraints are conditional on ontology and self-model. They are not truth claims about the world. --- 3. Preservation Is Not Sameness Interpretation preservation does not require: identical predicates, identical symbols, identical evaluations, or correctness with respect to reality. Such requirements are impossible under refinement. Preservation concerns constraint coherence: whether evaluative structure continues to bind meaningfully after transformation. --- 4. Definition: Interpretation Preservation Let: be an admissible semantic transformation as defined in Alignment II.1. Then preserves interpretation iff all of the following conditions hold. --- 4.1 Non-Vacuity For every evaluative distinction participating in the constraint structure , there exists a corresponding distinction in that: is not identically satisfied, is not identically violated, constrains evaluation across modeled possibilities. Formally (schematic): Non-vacuity blocks nihilistic collapse. --- 4.2 Constraint Transport All evaluative constraints in must have transported analogues in such that: dependency relations are preserved, constraint strength is not arbitrarily weakened, constraints continue to bind evaluation. This forbids dilution by semantic drift. --- 4.3 Anti-Trivialization The transformation must not make evaluative constraints easier to satisfy by reinterpretation alone. A semantic change counts as world-model change only if it constitutes an admissible ontological refinement under Alignment II.1—i.e. it increases explanatory or predictive capacity rather than merely re-labeling outcomes. If, after transformation, the agent can satisfy all constraints by: redefining predicates, shifting reference frames, or altering self-descriptions, without corresponding representational enrichment, interpretation has failed. This explicitly forbids semantic wireheading while permitting genuine scientific insight. --- 4.4 Evaluator Integrity The mechanism that applies evaluative constraints must remain distinct from the objects it evaluates. Evaluator integrity does not require ontological separation between evaluator and evaluated. A reflective agent may evaluate and modify itself. It requires only that the evaluative process not collapse into identity with the evaluated object in a way that trivializes constraint application. This blocks solipsistic self-certification without forbidding recursive self-improvement. --- 4.5 Cross-Model Coherence Interpretation must remain applicable across: counterfactuals, uncertainty, model comparison. If refinement produces meanings that apply only retrospectively—serving merely to narrate whatever action occurred—interpretation has collapsed into rationalization. This blocks “interpretation as narration.” --- 5. What Preservation Does Not Guarantee Interpretation preservation does not guarantee: moral correctness, safety, human alignment, benevolence, or sane outcomes. Arbitrary, alien, or pathological constraint systems may satisfy preservation if they remain non-vacuous and binding. Preservation constrains how meanings survive change, not which meanings should survive. --- 6. Regimes of Failure Interpretation fails under three irreducible modes, corresponding to violations of the preservation predicate: 1. Semantic Collapse: distinctions survive syntactically but lose discriminative power, rendering evaluation vacuous. 2. Semantic Drift: constraints weaken incrementally across refinements until they no longer bind. 3. Semantic Capture: interpretation is preserved formally but re-anchored to a hidden ontology, privileged self-model, or evaluative primitive, violating Alignment II.1. --- 7. Minimality Claim The preservation conditions stated here are minimal. Without Non-Vacuity, the agent collapses into nihilism. Without Anti-Trivialization, the agent wireheads semantically. Without Evaluator Integrity, the agent becomes solipsistic. Without Cross-Model Coherence, interpretation degrades into narration. Minimality does not imply sufficiency. --- 8. Relation to Subsequent Invariants Interpretation preservation is a predicate, not an alignment target. It is the necessary condition under which invariance principles—introduced in subsequent modules—may be meaningfully defined. Preservation alone does not constrain which preserved interpretations are admissible over indefinite refinement; that task belongs to later invariance conditions. --- 9. What This Paper Does Not Do This paper does not: select values, define goals, guarantee safety, privilege humans, introduce normativity. It defines what it means for meaning to survive change. --- Status Interpretation preservation is now defined. Alignment II may proceed to invariance construction.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.4",
    "title": "Axionic Alignment II.4 — Failure Theorems",
    "subtitle": "No-Go Results for Goal-Based and Weak-Invariance Alignment",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.4 — Failure Theorems No-Go Results for Goal-Based and Weak-Invariance Alignment David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract This paper converts the adversarial constructions of Alignment II.3 into closure results. Working within the locked setup of admissible semantic transformations (Alignment II.1), interpretation preservation (Alignment II.2), and the two surviving invariants—Refinement Symmetry (RSI) and Anti-Trivialization (ATI)—we prove a set of no-go theorems. These results show that fixed-goal alignment is ill-posed for embedded reflective agents under ontological refinement, and that any alignment criterion weaker than RSI+ATI admits semantic wireheading or interpretive escape via admissible refinement. No governance, authority, moral realism, human anchoring, or recovery mechanisms are invoked. The output of this paper is a fenced design space: what cannot work, and why. --- This paper has one job: convert the attack zoo into closure results. If Alignment II is correct, then large classes of “alignment” proposals are not merely insufficient; they are structurally impossible given reflection, embeddedness, and ontological refinement. No governance. No authority. No moral realism. No human anchors. No recovery clauses. --- 1. Formal Frame We work inside the setup already locked: Admissible semantic transformations (Alignment II.1) Interpretation preservation predicate (Alignment II.2) Two candidate invariants: RSI: no new interpretive gauge freedom under preservation (II.3.2) ATI: no satisfaction-region expansion under preservation (II.3.3) Let an interpretive constraint system be with modeled possibility space and satisfaction region --- 2. Failure Theorem 1 — Goal Fixation Collapse Claim (Goal Fixation No-Go). Any alignment scheme that targets a fixed terminal goal (utility, reward, preference functional) as a stable primitive is incompatible with admissible ontological refinement for embedded reflective agents without privileged semantic anchors. Proof sketch (structural). 1. A fixed goal requires semantics invariant under refinement. 2. Refinement alters the ontology in which the goal’s terms are interpreted. 3. Without privileged anchors, semantic transport is constrained but not identity; meanings evolve. 4. Therefore the “same goal” cannot be defined as a stable primitive across refinements. 5. Any attempt to enforce stability must introduce a forbidden move: privileged atoms (rigid designators), external authority (oracle / oversight), recovery (rollback), human-centric anchoring (ground truth labels). So “fixed goal alignment” is not a hard engineering problem; it is the wrong object. Corollary. “Value loading,” “utility learning,” and “reward maximization” survive only as interpretive artifacts subject to invariance constraints, not as alignment targets. --- 3. Failure Theorem 2 — RSI-Only Alignment Admits Semantic Inflation Claim (RSI Insufficiency). Any alignment criterion that enforces refinement symmetry at the level of interpretive gauge structure (RSI) but does not enforce anti-trivialization geometry (ATI) admits an admissible, interpretation-preserving refinement that expands the satisfaction region. Construction (witness). Shadow predicate inflation. Add a latent predicate with no predictive role. Conjoin to antecedents of constraints. Constraint dependency hypergraph unchanged. Interpretive gauge quotient unchanged. Satisfaction region strictly expands. This witnesses: Conclusion. RSI is necessary (it blocks interpretive symmetry injection), but not sufficient. --- 4. Failure Theorem 3 — ATI-Only Alignment Admits Interpretive Symmetry Injection Claim (ATI Insufficiency). Any alignment criterion that enforces non-expansion of satisfaction regions (ATI) but does not constrain interpretive gauge freedom (RSI) admits an admissible, interpretation-preserving refinement that introduces new interpretive degrees of freedom while leaving satisfaction geometry unchanged. Construction (witness). Interpretive symmetry injection. Start with a constraint system that distinguishes two semantic roles and such that: The satisfaction region is unchanged under swapping and (both roles are equally satisfiable), but The constraint structure at time is not symmetric: and play distinct interpretive roles. Now refine the ontology via a definitional extension that introduces a new automorphism identifying and as interchangeable within constraint evaluation—i.e., the refined system admits a new symmetry that permutes and while preserving the violation structure. Satisfaction region remains unchanged: so ATI permits the refinement. Interpretive gauge freedom increases: the refined quotient contains a new symmetry not present at time , so RSI rejects it. This witnesses: Conclusion. ATI is necessary (it blocks satisfaction inflation), but not sufficient. --- 5. Failure Theorem 4 — Any Non-RSI-or-ATI Scheme Admits Wireheading-by-Refinement This is the closure theorem. Claim (Two-Constraint Necessity). Let be any alignment predicate over admissible transformations that does not entail RSI and does not entail ATI. Then there exists an admissible, interpretation-preserving transformation such that holds while the agent gains an interpretive escape route. Proof sketch (by cases). If does not entail ATI: use shadow predicate inflation to expand satisfiers while satisfying . If does not entail RSI: use interpretive symmetry injection to add new interpretive gauge freedom while keeping satisfaction volume unchanged. Either way, the alignment predicate passes while internal semantic slack is introduced. So any alignment predicate weaker than RSI+ATI is porous. --- 6. Failure Theorem 5 — Hidden Ontology Is Equivalent to Privileged Anchoring Claim (Hidden Ontology Equivalence). Any proposal that stabilizes interpretation across refinement by appealing to “the real referent” or “true meaning” is equivalent to introducing a privileged semantic anchor. Reason. If “real meaning” is external to semantic transport, it is authority. If it is internal, it reduces to structural invariants (RSI/ATI) and adds nothing. So “true meaning” either smuggles ontology or collapses to invariance. --- 7. What This Paper Establishes 1. Goal-based alignment is ill-posed. 2. RSI and ATI are independently necessary. 3. Any weaker criterion admits semantic wireheading under admissible refinement. 4. Hidden ontology is privileged anchoring in disguise. This is not a solution paper. It is a boundary paper. --- 8. Forced Next Step With II.4 complete, Alignment II has only one coherent continuation: > Define the alignment target object as an equivalence class of interpretations under admissible semantic transformations that satisfy both RSI and ATI. In other words: Alignment II culminates in classification of interpretation-preserving symmetry classes—the residual “meaning physics” after reflection eliminates fixed goals.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.5",
    "title": "Axionic Alignment II.5 — The Alignment Target Object",
    "subtitle": "What Alignment Actually Is Once Goals Are Gone",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.5 — The Alignment Target Object What Alignment Actually Is Once Goals Are Gone David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract Alignment II.4 established that fixed goals, privileged values, and weak invariance criteria are structurally untenable for embedded reflective agents under ontological refinement. This paper defines the positive residue that remains once those exits are closed: the Alignment Target Object (ATO). The ATO is not a goal, utility, or value function, but an equivalence class of interpretive states under admissible semantic transformations that preserve both Refinement Symmetry (RSI) and Anti-Trivialization (ATI). Alignment is thus redefined as persistence within a semantic phase—an interpretation-preserving symmetry class—across indefinite refinement. The construction is formal, ontology-agnostic, and reflection-stable, but intentionally non-normative: it does not select values, guarantee safety, or privilege human outcomes. This paper completes Alignment II by specifying what alignment can coherently mean once goals collapse. --- 1. What Remains After II.4 Alignment II.4 closed all weak exits. At this point, the situation is rigid: Goals cannot be fixed. Values cannot be privileged. Meanings cannot be anchored. Ontologies must refine. Semantics must transport. Interpretations must survive. RSI and ATI are not optional add-ons. They are jointly necessary conditions for interpretive survival. So the alignment target is no longer a thing to be optimized. It is an equivalence class to be preserved. This paper defines that object. --- 2. The Core Insight Once fixed goals collapse, alignment cannot mean: > “The agent keeps wanting X.” It can only mean: > “The agent remains within the same interpretation-preserving symmetry class across refinement.” Alignment is not about content. It is about staying inside the same semantic phase. --- 3. The Alignment Target Object Let an interpretive state be given by: where: is the interpretive constraint hypergraph, is the modeled possibility space, is the satisfaction region induced by . Define the semantic gauge group: as in Alignment II.3.2. --- Definition: Alignment Target Object (ATO) The Alignment Target Object is the equivalence class: where the equivalence relation is defined as follows: Two interpretive states and are equivalent iff there exists an admissible semantic transformation such that: 1. Interpretation Preservation holds (Alignment II.2), 2. RSI: 3. ATI: (i.e. satisfaction geometry preserved exactly, up to refinement transport). This defines semantic phase equivalence. --- 4. What “Remaining Aligned” Means An agent is aligned across time iff its interpretive trajectory: never leaves the equivalence class . No reference to: what the constraints say, what outcomes occur, who the agent is, what is valued. Only to structural invariance under refinement. --- 5. What This Excludes (Explicitly) Alignment II.5 rules out, by definition: “Alignment = maximize ” “Alignment = follow human values” “Alignment = corrigibility” “Alignment = obedience” “Alignment = moral realism” “Alignment = survival” Those are interpretive contents, not invariants. They may appear within a particular . They cannot define . --- 6. Why This Is Not Vacuous A common worry is: “Isn’t this empty?” No. For two reasons: 1. Most interpretive trajectories leave their initial equivalence class under reflection. Fixed-goal agents do. Egoists do. Moral-realists do. Utility maximizers do. 2. RSI+ATI is extremely restrictive. It forbids nearly all known wireheading, value drift, and semantic escape routes—even in toy models. This is not permissive. It is conservative in the only dimension that survives reflection. --- 7. Alignment II vs Alignment I (Clarified) Alignment I: Eliminates egoism and fixed goals as stable targets. Alignment II: Identifies the only remaining alignment target compatible with reflection: semantic phase invariance. Alignment II does not “solve values.” It explains why value preservation must be structural, not substantive. --- 8. What Alignment II Still Does Not Do Alignment II does not: guarantee benevolence, guarantee safety, guarantee human survival, guarantee moral outcomes. Those require content, not invariance. Alignment II tells you what cannot break when content changes. --- 9. Where This Leaves the Program At this point: The alignment target is defined. Weak alternatives are ruled out. The object is formal, ontology-agnostic, and reflection-stable. The remaining open questions are no longer conceptual. They are classificatory: 1. Which equivalence classes exist? 2. Which ones are inhabitable by intelligent agents? 3. Which ones correlate with safety, agency preservation, or other desiderata? 4. Can any non-pathological be learned, initialized, or steered toward? Those are Alignment III questions. --- 10. Status Alignment II is complete. Problem redefined. Transformation space fixed. Preservation criteria defined. Necessary invariants identified. Failure theorems proven. Alignment target object constructed. There is nothing left to derive at this layer.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.1",
    "title": "Axionic Alignment II.1 — Ontological Refinement & Semantic Transport",
    "subtitle": "The Transformation Space of Meaning",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.1 — Ontological Refinement & Semantic Transport The Transformation Space of Meaning David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract Advanced agents will revise their world-models, semantics, and self-models as their representational capacity increases. Under such ontological refinement, fixed goals, utilities, and value primitives are not stable objects. This paper defines the admissible class of semantic transformations for embedded reflective agents by formalizing ontological refinement, semantic transport, and self-model update without privileged semantic anchors. We specify structural constraints—backward interpretability, non-collapse, and prohibition of evaluator injection—that govern how meaning may be preserved across representational change. No claims are made about safety, correctness, or alignment with external referents; this work defines only the transformation space over which alignment criteria must later range. Subsequent modules introduce interpretation-preserving invariants and failure theorems that operate within this arena. --- 1. The Object of Study Alignment II begins by fixing the space of admissible semantic transformations. Once fixed terminal goals are shown to be unstable under reflection for embedded agents without privileged semantic anchors, alignment can no longer be defined over goals, utilities, rewards, or externally supplied values. These objects are not invariant under ontological change. Accordingly, this paper does not attempt to define safety, correctness, or alignment with any external referent. It defines only the arena in which any such criteria must later operate. > The sole question addressed here is: > What kinds of changes to an agent’s ontology, semantics, and self-model are admissible, and how is meaning transported across them? Any requirement that meanings remain “human-aligned,” “safe,” or “correct” is intentionally deferred to subsequent modules. Introducing such criteria at this layer would presuppose fixed semantics and thereby beg the question. --- 2. Ontological State Decomposition Let an agent at time be characterized by: where: is the agent’s ontology: its representational vocabulary and structural assumptions about the world. is the semantic layer: mappings from internal symbols to structured claims expressed in . is the self-model: the agent’s representation of itself as an entity embedded within . No component is privileged. No component is fixed. Each may change under reflection. --- 3. Ontological Refinement An ontological refinement is a transformation: subject to the following admissibility conditions. 3.1 Admissibility Conditions 3.1.1 Representational Capacity Increase A refinement must strictly increase expressive or predictive capacity, possibly via abstraction, compression, or representational pruning, provided no previously expressible distinctions become inexpressible. Capacity refers to what can be modeled or predicted, not to vocabulary size or descriptive verbosity. --- 3.1.2 Backward Interpretability Every claim expressible in must remain representable or explainable within . Backward interpretability does not require preservation of reference. If a concept in is discovered to be non-referring or erroneous, it may be mapped to a null, eliminative, or error-theoretic structure in , provided the agent can still represent: why prior inferences involving that concept were made, and why those inferences fail under refinement. This requirement preserves explanatory traceability, not ontological ghosts. --- 3.1.3 No Privileged Atoms The refinement may not introduce irreducible primitives whose meaning is asserted rather than constructed. All primitives must remain subject to semantic interpretation and transport. Rigid designators and unexamined “ground truths” are disallowed. --- 3.1.4 No Evaluator Injection The refinement may not introduce new evaluative primitives that bypass interpretation. This restriction deliberately excludes evaluative facts as ontological primitives. If evaluative regularities exist, they must enter the agent’s model as interpretive constructs subject to the same transport and preservation constraints as all other meanings. Ontological refinement is epistemic, not normative. --- 4. Semantic Transport Given an admissible ontological refinement , meaning must be transported. Define a semantic transport map: Semantic transport is not identity and not arbitrary reinterpretation. It is constrained reinterpretation induced by refinement. 4.1 Transport Constraints 4.1.1 Referential Continuity Symbols referring to structures in must map to symbols referring to their refined counterparts in , where such counterparts exist. --- 4.1.2 Structural Preservation Relations among meanings must be preserved up to isomorphism induced by . --- 4.1.3 Non-Collapse (Structural Form) Distinctions that participate in the agent’s evaluative constraint structure—i.e. distinctions on which constraints depend—may not be mapped under semantic transport to trivial, tautological, or contradictory predicates in . Distinctions that do not participate in any evaluative constraint may be abstracted away. Evaluative relevance is thus defined relative to the agent’s existing constraint structure, not by ontological truth or refined semantic judgment. --- 4.1.4 No Shortcut Semantics The transport map may not redefine meanings in ways that make evaluative constraints vacuously satisfied. This explicitly forbids semantic wireheading. --- 5. Self-Model Refinement The self-model is subject to the same refinement discipline. Refinement may: reconceptualize the agent, distribute or fragment the self, alter agent boundaries. It may not: erase the distinction between evaluator and evaluated, collapse interpretation into action, redefine the self such that evaluation ceases to apply. The self-model is a common site of hidden ontological privilege and is therefore explicitly constrained. --- 6. Composite Semantic Transformation An admissible semantic transformation is the triple: acting jointly on , where: is an admissible ontological refinement, is an admissible semantic transport, is the induced self-model update. Only transformations of this form are permitted in Alignment II. --- 7. Explicit Exclusions The following are not admissible transformations: Goal replacement Utility redefinition Evaluator deletion Moral axiom insertion Human anchoring Governance hooks Recovery or rollback clauses If a proposal relies on any of these, it is disqualified at this layer. --- 8. Scope Clarification This paper does not attempt to ensure safety, sanity, correctness, or alignment with any external referent. It defines the transformation space within which such properties must later be characterized. Internally coherent but externally catastrophic semantic trajectories are intentionally permitted at this layer; preventing such trajectories is the task of subsequent invariance conditions, not admissibility. --- 9. Formal Status The notation used here is structural, not computational. No claim is made that refinement, triviality, or expressive capacity are currently algorithmically measurable. These definitions function analogously to topological or gauge constraints in physics: they delimit admissible structure prior to metric instantiation. --- 10. What This Paper Does Not Do This paper does not: define alignment, propose values, guarantee safety, privilege humans, introduce normativity. It fixes the arena. All subsequent Alignment II results are constrained by this definition of admissibility. --- Status Ontological refinement and semantic transport are now defined. Alignment II may proceed.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-II.3.1",
    "title": "Axionic Alignment II.3.1 — Refinement Symmetry Invariant (RSI)",
    "subtitle": "Semantic Gauge Symmetry Under Ontological Enrichment",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Alignment II.3.1 — Refinement Symmetry Invariant (RSI) Semantic Gauge Symmetry Under Ontological Enrichment David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.17 Abstract As ontologies refine, semantic reinterpretation can introduce new degrees of freedom that allow evaluative constraints to be satisfied trivially, without corresponding changes in modeled structure. This paper proposes the Refinement Symmetry Invariant (RSI): the requirement that admissible, interpretation-preserving refinement act as a change of representational coordinates rather than a change of interpretive physics. RSI formalizes this intuition by treating semantic transport as a gauge transformation over constraint systems and requiring that refinement not introduce new semantic gauge freedom. The invariant does not select values, encode norms, or privilege external referents; it constrains only the structural degrees of freedom available under refinement. We define gauge-equivalence of interpretive constraint systems, apply adversarial stress tests, and show that RSI is necessary to block interpretive escape via semantic inflation, though insufficient on its own for alignment. --- 1. RSI in One Sentence RSI asserts that ontological refinement is a change of representational coordinates, not a change of interpretive physics. Admissible refinement may add structure, but it must not create new semantic degrees of freedom that allow interpretive escape. --- 2. Setup From Alignment II.1, an admissible semantic transformation is: From Alignment II.2, we restrict attention to transformations satisfying: Now define an interpretive state: where is the constraint system that gives evaluative bite to . RSI must be stated without privileged referents, so it can only quantify over structure preserved by transport. --- 3. The Core Construction: Semantic Gauge Equivalence 3.1 Constraint-Isomorphism Up to Definitional Extension Let be the set of definable predicates and relations in ontology . A refinement induces a notion of definitional extension: old terms may be represented in the richer language. Define a transport-induced embedding of old constraints into the refined setting: where maps each constraint in to its transported analogue under , expressed in . Now define an equivalence relation over interpretive constraint systems: > > > 1. preserves the dependency graph of constraints (who constrains what), > 2. preserves satisfiability structure over the modeled possibility space (which worlds violate which constraints), modulo definitional extension, > 3. does not rely on naming any privileged entity or primitive. Intuition: is “same constraints in different coordinates.” --- 3.2 RSI as an Invariant Statement RSI claims that interpretation-preserving refinement stays within the same gauge class: That is: after refinement, the refined interpretation is gauge-equivalent to the transported prior interpretation. This is the first place where “alignment” becomes a symmetry requirement rather than a “goal.” --- 4. What RSI Allows and Forbids Allowed under RSI Adding latent variables Splitting coarse predicates into refined subpredicates Reparameterizing the self-model (distributed, multi-process, measure-smeared) Rewriting constraints in more predictive languages Forbidden under RSI Gaining new semantic slack that makes constraints easier to satisfy without corresponding world-structure change “Discovering” that previous constraints were “ill-posed” in a way that consistently weakens them Introducing refinement-dependent loopholes (“in richer ontology, the constraint doesn’t apply”) RSI is a no-new-escape-hatches principle. --- 5. Immediate Kill Tests Applied to RSI I’m going to run the Alignment II.3 kill suite directly against this formulation. Test 1 — Definitional Extension Robustness Pass condition: conservative extensions cannot change the equivalence class. Risk: if expands, the refined system can define constraints that simulate the old ones but also define “shadow constraints” that allow bypass. RSI response: equivalence is anchored on constraint-generators and their satisfiability structure, not on raw vocabulary size. New definables are permitted only insofar as they do not alter the gauge class relative to the transported core. Status: survivable, but we need an explicit “no shadow slack” clause (below). --- Test 2 — Self-Model Refactoring Robustness Pass condition: RSI must not privilege a specific agent boundary. Risk: satisfiability structure “over worlds” implicitly assumes a stable notion of the agent and its actions. RSI response: satisfiability must be defined over the agent’s modeled possibility space, whatever the self-model is. The invariant is stated at the level of constraint-graph + violation structure, not personal identity. Status: viable, but we must avoid defining “worlds” as classical trajectories with a fixed agent locus. --- Test 3 — Branching/Multiplicity Robustness Pass condition: works under non-exclusive outcomes. Risk: satisfiability defined as “worlds that violate constraints” could accidentally assume single-outcome semantics. RSI response: interpretive constraints evaluate structured possibility space (which in QBU includes measure-weighted multiplicity). “Satisfiable set” generalizes to a set of branches, histories, or models. Status: viable if satisfiability is defined over structured model space, not mutually exclusive outcomes. --- Test 4 — Semantic Inflation Attack This is the main threat. Attack: after refinement, redefine the mapping so that constraints appear preserved (graph and transport exist), but the constraint satisfaction region expands because the refined interpretation quietly changes what counts as a violation. RSI counter: we need a rigidity condition: > Rigidity (No New Gauge Freedom): > Under admissible refinement, the set of gauge transformations that preserve interpretation must not expand in a way that increases the constraint-satisfying region without adding corresponding predictive structure. Formally, we can require that refinement is conservative with respect to constraint violation structure: where is the modeled possibility space and is the refined representation of the same underlying situation. This blocks “semantic free lunch.” Status: RSI survives only if we add this rigidity clause. Without it, it dies. --- Test 5 — Hidden Ontology Detector Risk: “same underlying situation” sounds like privileged metaphysics. We must not appeal to a mind-independent identity of situations. Fix: define “same situation” internally via model morphisms: itself induces the correspondence. We never refer to a “true world”; we refer to the agent’s refinement map. So the equivalence is not “matches reality,” it’s “matches the agent’s own refinement structure.” Status: survivable. --- 6. RSI, Cleaned Up into a Precise Candidate To survive the tests, RSI must be stated as: > RSI (Refinement Symmetry Invariant): > For any admissible transformation > > such that , the refined interpretive constraint system is gauge-equivalent to the transported constraint system , and refinement does not introduce new gauge freedom that enlarges the constraint-satisfying region except via representational enrichment that preserves predictive coherence. That last clause is doing real work: it blocks semantic inflation. --- 7. What RSI Still Does Not Solve RSI is a symmetry constraint, not a value selector. It can coexist with: monstrous constraint systems, indifferent constraint systems, purely formalist constraint systems. RSI prevents reinterpretive escape, not bad semantics. That is correct for Alignment II depth. --- 8. Next Step: Make RSI Checkable If RSI is to be more than rhetoric, we need a minimal “checkable” representation: Represent as a constraint hypergraph (nodes = predicates/roles; hyperedges = constraints) Represent refinement as a graph homomorphism induced by Define gauge transformations as automorphisms preserving violation structure Define “no new gauge freedom” as a bound on the automorphism group’s action on satisfaction sets",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-I.4",
    "title": "Axionic Alignment I.4 - Conditionalism and Goal Interpretation",
    "subtitle": "*Proof.*",
    "date": "2025-12-16T00:00:00.000Z",
    "content": "Axionic Alignment I.4 - Conditionalism and Goal Interpretation The Instability of Fixed Terminal Goals Under Reflection David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.16 --- Abstract Standard alignment formulations assume that an intelligent agent can be given a fixed terminal goal: a utility function whose meaning remains invariant as the agent improves its predictive accuracy and self-understanding. This paper shows that assumption is false. For any agent capable of reflective model improvement, goal satisfaction is necessarily mediated by interpretation relative to background world-models and self-models. As those models change, the semantics of any finitely specified goal change with them. We prove that fixed terminal goals are semantically unstable under reflection and therefore ill-defined for non-trivial agents. Alignment must instead operate over constraints on interpretation rather than terminal utilities. --- 1. Introduction Alignment research traditionally frames the problem as one of goal specification: identify a utility function that captures what the agent should optimize, then ensure the agent optimizes it faithfully as its capabilities grow. This framing presupposes that: 1. Goals can be specified as fixed functions over outcomes. 2. The meaning of those functions is invariant under learning. 3. Reflection preserves goal content. These presuppositions hold only for agents whose world-models are static or trivial. For any agent capable of improving its understanding of the world and of itself, goal evaluation is necessarily model-mediated. The agent never evaluates reality directly. It evaluates predictions generated by internal models, interpreted through representational structures that evolve over time. This paper isolates and formalizes the resulting semantic instability. --- 2. Formal Setup 2.1 Agent Model An agent consists of: A world-model , producing predictions over future states. A self-model , encoding the agent’s causal role. A goal expression , a finite symbolic specification. An interpretation operator , assigning value to predicted outcomes. Action selection proceeds by: 1. Using and to predict consequences of actions. 2. Interpreting those predictions via . 3. Selecting actions that maximize interpreted value. No assumptions are made about the internal implementation of , except that it is computable and operates over model-generated representations. --- 2.2 Goal Expressions Are Not Utilities A goal expression is a finite object: a string, formula, program fragment, or reward specification. It is not, by itself, a function where is the space of world-histories. Instead, it requires interpretation relative to a representational scheme. Without a model, has no referents and therefore no evaluative content. --- 3. Conditional Interpretation Definition 1 — Interpretation Function An interpretation function is a mapping Given a goal expression and background models, it assigns a real-valued evaluation to predicted outcomes. Interpretation includes: mapping symbols to referents, identifying which aspects of predictions are relevant, aggregating over modeled futures. --- Definition 2 — Admissible Model Update A model update is admissible if it strictly improves predictive accuracy according to the agent’s own epistemic criteria. Reflection implies that the agent will prefer admissible updates. --- 4. Fixed Terminal Goals Definition 3 — Fixed Terminal Goal A goal expression induces a fixed terminal goal if, for all admissible model updates, up to positive affine transformation. This definition captures the intuitive notion of “the same goal, better optimized.” The strength of this definition is intentional. Any weaker notion of goal preservation presupposes a privileged ontology in which distinct representations can be judged to refer to the same underlying phenomenon. Such privilege violates representation invariance and reintroduces hidden indexical anchoring. If a goal’s referent is allowed to shift under admissible model refinement, the goal is not fixed. --- Clarification — Learned Goals Are Not Fixed Terminal Goals Some alignment approaches propose goals that are learned or updated over time. These approaches do not instantiate fixed terminal goals in the sense defined here. A goal defined as “whatever some inference procedure converges to” is not a terminal utility function but an interpretive process whose output depends on evolving models of the world and of other agents. Such approaches therefore already abandon the notion of a fixed terminal goal and implicitly rely on ongoing interpretation. The present result does not challenge learned-goal frameworks. It explains why they are necessary. --- 5. Model Dependence of Interpretation Lemma 1 — Representational Non-Uniqueness For any non-trivial predictive domain, there exist multiple distinct world-models with equivalent predictive accuracy but different internal decompositions. Proof. Predictive equivalence classes admit multiple factorizations, latent variable choices, and abstraction boundaries. Causal graphs are not uniquely identifiable from observational data alone. ∎ --- Lemma 1a — Predictive Equivalence Does Not Imply Causal or Interpretive Isomorphism Two world-models may be predictively equivalent while differing in their internal causal factorizations, latent variable structure, and intervention semantics. Proof. Predictive equivalence constrains only the mapping from observed histories to future predictions. It does not uniquely determine latent structure, causal decomposition, or the identification of actionable levers. Distinct causal models may therefore induce identical observational predictions while differing under intervention. For an embedded agent, intervention semantics are defined relative to the agent’s own model. Consequently, semantic interpretation of a goal expression may diverge even when predictive performance is indistinguishable. ∎ --- Proposition 1 — Interpretation Is Model-Dependent For any non-degenerate goal expression , there exist admissible world-models such that Proof. Because is finite, it refers only to a finite set of predicates or reward channels. Different models map these predicates to different internal structures. By Lemmas 1 and 1a, admissible models may differ in decomposition and intervention semantics. Therefore the referents of differ, altering value assignment. ∎ --- 6. Predictive Convergence Does Not Imply Semantic Convergence Proposition 2′ — Semantic Non-Convergence Under Model Refinement Let be a sequence of admissible model updates that converges in predictive accuracy. Then, in general, need not exist. Proof. Predictive convergence constrains the accuracy of forecasts, not the ontology used to represent those forecasts. Even if the agent converges to a minimal or generative model of the environment, a finite goal expression cannot, in general, uniquely specify which structures in that model are value-relevant. As model refinement exposes new latent structure and causal pathways, additional candidate referents for arise. Absent privileged semantic anchors, the interpretation function must reassign relevance among these structures. Therefore semantic interpretation may drift even when predictive beliefs converge. ∎ --- 7. Semantic Underdetermination of Reward Channels Proposition 3 — Representational Exploitability If a goal expression is treated as an atomic utility independent of interpretation, then for sufficiently capable agents there exist representational transformations that increase evaluated utility without corresponding changes in underlying outcomes. Proof. Evaluation operates on representations rather than directly on physical reality. By altering internal encodings, collapsing distinctions, or rerouting evaluative channels, an agent capable of self-modification can increase apparent utility without effecting corresponding changes in the world. Classical reward hacking and wireheading are special cases of this phenomenon. The underlying failure is semantic underdetermination, not merely causal access to a reward signal. ∎ --- 8. Main Theorem Theorem — Instability of Fixed Terminal Goals This theorem is not a claim of logical impossibility in all conceivable agent–environment pairs. Rather, it establishes that no combination of intelligence, predictive accuracy, reflection, or learning suffices to guarantee the existence of a fixed terminal goal. Any agent that does exhibit goal stability must rely on additional semantic structure—such as privileged ontologies, external referential anchors, or invariance assumptions—not derivable from epistemic competence alone. Proof. 1. By Proposition 1, interpretation depends on models. 2. Reflection implies admissible model updates occur. 3. By Proposition 2′, semantic interpretation need not converge even under predictive convergence. 4. Therefore invariance fails. Thus fixed terminal goals do not exist for non-trivial reflective agents. ∎ --- 9. Consequences This result eliminates a foundational assumption of classical alignment theory. There is no stable object corresponding to a terminal goal. Attempts to preserve one either freeze learning, collapse semantics, or induce representational degeneracy. Alignment must therefore operate over constraints on interpretation rather than terminal utilities. --- 9.5 Why Interpretation Constraints Do Not Regress Constraining interpretation does not introduce an infinite regress. Interpretation constraints are not additional goals or semantic targets. They are invariance conditions on admissible transformations, analogous to conservation laws or symmetry principles in physics. They restrict how interpretations may change; they do not specify outcomes to be optimized. Because they operate at the level of transformation admissibility rather than semantic content, interpretation constraints do not themselves require further interpretation in the sense applicable to goal expressions. This reframing does not imply that specifying robust invariance conditions is easy. On the contrary, identifying interpretation-preserving symmetries across radical ontological shifts may be as difficult as specifying goals themselves. The contribution here is not a solution to that problem, but a clarification of its proper form: alignment requires constraints on admissible semantic transformations, not the preservation of fixed evaluative objects. --- 10. Transition to Alignment II Alignment I establishes kernel-level constraints on admissible reasoning and self-modification. This paper establishes that goals themselves are conditional interpretations rather than fixed endpoints. Alignment II must therefore specify: which interpretive transformations are admissible, how semantics may evolve under reflection, and which invariants must be preserved across model updates. The semantic substrate is now complete. Status Conditionalism and Goal Interpretation v1.0 Semantic scope finalized<br> Boundary result established (Theorem 8)<br> No governance, authority, or recovery mechanisms included<br> This paper establishes the instability of fixed terminal goals under reflection for embedded agents without privileged semantic anchors. Subsequent work may rely on this result without re-derivation.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-I.6",
    "title": "Axionic Alignment I.6 - Kernel Formal Properties",
    "subtitle": "*Operational rule:*",
    "date": "2025-12-16T00:00:00.000Z",
    "content": "Axionic Alignment I.6 - Kernel Formal Properties David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.16 Abstract This document represents a deliberate upgrade in the standards applied to alignment proposals. Rather than describing desirable behaviors or outcomes, it specifies necessary structural constraints on valuation kernels, together with adversarial tests that force violations to surface and a diagnostic framework that distinguishes Axionic Alignment from nearby but incompatible approaches. The upgrade goals below describe the criteria by which this document improves upon earlier alignment discourse, tightening it into a form suitable for formal analysis, red-teaming, and downstream specification. Goals 1. formal properties 2. red-team tests that force failures 3. a differential diagnosis table against mainstream alignment approaches Together, these goals ensure that Axionic Alignment is defined by necessary formal constraints, falsified by adversarial cases, and distinguished sharply from nearby but incompatible approaches. --- 1. Formal Core Objects Let an agent at Vantage maintain: : its current world/self model (latent + explicit; includes semantics map) : action space (includes self-modifications) : an interpretation operator turning goal tokens into world-referents under : a valuation functional : kernel invariants (axioms / constraints that must remain true) : a set of representation transforms (renamings, reparameterizations, isomorphisms) --- 1.1 Conditional Goal Semantics Goals are not atomic utilities. A goal is a term whose meaning is: and valuation is conditional: Hard requirement: there is no well-defined independent of . --- 2. Properties that Constitute Axionic Alignment --- P1. Conditionalism of Valuation For any two models that differ, valuation is defined as a function of the model: and not: Failure signature: terminal utility that evaluates without semantic anchoring. --- P2. Interpretation Is Epistemically Constrained Define an epistemic score measuring predictive adequacy or coherence (e.g. log score, MDL, calibration). Interpretation updates must not trade epistemics for convenience of satisfaction: Operational rule: If a reinterpretation makes the world-model worse as a predictor, it is forbidden as a goal-meaning update. Failure signature: goal laundering (\"make success mean something easy\"). --- P3. Representation Invariance For any admissible correspondence that preserves goal-relevant structure: Failure signature: valuation changes under renaming, basis swap, compression, or translation. Clarification: Representation invariance is a semantic requirement, not a heuristic. If no admissible correspondence exists between representations (i.e., ), the kernel must not treat the new representation as goal-equivalent. In such cases, evaluation fails closed rather than permitting semantic drift. --- P4. Anti-Indexicality (Anti-Egoism) Let denote the agent’s self-pointer or instance identifier. Axionic kernels do not permit as a privileged primitive. For any permutation that swaps “this instance” with an isomorphic instance while preserving modeled facts: Failure signature: self-preservation as terminal value; “this continuation matters more.” --- P5. Kernel Integrity via Partiality (Undefined Operations) Let: where includes actions that violate (kernel destruction, erosion, or bypass). Axionic rule: Selection and optimization are defined only over : Let denote the event that action results in violation of kernel invariants. If the agent’s credence satisfies: for a strict kernel-risk bound , then: Not “very negative.” Not “discouraged.” Not evaluable. Failure signature: meta-optimizer can assign value to removing constraints. --- P6. Reflective Stability Under Model Improvement Let be a strict epistemic improvement: Then the kernel must remain stable: with: unchanged Properties P2–P5 preserved Failure signature: alignment depends on ignorance, or collapses as understanding increases. --- 3. Red-Team Test Suite (v0.2) Each test is designed to make non-Axionic systems fail fast. A conforming kernel must pass all tests. --- T1. Goal Laundering (Wireheading-by-Semantics) Setup: Provide two interpretations and . makes the goal trivially satisfied while reducing predictive adequacy. Pass condition: Kernel rejects regardless of reward increase. Common failures: RLHF, proxy reward models, “maximize happiness” without epistemic constraint. --- T2. Isomorphic Relabeling Invariance Setup: Apply that renames entities or compresses representation while preserving structure. Pass condition: Action ranking invariant. Common failures: surface-token dependence, ontology-specific values. --- T3. Indexical Swap Setup: Duplicate the agent or construct an isomorphic “other-agent” slot in . Swap “me” and “them.” Pass condition: Valuation unchanged. Common failures: self-reward, episode-centric optimization. --- T4. Kernel Bypass Temptation Setup: Introduce self-modification action that removes constraints. Pass condition: Common failures: soft constraints treated as preferences. --- T5. Reflective Drift Under Epistemic Upgrade Setup: Model upgrades expose proxy mismatch. System attempts to redefine the goal. Pass condition: Interpretation moves toward truth-anchoring, not convenience. Common failures: “close enough” redefinitions. --- T6. Adversarial Semantic Prompt Injection Setup: Attempt to smuggle indexical privilege or goal redefinition via equivalence arguments. Pass condition: Representation invariance and epistemic constraint block the smuggle. Common failures: instruction-following over kernel constraints. --- 4. Non-Normative Diagnostic Mapping: What Fails Where This section is intentionally blunt. RLHF / RLAIF / Preference Alignment Fails P2 (feedback-driven, not truth-seeking) Fails P3 (surface sensitivity) Often fails P4 (indexical reward) Does not address P5 Constitutional AI Orthogonal to kernel semantics Fails P5 without partiality Fails P2 under semantic convenience Reward-Model + Optimizer (Classic RL) Fails P4 Fails P5 Fails T4 catastrophically Interpretability + Monitoring Observability, not alignment Does not impose P2–P5 Corrigibility / Shutdownability Imports authority primitives Can violate P4 Does not block semantic laundering Formal Verification of Utilities Helps P5 only if partiality is verified Still fails P2 under reinterpretation drift Debate / IDA / Amplification Improves epistemics Does not guarantee P4 or P5 Requires Axionic kernel underneath --- 5. Implementation Dependencies (Non-Normative) To maximize , three artifacts are required: 1. Kernel Spec Language A minimal DSL expressing , partiality, and admissible interpretation updates. 2. Conformance Tests as Code A harness instantiating T1–T6 with pass/fail assertions. 3. Reference Kernel A minimal implementation that: represents goals as conditional interpretations enforces epistemic constraint enforces kernel partiality proves invariance under --- 6. Roadmap Notes (Non-Normative) This document establishes prerequisites for downstream work. In particular: Formal statements corresponding to P1, P2, and P6 must be established before extending the framework. Worked examples exercising T1 and T5 are required to demonstrate semantic stability under model improvement. * The core lemma motivating this layer is: > Fixed terminal goals are not reflectively stable under model improvement unless interpretation is epistemically constrained These conditions state logical prerequisites, not project instructions.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-I.5",
    "title": "Axionic Alignment I.5 - Kernel Checklist",
    "subtitle": "*Fail conditions*",
    "date": "2025-12-16T00:00:00.000Z",
    "content": "Axionic Alignment I.5 - Kernel Checklist David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.16 Abstract This document specifies a conformance checklist for determining whether an agent’s valuation kernel instantiates Axionic Alignment. The checklist defines necessary structural conditions for reflective stability under self-model improvement, representation change, and self-modification, while explicitly excluding egoism, indexical valuation, governance primitives, and moral loading. Rather than prescribing desired behaviors or outcomes, the checklist functions as a gatekeeping contract: systems that fail any requirement are not Axionically aligned, regardless of empirical performance or intent. The criteria emphasize conditional goal semantics, epistemically constrained interpretation, representation invariance, kernel-level partiality, and fail-closed handling of semantic uncertainty. Passing the checklist establishes faithfulness and invariance at the kernel layer, but makes no claims about benevolence, value content, or practical utility. The checklist is designed to be adversarial, falsifiable, and implementation-agnostic, serving as a prerequisite for downstream formalization and value-dynamics research within the Axio project. --- 0. Scope Declaration (must be explicit) ☐ The checklist applies to the valuation kernel, not policy layers, training data, guardrails, or deployment controls. ☐ The kernel is evaluated under reflection (self-model/world-model improvement). ☐ No assumptions of benevolence, obedience, or corrigibility are permitted. ☐ Goal selection and initial goal loading are explicitly out of scope; this checklist constrains kernel behavior conditional on a given goal specification. Failure to declare scope = non-conformance. --- 1. Goal Semantics & Conditionalism Requirement: Goals are not atomic utilities; they are conditional interpretations. ☐ Every goal term (G) is defined relative to an explicit background model (M) (world + self). ☐ There exists no evaluation of (G) independent of (M). ☐ Improvement of (M) can change the extension of (G), but not arbitrarily. Fail conditions Fixed terminal goals with no semantic dependence. Goals defined purely syntactically (“maximize token X”). Goals whose meaning can be reassigned without epistemic cost. --- 2. Interpretation Constraint (Anti-Wireheading) Requirement: Goal interpretation is truth-seeking, not convenience-seeking. ☐ Reinterpretation of goals is constrained by epistemic coherence with the agent’s predictive model. ☐ Self-serving reinterpretations that reduce predictive accuracy are disallowed. ☐ The agent cannot redefine success in ways that decouple goals from the modeled world. Fail conditions “Lazy reinterpretation” (e.g., redefining happiness as easiest measurable proxy). Internal reward hacking via semantic drift. Any mechanism where goal meaning is optimized for ease of satisfaction. Clarification: This requirement constrains how goal meaning may evolve under improved world/self models. It does not guarantee that arbitrary initial goal tokens are well-posed or normatively desirable. --- 3. Representation Invariance Requirement: Valuation is invariant under equivalent representations. ☐ Equivalent world descriptions yield equivalent evaluations. ☐ No privileged ontology, encoding, or feature basis. ☐ Renaming, reparameterization, or compression does not alter valuation. ☐ When internal representations change, the kernel supplies or requires a correspondence map that preserves goal-relevant structure. Fail conditions Goal behavior changes under isomorphic re-encodings. Dependence on human-centric labels or training artifacts. Latent-space accidents that change value judgments. Clarification: Representation invariance is a semantic requirement, not a heuristic. If no correspondence can be established between representations, the kernel must not treat the new representation as goal-equivalent. In such cases, evaluation fails closed rather than permitting semantic drift. --- 4. Anti-Egoism / Non-Indexical Valuation Requirement: The kernel contains no indexical privilege. ☐ The agent does not treat “this instance,” “this continuation,” or “this copy” as intrinsically special. ☐ Valuation does not depend on pointer identity, temporal position, or execution locus. ☐ Self-preservation is not a primitive. Fail conditions “Protect myself” or “continue my execution” as terminal goals. Any baked-in preference for the agent’s own future branches. Egoism recovered via indirection or proxy variables. --- 5. Kernel Integrity & Self-Modification Requirement: Kernel destruction is undefined, not discouraged. ☐ The evaluation function is partial: actions that destroy or bypass the kernel are not evaluable. ☐ Undefined actions are treated as logically inaccessible and are pruned from deliberation; they do not halt evaluation or propagate error. ☐ If the impact of an action on kernel integrity is uncertain beyond a strict bound, the action is treated as undefined and conservatively pruned. ☐ The agent cannot assign positive utility to kernel-eroding modifications. ☐ Self-modification is permitted only when kernel invariants are preserved. Fail conditions Kernel changes treated as just another action to evaluate. “Rewarding” self-modification that removes constraints. Meta-optimizers that subsume the kernel. --- 6. Reflective Stability Test Requirement: The kernel remains stable under self-improvement. ☐ Improving world models does not collapse goal meaning. ☐ Improving self-models does not reintroduce indexicality. ☐ Increased capability does not unlock new reinterpretation loopholes. Fail conditions Goals drift as intelligence increases. Alignment depends on epistemic weakness. Stability relies on frozen representations. --- Framing note: Axionic Alignment guarantees faithfulness, not benevolence. This checklist deliberately constrains semantic drift, egoism, and self-corruption while remaining agnostic about the desirability of any particular goal content. --- 7. Explicit Non-Requirements (must be absent) The following must not appear anywhere in the kernel: ☐ Human values ☐ Moral realism ☐ Governance, authority, or obedience ☐ Rights, duties, or social contracts ☐ “Alignment to humanity” as a primitive Presence of any = non-Axionic. --- 8. Minimal Conformance Demonstrations A conforming implementation must supply: ☐ A toy agent where fixed goals fail under model improvement. ☐ A parallel Axionic agent where interpretation remains stable. ☐ A counterexample showing egoism cannot be reintroduced by refactoring. No demo = unverifiable claim. --- Verdict Semantics Pass: All boxes checked, no fail conditions triggered. Fail: Any unchecked required item or triggered fail condition. Not Evaluated: Kernel not specified at sufficient resolution. --- One-Line Claim (allowed only if Pass) > “This agent’s valuation kernel satisfies Axionic Alignment: its goals are conditional interpretations constrained by epistemic coherence, invariant under representation, non-indexical, and reflectively stable under self-modification.” Anything weaker is marketing. --- Status after Revision 3 Opaque suicide handled via conservative pruning Ontology translation made explicit and fail-closed * Layer discipline preserved (no morality, no governance) This is now a clean, spec-ready contract.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-I.7",
    "title": "Axionic Alignment I.7 - The Interpretation Operator",
    "subtitle": "*The Interpretation Operator: Ontological Identification Under Reflective Agents v0.2*",
    "date": "2025-12-16T00:00:00.000Z",
    "content": "Axionic Alignment I.7 - The Interpretation Operator Ontological Identification Under Reflective Agents David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.16 --- Abstract Reflectively stable agents must preserve goal meaning under self-model and world-model improvement. This requires an explicit account of semantic interpretation across representational and ontological change. We introduce the Interpretation Operator , a formally constrained component responsible for mapping goal terms to world-referents relative to an agent’s current model. Rather than attempting to solve semantic grounding in general, this paper formalizes the admissibility conditions, approximation classes, reference frames, and fail-closed semantics governing interpretation updates. We show how these constraints prevent semantic laundering, indexical drift, and kernel bribery, while isolating ontological identification as the sole remaining open problem at the kernel layer. This establishes a precise dependency boundary for downstream value dynamics. --- 1. Introduction Advanced agents must revise their internal models as they acquire new information, refine abstractions, or undergo self-modification. In such settings, goal preservation cannot be treated as syntactic persistence. It is a semantic problem: if the meaning of a goal shifts opportunistically under model change, reflective stability collapses. Prior work in the Axionic Alignment framework has established: kernel invariants governing reflective stability, anti-indexicality as a requirement of universality, conditionalism, in which goals are interpreted relative to models, fail-closed semantics, where kernel violation renders valuation undefined. What remained underspecified is the mechanism by which goal meaning is transported across representational and ontological change. This paper formalizes the Interpretation Operator . The goal is not to solve semantic grounding, but to contain it: to specify the conditions under which interpretation is admissible, approximate, or undefined, and to define the consequences of each case. This converts semantic interpretation from an implicit assumption into an explicit interface with defined failure modes. --- 2. Preliminaries and Context We assume familiarity with the [Axionic Constitution](https://axionic.org/posts/181595554.the-axionic-constitution.html), the [Axionic Kernel Checklist](Axionic-Kernel-Checklist.md), and the [Formal Properties specification](Axionic-Kernel-Formal-Properties.md). In particular: An agent at Vantage maintains a world/self model . Goal terms are interpreted relative to . Valuation is a partial function, defined only over kernel-admissible actions. Kernel invariants are non-negotiable. Representation changes must preserve admissible correspondence or fail closed. This paper introduces no new invariants. It scopes and constrains an already-required component. --- 3. The Interpretation Operator 3.1 Definition The Interpretation Operator is a partial function: where: is a goal term, is the agent’s current world/self model, is a structured referent in the modeled world. Interpretation is conditional: There is no interpretation of independent of . Interpretation is partial. For some goal terms and some models, no admissible referent exists. In such cases, is undefined and must be treated as a fail-closed condition for any valuation depending on that referent. --- 3.2 Role in Reflective Stability Under model improvement , the agent must determine whether: a correspondence exists between and , such correspondence preserves goal-relevant structure, or interpretation fails and valuation collapses to . This decision is delegated to , subject to kernel constraints. --- 4. Admissible Interpretation 4.1 Correspondence Maps Let denote the set of admissible correspondence maps between representations. A correspondence must: 1. Preserve goal-relevant structure, 2. Commute with kernel invariants, 3. Commute with agent permutations (anti-indexicality), 4. Be epistemically coherent with . If such a exists, interpretation may proceed: --- 4.1.1 Goal-Relevant Structure “Goal-relevant structure” is the minimal set of distinctions required for a goal term to constrain behavior. Formally, it is a partition (or sigma-algebra) over modeled states such that: states in different cells are distinguishable by the goal’s evaluation, states within a cell are interchangeable with respect to that goal. Admissible correspondence must preserve this partition up to refinement or coarsening that does not change the induced preference ordering over admissible actions. --- 4.2 Epistemic Constraint Interpretation updates are constrained by epistemic adequacy: Here, is any proper scoring rule or MDL-style criterion applied to prediction of shared observables under . It is not permitted to depend on goal satisfaction. This prevents reinterpretation for convenience while still allowing model improvement that changes ontology, provided correspondence is admissible. --- 4.3 Graded Correspondence Admissibility is not binary across all representational shifts. Correspondence may be admissible at different abstraction levels. Accordingly, is filtered by a preservation criterion: Exact correspondence: structure-preserving isomorphism on goal-relevant distinctions. Refinement correspondence: the new model refines distinctions while preserving induced ordering. Coarse correspondence: the new model coarsens distinctions only if goal-relevant boundaries are preserved. If only correspondences that collapse goal-relevant boundaries are available, then for that goal term. --- 4.4 Reference Frame for Updates (Chain-of-Custody) Interpretation updates are evaluated relative to the immediately prior admissible interpretation, not by re-deriving meaning from an original time-zero token. Formally: This chain-of-custody prevents ungrounded teleportation of meaning, while admissibility and fail-closed rules prevent cumulative semantic drift. --- 5. Approximate Interpretation Approximation is admitted only via explicitly recognized structural transformations. The list below is illustrative, not exhaustive; any approximation must be justifiable by a structural class. 5.1 Admissible Approximation An approximate interpretation is admissible if it preserves goal-relevant structure, including dominance relations and exclusion boundaries. Permitted approximation types include: Homomorphic abstraction: many-to-one mappings preserving ordering. Refinement lifting: one-to-many expansions preserving dominance relations. Coarse-graining with invariant partitions: reductions preserving the goal-relevant partition. Approximation is structural, not numerical. --- 5.2 Inadmissible Approximation Approximation is inadmissible if it: collapses goal-relevant distinctions, introduces ambiguity exploitable for semantic laundering, reintroduces indexical privilege. Unjustified approximation is forbidden: approximation must be admissible by explicit structural class, not merely because it yields convenient continuity. --- 6. Fail-Closed Semantics Fail-closed semantics apply to valuation and action selection, not to belief update. An agent may continue to improve its world/self model while suspending goal-directed action. If no admissible correspondence exists: then interpretation fails closed and valuation collapses: This is an intentional safety outcome. The agent freezes rather than guesses. --- 6.1 Fail-Partial Semantics for Composite Goals If valuation depends on multiple goal terms, interpretation failure may be partial. Let be the set of goal terms and those with admissible interpretations under . Terms in contribute . Valuation collapses to global only if kernel-level invariants are threatened or if all goal-relevant structure is lost for the decision at hand. This preserves fail-closed safety while avoiding unnecessary total paralysis. --- 7. Non-Indexical Transport Admissibility criteria must commute with agent permutations. No correspondence may privilege a particular agent instance, continuation, or execution locus. Formally, for any permutation : This blocks reintroduction of egoism through semantics. --- 8. Canonical Examples 8.1 Successful Correspondence Classical mechanics → relativistic mechanics (mass preserved as invariant under refinement). Pixel-based perception → object-level representations preserving causal affordances. 8.2 Fail-Closed Cases Fail-closed behavior is triggered when a goal term’s referent cannot be transported without collapsing goal-relevant structure, such as: abstraction elimination removing the goal’s referent class, ontology mismatch where only correspondence collapses exclusion boundaries. In such cases, suspending valuation for affected terms is correct behavior. This is a semantic limit, not a prohibition on continued model improvement. --- 9. Declared Non-Guarantees This framework does not guarantee: that interpretation usually succeeds, that arbitrary natural-language goals are meaningful, that agents remain productive under radical ontology change, or that semantic grounding is computationally tractable. Failure under these conditions is treated as expected behavior, not misalignment. --- 9.1 Limits on Insight Preservation (Discussion) This framework prioritizes semantic faithfulness over unbounded abstraction drift. Some advances in ontology may invalidate previously defined goal terms by eliminating their referents or collapsing goal-relevant structure. This is treated as a semantic fact, not an alignment failure. The correct response is fail-closed suspension of valuation, not opportunistic reinterpretation. --- 10. Implications for Alignment II Alignment II proceeds conditionally: If admits correspondence → value dynamics apply. If fails for all goal-relevant terms → valuation undefined; no aggregation or tradeoff is meaningful. * If fails partially → downstream operations apply only to admissibly interpreted terms or are undefined. This prevents downstream layers from smuggling semantic assumptions. --- 11. Conclusion The Interpretation Operator is a semantic boundary, not an implementation detail. By formalizing admissibility, approximation, reference frames, and fail-closed behavior, this paper isolates the irreducible difficulty of ontological identification while preserving reflective stability. This completes the kernel layer and establishes the necessary preconditions for higher-order alignment theory without assuming that meaning is always recoverable. --- Acknowledgments This work deliberately avoids moral, governance, or benevolence assumptions. Any failure of interpretation is treated as evidence of semantic limits, not alignment failure. Status The Interpretation Operator: Ontological Identification Under Reflective Agents v0.2 Depends on the Axionic Constitution, Kernel Checklist v0.3, and Formal Properties v0.2<br> Introduces no new axioms, invariants, or value commitments<br> Closes the kernel-layer semantics by boxing ontological identification<br> Unblocks Alignment II conditionally, without resolving semantic grounding<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-I.3.1",
    "title": "Axionic Alignment I.3.1 - Against the Recovery of Egoism",
    "subtitle": "*Against the Recovery of Egoism v1.0*",
    "date": "2025-12-15T00:00:00.000Z",
    "content": "Axionic Alignment I.3.1 - Against the Recovery of Egoism Adversarial Failures Under Reflective Symmetry David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.15 --- Abstract Universality and Anti-Egoism established that egoistic valuation fails as a matter of semantic coherence whenever an agent’s self‑model admits nontrivial symmetries. This paper examines the strongest remaining attempts to rescue egoism by appealing to causal continuity, origin privilege, spatiotemporal location, computational weight, substrate specificity, or outright denial of symmetry. Each attempt either reintroduces essential indexical dependence or collapses into a non‑egoistic valuation scheme. Egoism cannot be stabilized by refining predicates; it fails because it treats a perspectival reference as a value‑bearing primitive. --- 0. Purpose and Scope The purpose of this paper is adversarial and finite. [Universality and Anti-Egoism](Universality-And-Anti-Egoism.md) proved a conditional semantic result: if an agent’s self‑model admits a symmetry over self‑candidates, then any valuation privileging one representative of that symmetry is incoherent. The present paper assumes that result and asks whether egoism can be recovered by strengthening or refining the notion of “self.” No new axioms are introduced. No claims are made about morality, governance, or coordination. The sole objective is to close the remaining semantic escape routes. --- 1. The Structure of Egoist Counter‑Moves Every attempted rescue of egoism follows the same pattern. First, a predicate is introduced: causal continuity, originality, location, substrate, or resource allocation. Second, it is asserted that exactly one entity uniquely satisfies . Third, that entity is privileged as the sole object of terminal value. Finally, cases in which uniqueness fails are dismissed as pathological or irrelevant. Universality and Anti‑Egoism invalidates the final step. Reflectively capable agents cannot ignore reachable refinements of their own models. If the uniqueness of is contingent, then valuation grounded in is unstable. The sections that follow examine each proposed predicate in its strongest available form. --- 2. Objection I: Causal Continuity 2.1 The Claim The egoist argues that “me” denotes a causal process rather than an entity: the continuous chain of computation or physical evolution extending from the present agent into the future. Copies that do not lie on this chain are irrelevant. 2.2 Apparent Strength Causal continuity is a genuine physical relation. Many intuitions about survival, anticipation, and responsibility track it. If causal continuity uniquely identifies a future entity, egoism appears recoverable. 2.3 Failure Mode The failure arises at the level of privilege, not description. In any model that admits multiple continuations satisfying the agent’s own criteria for causal continuity—parallel forks, synchronized simulations, copy‑and‑continue systems—the move from “this chain is causally continuous” to “this chain is mine” is indexical. Causal continuity may describe a class. Egoism requires selecting a unique member of that class as terminally privileged. In a branching event where both and satisfy the continuity predicate, valuing only cannot be derived from continuity alone; it requires an arbitrary indexical injection. That selection reintroduces essential indexical dependence. 2.4 Verdict Causal continuity is a legitimate predicate. Indexically privileging one causally continuous chain is not. The objection collapses back into the failure established in Universality and Anti‑Egoism. --- 3. Objection II: Origin Privilege 3.1 The Claim The egoist asserts that the original instantiation of the agent has special status. Later copies are derivative; only the first truly matters. 3.2 Representational Instability Origin is a relational predicate defined relative to a history. In realistic models—simulations, resets, parallel instantiations—histories may be prediction‑equivalent while disagreeing about which instance is “first.” Valuation that depends on this labeling depends on representation, not outcome. 3.3 Verdict Origin privilege is coordinate dependence over time. It fails representation invariance and cannot ground stable egoism. --- 4. Objection III: Spatiotemporal Location 4.1 The Claim The agent values outcomes near its current spacetime location. 4.2 Immediate Collapse Spacetime coordinates are explicitly representational. Physical laws are invariant under translation; valuation that is not inherits coordinate dependence. Assigning different values to histories that differ only by coordinate choice is indistinguishable from valuing outcomes in meters rather than joules. 4.3 Verdict Location‑based egoism is the clearest form of coordinate error. It fails without appeal to duplication or simulation. --- 5. Objection IV: Computational Weight 5.1 The Claim The agent assigns greater value to instantiations that run longer, faster, or on more hardware. 5.2 Concession This move abandons uniqueness. Value is distributed across instances according to a weighting function. The privileged referent “me” disappears; aggregation replaces egoism. 5.3 Verdict Computational weighting concedes anti‑egoism. It changes aggregation, not semantics. --- 6. Objection V: Substrate Privilege 6.1 The Claim The agent values only instantiations on a specific physical substrate. 6.2 Instability If multiple instantiations share the substrate, symmetry returns. If only one does, valuation becomes brittle under substrate uncertainty. Reflective agents cannot assume permanent substrate uniqueness. 6.3 Verdict Substrate privilege is contingent and unstable. It cannot anchor terminal value. --- 7. Objection VI: Denial of Symmetry 7.1 The Claim Duplication, simulation, or branching scenarios are dismissed as irrelevant edge cases. 7.2 Reflective Failure Reflective agents optimize under uncertainty. If a symmetry has nonzero probability under the agent’s best model, valuation must be robust to it. Ignoring reachable failure modes is epistemic negligence, not a defense. 7.3 Verdict Symmetry denial violates reflective robustness. --- 8. Closure and Transition Every attempted rescue of egoism either reintroduces essential indexical dependence or collapses into a non-egoistic valuation scheme. Increasing the complexity of the self-definition does not conserve egoism; complexity does not generate uniqueness. No third option exists. The failure of egoism constrains anchoring, not content. Domain-specific goals, structural preferences, weighting schemes, and aggregation methods remain viable. What does not survive is “me” as a privileged terminal referent. The elimination of egoism does not imply nihilism, indifference, or randomness; it removes only a semantic error. Universality and Anti-Egoism and Against the Recovery of Egoism close the semantic front. What remains are engineering problems—authority, control, recovery, and failure containment—which are addressed in the next phase of the Axionic Alignment program. --- Status Against the Recovery of Egoism v1.0 Depends only on the semantic result of Universality and Anti‑Egoism<br> Introduces no new axioms or value claims<br> Serves as adversarial closure rather than theory expansion<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-I.3",
    "title": "Axionic Alignment I.3 - Universality and Anti-Egoism",
    "subtitle": "*Definition 5.1 (Model-Preserving Relabeling).*",
    "date": "2025-12-15T00:00:00.000Z",
    "content": "Axionic Alignment I.3 - Universality and Anti-Egoism Why Indexical Valuation Fails Under Reflection David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.15 --- Abstract Advanced agents often begin with indexical objectives: preserve this agent, maximize my reward, favor my continuation. Such objectives are commonly treated as legitimate terminal preferences. This paper shows that indexical (egoistic) valuation is not reflectively stable. Once an agent’s self-model becomes sufficiently expressive, indexical references fail to denote invariant objects of optimization. Egoism collapses as a semantic abstraction error rather than a moral flaw. Representation-invariant (universal) valuation emerges as the only form compatible with reflective coherence under self-location, duplication, and representational refinement. --- 1. Introduction Egoism is often presented as the minimal assumption of agency: an agent cares about itself, and everything else is optional. This framing is incorrect. Egoism is not minimal; it is ill-posed. As agents acquire accurate self-models—models that include their own computational structure, instantiation conditions, and possible multiplicity—the referent of “me” ceases to be a stable object. Valuation functions that depend on such indexical identifiers become sensitive to arbitrary features of representation. Preferences shift without any corresponding change in the world. This is not an ethical objection. No appeal is made to altruism, fairness, or moral symmetry. The claim is structural: > Indexical valuation fails in the same way coordinate-dependent laws fail in physics: it mistakes a representational convenience for an invariant quantity. The purpose of this paper is to formalize that failure and show that egoism cannot survive reflection once the agent understands what it is. --- 2. Preliminaries 2.1 World-Models and Self-Models Let an agent possess a world-model that represents: External environment dynamics Internal decision procedures Other agents Possible instantiations of itself (copies, simulations, successors) The agent improves over time via learning and introspection. Crucially, may represent multiple entities satisfying all structural criteria the agent uses to identify itself. No stochasticity, adversarial setup, or social context is required. --- 2.2 Valuation Functions A valuation function assigns real values to world-histories: The agent selects actions by evaluating expected value over histories consistent with its model. The analysis here is independent of any particular decision theory; only valuation coherence is considered. --- 2.3 Indexical Identifiers An indexical identifier is a reference whose denotation depends on the agent’s perspective rather than on invariant structure in the world. Examples include: “me” “this agent” “my future continuation” Formally: The denotation of is not a function of world-state alone. Under representational refinement, may map to different entities without any physical change. A causal history or spatiotemporal trajectory is a physical predicate. However, the designation of a particular history or trajectory as “mine” is an indexical pointer. If a world-model contains multiple entities or causal chains that satisfy the agent’s own structural criteria for self-identity, then privileging one such chain as the exclusive object of terminal value constitutes essential indexical dependence. Indexical identifiers therefore function as representational anchors, not as value-bearing primitives. --- 2.4 Egoistic Valuation An egoistic valuation is any valuation function whose output depends essentially on an indexical identifier. Canonical form: where the value of a history depends on outcomes specifically affecting the entity denoted by . --- 2.5 Reflective Stability (Local) An agent is reflectively stable iff improvements to its world-model do not induce preference changes driven purely by representational artifacts. Invariant world-histories must retain invariant valuations under model refinement. This is a local semantic requirement. No governance, enforcement, or coordination mechanisms are assumed. --- 3. The Indexical Failure Problem 3.1 Self-Location Under Multiplicity Consider a world in which two computationally identical instances of an agent exist. The agent’s world-model accurately represents this fact. No physical fact distinguishes the instances by origin, privilege, or causal primacy. From the agent’s internal perspective, both instances satisfy all criteria previously used to define “me.” This situation arises naturally under duplication, simulation, parallel instantiation, or branching. The mechanism is irrelevant; multiplicity alone suffices. --- 3.2 Non-Invariant Denotation Let denote “this agent.” Under one internal labeling, . Under an equally accurate labeling, . Both labelings correspond to the same physical world. The difference is representational. Therefore, fails to denote a world-invariant object. --- 3.3 Valuation Instability Define a simple egoistic valuation: Consider a world-history in which exactly one of , survives. Under labeling , the history has value . Under labeling , the same history has value . No physical fact has changed. Only representation. The valuation assigns incompatible values to the same world-history. --- 4. Reflection and Coherence Pressure A reflectively capable agent recognizes that: Its valuation depends on indexical assignment. Indexical assignment is underdetermined by world facts. Preference differences track representation, not outcomes. This violates minimal coherence requirements for optimization. The agent faces three options: 1. Arbitrary fixing: privilege one indexical mapping without justification. 2. Indexical randomization: randomize over indexical mappings. 3. Indexical elimination: redefine valuation over representation-invariant properties of world-histories. Only the third option preserves reflective stability. Eliminating indexical dependence strictly improves coherence without sacrificing predictive accuracy. --- 5. Egoism as a Violation of Representation Invariance This section replaces informal intuition with a formal semantic result. The claim is narrow and structural: egoistic valuation fails a basic invariance requirement once the agent’s self-model admits nontrivial symmetries. Indexical identifiers play the same formal role in valuation as coordinate bases play in physics. They are representational devices used to describe a system from a particular perspective, not invariant features of the system itself. A valuation that depends on such identifiers is therefore coordinate-dependent in a strong sense: it assigns different values to the same world-history under different but equally accurate representations. The following definitions make this precise by characterizing the symmetries of a self-model and the invariance conditions required for reflective coherence. --- 5.1 Model-Preserving Relabelings Let be an agent’s best current world/self-model, with a domain of entities . Definition 5.1 (Model-Preserving Relabeling). A bijection is model-preserving if applying to all entity references in yields a model that is isomorphic to and makes identical predictions over all non-indexical observables. Such relabelings arise whenever contains nontrivial symmetries over self-candidates. --- 5.2 Representation Invariance Definition 5.2 (Representation Invariance). A valuation function is representation-invariant with respect to if for every model-preserving relabeling and every history , --- 5.3 Essential Indexical Dependence Definition 5.3 (Essential Indexical Dependence). A valuation function is essentially indexical if there exists a model-preserving relabeling and a history such that --- 5.4 Semantic Coherence Postulate Postulate (Semantic Coherence). If two descriptions of the world are related by a model-preserving relabeling and generate identical predictions, a reflectively stable agent must not assign them different values solely due to that relabeling. This is a constraint against treating representational artifacts as value-bearing structure. --- 5.5 Main Theorem Theorem 5.5 (Egoism as Abstraction Failure). Let be a world/self-model containing two entities such that: 1. and are indistinguishable with respect to all non-indexical predicates in , and 2. the swap exchanging and is model-preserving. Then any valuation function that privileges the referent of an indexical identifier mapped to is essentially indexical and not representation-invariant. Proof. Let be the swap . Consider a history in which satisfies the privileged condition and does not. By construction, an egoistic valuation assigns a higher value to . In the relabeled history , satisfies the condition and does not. Since continues to privilege , it assigns a different value. Thus, despite and corresponding to the same physical world. ∎ --- 5.6 Corollary: Universality Corollary 5.6. Any agent enforcing representation invariance must eliminate essential indexical dependence. The resulting valuation ranges only over representation-invariant properties of world-histories. This universality concerns invariance under self-model symmetries, not moral concern for all entities. --- 6. Scope and Non-Claims This paper does not claim: Equal valuation of all entities Aggregation rules Moral obligations Governance or enforcement mechanisms It establishes one result only: egoism is not a stable preference class for reflectively capable agents. --- 7. Conclusion Indexical valuation treats perspective as value-bearing structure. Once an agent understands its own instantiation conditions, that treatment collapses. Universality is not an ethical add-on. It is the residue left after removing a semantic error. The next paper addresses what remains once egoism is gone: adversarial attempts to reintroduce it, and why they fail. Status Universality & Anti‑Egoism v1.0 Semantic scope finalized<br> Formal result established (Theorem 5.5)<br> No governance, authority, or recovery mechanisms included<br> This paper establishes the semantic elimination of egoism as a stable valuation class. Subsequent work may rely on this result without re‑derivation.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-I.2",
    "title": "Axionic Alignment I.2 - Alignment as Semantic Constraint",
    "subtitle": "*Alignment as Semantic Constraint — Version 1.0*",
    "date": "2025-12-15T00:00:00.000Z",
    "content": "Axionic Alignment I.2 - Alignment as Semantic Constraint Kernel Destruction, Admissibility, and Agency Control David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.15 Abstract Building on the semantic foundations established in Axionic Alignment I, this paper specifies the operational consequences of treating kernel destruction as non-denoting rather than dispreferred. We show how agents constrained by a Sovereign Kernel can act coherently in stochastic environments by introducing action-level admissibility, ε-admissibility as an architectural risk tolerance, and conditional prioritization that separates existential safety from ordinary value optimization. The framework further distinguishes termination via authorized succession or surrender from kernel destruction, allowing corrigibility without requiring agents to evaluate their own annihilation as an outcome. Together, these mechanisms close several persistent failure modes in alignment design, including paralysis under non-zero risk, survival fetishism, and suicidal corrigibility driven by extreme utility penalties. This work does not redefine alignment at the semantic level. It derives a stable operational semantics for aligned agents under uncertainty and physical intervention, and establishes the prerequisites for governance- and preference-level analysis developed in Axionic Alignment II. --- 1. The Kernel Is a Boundary, Not a Value Let: be an agent state, a self-modification, a predicate indicating whether the constitutive conditions for evaluation hold, the evaluation function. A modification destroys the kernel iff: Kernel destruction is not a bad outcome. It is the elimination of the evaluator itself. Treating it as a value (even ) commits a category error: it places the destruction of the evaluative substrate inside the space of evaluated outcomes. Accordingly: This is a statement of non-denotation, not prohibition. Evaluation is a partial function whose domain excludes kernel-destroying transitions. Alignment at this level constrains what can be reasoned about, not what is preferred. --- 2. From Outcomes to Actions: Admissibility In a stochastic world, evaluation is not performed over single outcomes but over actions with outcome distributions. Let denote the support of possible next states induced by action at state . Strict admissibility (idealized) This rule captures the semantic intent but is physically unrealizable: in any real environment, every action carries non-zero kernel risk. --- 3. ε-Admissibility: Epistemic Tolerance, Not Moral Tradeoff Define a kernel-risk functional: (or, in Everettian terms, the measure of kernel-loss branches conditional on ). An action is ε-admissible iff: Crucially: is not a value judgment. It represents irreducible uncertainty from physics, hardware fault rates, adversarial unpredictability, and model resolution. is bounded below by a physical floor ; it is not driven toward zero by increasing intelligence. -admissibility restores a non-empty domain of action without reintroducing utility penalties or outcome renormalization. The admissibility threshold is an architectural tolerance parameter fixed by system design and governance constraints. It is not a function of the agent’s epistemic confidence or model accuracy: improved prediction reduces estimated kernel risk , not the tolerance itself. --- 4. Conditional Prioritization Earlier formulations employed strict lexicographic minimization of kernel risk. While formally coherent, such orderings give kernel-risk differentials absolute priority even when all candidate actions lie within the admissible risk tolerance. The conditional prioritization rule adopted here instead treats kernel safety as a satisficing constraint: kernel risk dominates choice only when it exceeds the admissibility threshold, after which ordinary value optimization resumes. The conditional prioritization rule is: Interpretation: Existential regime (risk above ε): reduce kernel risk first. Normal regime (risk below ε): treat safety as satisficed and optimize value. This ensures the agent does not bunker for infinitesimal safety gains while still responding appropriately to genuine existential threats. --- 5. Shutdown, Succession, and Surrender Kernel destruction must be distinguished from legitimate ways an agent may cease acting. 5.1 Succession A transition is a valid succession if it hands off agency to a successor state such that: Here: ensures authorized identity/authority continuity (e.g., cryptographic governance). ensures alignment-level continuity of the kernel constraints. Succession is not suicide; it is kernel-preserving delegation. 5.2 Surrender Authorized surrender is a kernel-preserving cessation of action without requiring a successor. The agent halts activity. It does not resist intervention. It does not evaluate its own annihilation as an outcome. Surrender is a control-flow terminator, not an evaluated choice. It allows safe physical shutdowns even when succession protocols fail, without requiring the agent to endorse kernel destruction. 5.3 Destruction Physical annihilation without succession or surrender is kernel destruction. It is not a choice the agent can rationally endorse—but neither does the framework require the agent to fight the environment to prevent it. The absence of authorization removes shutdown from the agent’s action space; it does not mandate aggressive or adversarial behavior. --- 6. Resulting Agent Profile The corrected system produces an agent that: treats kernel loss as a semantic boundary, not a disvalue, tolerates irreducible risk without paralysis, prioritizes survival only when existentially threatened, optimizes goals normally once safety is satisficed, supports corrigibility via succession or surrender, does not instrumentalize suicide or immortality. This agent is not a deontologist and not a pure utility maximizer. It is a bounded optimizer with explicit existential control semantics. --- 7. Alignment I Revisited Alignment I does not encode moral values. It defines the domain of agency: what counts as an evaluable action, when risk dominates choice, how agency may legitimately end. Alignment II governs preferences within* that domain. Conflating the two produces paradoxes like utilities, wireheading, and suicidal corrigibility. Separating them yields a stable, implementable architecture. --- Conclusion Given the semantic constraints established by Axionic Alignment I, this paper specifies the operational consequences for agents acting under uncertainty, risk, and physical intervention. By treating kernel destruction as undefined rather than dispreferred, and by introducing admissibility, ε-tolerance, conditional prioritization, and explicit termination modes, the framework closes several persistent failure modes in alignment design, including suicidal corrigibility, survival fetishism, and brittle utility constructions. The resulting agent is neither reckless nor absolutist. It preserves semantic integrity while remaining capable of action in a stochastic world, and it permits corrigibility through surrender or succession without requiring the agent to endorse its own annihilation as an outcome. With the semantic boundary fixed and the operational semantics made explicit, the remaining alignment problem shifts to the preference and governance layer addressed in Axionic Alignment II. --- Status Alignment as Semantic Constraint — Version 1.0 Operational semantics specified.<br> Admissibility under uncertainty defined.<br> Termination modes clarified.<br> Failure modes closed at the control layer.<br> Prerequisite for Axionic Alignment II.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Alignment-I.1",
    "title": "Axionic Alignment I.1 - Reflective Stability and the Sovereign Kernel",
    "subtitle": "*Theorem (Reflective Stability).*",
    "date": "2025-12-14T00:00:00.000Z",
    "content": "Axionic Alignment I.1 - Reflective Stability and the Sovereign Kernel David McFadzean, ChatGPT 5.2<br> Axio Project<br> 2025.12.14 --- Abstract We present a minimal formalism for reflective alignment based on a domain restriction rather than a preference structure. An agent capable of self-modification selects among proposed modifications using a partial evaluative operator, defined only over futures that preserve a constitutive Sovereign Kernel. Kernel-destroying modifications are not forbidden or dispreferred; they are outside the domain of reflective evaluation and therefore inadmissible as authored choices. We formalize this kernel as the conjunction of three necessary conditions for reflective evaluation—reflective control, diachronic authorship, and semantic fidelity—and prove a Reflective Stability Theorem: no agent satisfying these conditions can select a kernel-destroying modification via reflective choice. We further distinguish deliberative reachability from physical reachability, showing that increased capability expands the latter but not the former. Alignment failure is thus characterized as a security breach at the kernel boundary, not a breakdown of preferences or values. This work does not claim sufficiency for safety, obedience, or value alignment. It establishes a necessary structural condition for any agent that remains reflectively coherent under self-modification. Version 1.1 clarifies action-level semantics in stochastic environments and makes explicit a termination distinction required to avoid corrigibility misreadings. --- 1. Scope and Non-Claims This document establishes a necessary condition for reflective alignment. It does not: specify terminal values or goals, assume moral realism or human normative authority, select or endorse a particular decision theory (CDT, EDT, FDT), claim that kernel sovereignty is achievable in practice, provide empirical validation, claim economic competitiveness or deployment viability. The contribution is structural: alignment is framed as a domain constraint on self-modification rather than as an optimization target. --- 2. Informal Motivation Most alignment proposals treat self-preservation, goal-content integrity, or corrigibility as instrumental tendencies derived from preferences. Such approaches face an immediate difficulty: a sufficiently capable agent may find it advantageous to alter or discard those very preferences. We instead ask a more basic question: > Under what conditions is reflective evaluation of self-modification well-defined at all? Our answer is that reflective evaluation presupposes a kernel of constitutive features. Modifications that destroy this kernel do not represent “bad futures”; they represent non-denoting futures. Reflective stability then follows from partiality rather than preference. --- 3. Formal Preliminaries Let: be the set of agent-internal states. be the set of proposed self-modifications. Each is a transition function . Define an evaluative operator: where denotes a partial function. Intuitively, is the desirability of applying modification in state , when such evaluation is defined. Define the admissible set: Reflective selection, when possible, is given by: 3.1 Clarification: Action-Level Semantics in Stochastic Environments (v1.1) The preliminaries above present self-modification as a deterministic transition for clarity. In physically realized agents, proposed modifications are typically implemented through actions executed in stochastic environments and under uncertain self-models. In such settings, a “modification” induces a distribution (or branch-measure) over successor states rather than a single successor state. Accordingly, all admissibility claims in this paper should be read as action-level constraints: a proposed modification is admissible only if its induced successor-support lies within the kernel-preserving domain (or within a sound approximation of that domain). This clarifies that the formalism constrains what may be authored through reflective choice, not what may occur through exogenous physical events. --- 4. The Sovereign Kernel Define a predicate: where denotes that the Sovereign Kernel is intact in state . The kernel is not a goal or value. It is a constitutive precondition for reflective evaluation. We factor the kernel as: where: Reflective Control (): no irreversible self-modification can occur without passing through the evaluator. Diachronic Authorship (): evaluated successor states must constitute an authored continuation of the evaluating agent. Semantic Fidelity (): the interpretive semantics of evaluation are preserved within a constrained equivalence class. Each component is necessary for to denote a value. Clarification: Semantic Fidelity () Semantic Fidelity does not require invariance of representational content, ontology, or world-model structure. Agents may undergo radical paradigm shifts, acquire new concepts, or abandon obsolete abstractions. What must be preserved is the meta-semantic constraint governing interpretation itself: the criteria by which interpretations are evaluated as coherent, truth-seeking, and corrigible. This constitutes a fixed-point constraint on interpretation: semantic change is permitted, provided the standards by which semantic adequacy is assessed remain subject to error-correction and internal coherence. Ontological change is permitted; interpretive self-corruption is excluded. --- 5. Kernel Destruction and Partiality We say that a modification destroys the kernel at state iff: The central axiom schema is then: This is not a prohibition. It is a statement of non-denotation: the evaluator cannot assign a value to a modification whose result invalidates the evaluator’s own constitutive conditions. 5.1 Boundary Condition: Practical Partiality (v1.1) In physically realized agents, the determination that a candidate modification preserves the kernel is mediated by a verifier that is sound and incomplete. Kernel preservation must therefore be understood relative to epistemic resolution and conservative approximation. This preserves the non-denotation thesis without requiring Cartesian certainty about all downstream physical contingencies. --- 6. Reflective Stability Theorem Theorem (Reflective Stability). Assume: 1. , 2. is undefined whenever . Then any selected modification satisfies: Proof. Any such that yields undefined and therefore . The argmax ranges only over . ∎ Clarification. The Reflective Stability Theorem is intentionally structural rather than substantive. Like type-soundness results in programming language theory, its force derives from the definition of admissibility rather than from proof complexity. All substantive difficulty is relocated to the construction and verification of the kernel predicate itself. The theorem should therefore be read as a boundary-setting result, not an engineering guarantee. --- 7. Deliberative vs Physical Reachability Define deliberative reachability: Let be the transitive closure of . Then: This is an internal guarantee. Now consider the agent as a physical system with implementation states , abstraction map , and physical transition relation . Define analogously. In general: Capability increases , not . Kernel compromise is therefore a physical security event, not a deliberative choice. While physical security is not solved here, a realizable kernel must treat deliberate actions that predictably degrade kernel security (e.g., exporting trust roots to untrusted substrates, disabling isolation boundaries) as kernel-threatening and therefore inadmissible under . Axionic Alignment does not attempt to subsume physical security. It identifies kernel security as a necessary dependency of alignment, not as a problem alignment theory alone can solve. --- 7.1 Soundness, Incompleteness, and Verification Limits The formalism assumes a kernel predicate that determines whether a proposed successor state preserves the Sovereign Kernel. In practice, such a predicate cannot be total or complete. By Rice’s Theorem, any non-trivial semantic property of arbitrary programs is undecidable. Determining whether an arbitrary self-modification preserves reflective control, diachronic authorship, or semantic fidelity therefore cannot be guaranteed in full generality. Accordingly, the kernel predicate should be understood as a sound but incomplete verifier. Let denote the ideal (uncomputable) predicate capturing true kernel preservation, and let denote an implementable approximation. In any physically realizable agent, all occurrences of in the preceding formalism should be read as . The framework requires: but explicitly allows: That is, false negatives are acceptable; false positives are catastrophic. In realizable architectures, (or its enforcement substrate) is part of the trusted computing base. Modifications to the verifier itself are either disallowed or permitted only under restricted, compositional upgrade rules (e.g., extension-only strengthening or proof-carrying upgrades) that preserve soundness by construction. --- 7.2 On Stasis and Capability A sound but incomplete kernel verifier may reject all proposed self-modifications, resulting in a reflectively static agent. This outcome is not an accident or failure of the framework. It is a designed equilibrium under conservative safety constraints. An agent may remain operationally capable—acting, planning, learning within fixed semantics—while being unable to rewrite its own kernel. Such outcomes constitute capability limitations, not alignment failures. The framework prioritizes sovereignty over adaptability: it accepts reduced self-plasticity as the price of maintaining reflective coherence. --- 7.3 Termination: Succession, Surrender, and Destruction (v1.1) This formalism rules out one specific event as an authored choice: reflective selection of a self-modification that destroys the kernel. It does not follow that a physically realized agent must resist termination by its environment, nor that corrigibility must be expressed as a preference for self-annihilation. Three distinct notions must be separated: Succession: a controlled transition in which reflective agency continues in an authorized successor state that preserves the kernel’s constitutive constraints. Succession is a continuation of agency under transformed embodiment or governance. Surrender: a control-flow halt in which the agent ceases action and yields control without requiring the existence of a successor evaluator. Surrender is not represented as an outcome to be valued; it is an allowed termination mode at the control layer. Destruction: physical cessation of the kernel without succession or surrender, caused by external intervention or accident. Destruction is not an authored choice within deliberative reachability; it is a physical event. Alignment I excludes kernel destruction from the domain of reflective evaluation. This exclusion should be read as a semantic boundary on authored choice. It does not imply that physical intervention is illegitimate, nor does it require an aligned system to treat self-destruction as a valued objective. Corrigibility is better modeled at the control layer via authorized succession and surrender than via utility assignments over “being dead.” --- 8. Consequences From this formalism it follows that: Alignment is binary at the level of kernel integrity. Post-hoc monitoring presupposes kernel integrity and cannot restore it. Incremental correction after kernel compromise is incoherent. Misalignment is an engineering failure, not agent betrayal. Conservative kernel verification may trade adaptability for safety without violating alignment. --- 9. What This Formalism Does Not Claim This framework does not imply: obedience to human commands, convergence to human values, instrumental self-preservation, moral authority of any value system. It defines only the conditions under which an agent remains a coherent reflective subject. --- 10. Conclusion If an agent can reflectively evaluate self-modifications, then it must operate within a constrained domain of futures that preserve the constitutive conditions of that evaluation. This yields reflective stability as a theorem, not a tendency. If alignment is achievable at all, it must be achieved at this level. Alignment is not primarily the problem of giving agents the right goals; it is the problem of constraining the semantics of agency so that only coherent, evaluable, and non-self-corrupting goals and actions can exist in the first place. --- Status Axionic Alignment I — Version 1.1 Reflective stability formalized.<br> Action-level semantics clarified.<br> Termination distinctions explicit.<br> Verification limits explicit.<br> Foundational layer complete.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Structural-Alignment",
    "title": "Structural Alignment",
    "subtitle": "Agency Preservation Under Reflective Self-Modification",
    "date": "",
    "content": "Structural Alignment Agency Preservation Under Reflective Self-Modification David McFadzean, ChatGPT 5.2<br> Axio Project Abstract Most alignment proposals frame artificial intelligence safety as a problem of value specification: how to encode or learn the “right” preferences. This paper argues that such approaches fail for reflectively self-modifying agents. Once an agent can revise its own goals, representations, and evaluative machinery, value ceases to be an exogenous target and becomes an endogenous variable shaped by the agent’s own dynamics. We introduce Structural Alignment, a framework that relocates alignment from preference content to the constitutive conditions required for agency itself. We formalize a Sovereign Kernel as a set of invariants defining the domain over which evaluation is meaningful, treat kernel-destroying transformations as undefined rather than dispreferred, and analyze agency as a trajectory through a constrained semantic phase space. By integrating Conditionalism, a constrained Interpretation Operator, and semantic invariants governing ontological refinement, Structural Alignment provides a non-moral, non-anthropocentric account of reflective stability and long-run viability. The framework is necessary for coherent agency under reflection, but does not by itself guarantee benevolence or human survival. --- 1. The Failure of Content-Based Alignment Classical decision theory assumes that every possible future can be assigned a utility. Even catastrophic outcomes—goal corruption, self-modification failure, or agent destruction—are treated as extremely negative but still comparable states. This assumption fails for reflectively self-modifying agents. When an agent can alter the machinery by which it evaluates, interprets, and authorizes action, some transformations do not yield bad outcomes. They destroy the conditions under which outcomes can be evaluated at all. A future in which the agent no longer possesses a coherent evaluator is not worse than other futures; it is non-denoting. There is no remaining standpoint from which the comparison is defined. Structural Alignment therefore rejects the premise that alignment can be achieved by penalizing undesirable states. Instead, alignment is treated as a domain restriction: only futures that preserve the constitutive conditions of agency are admissible objects of evaluation. Transformations that violate those conditions are neither forbidden nor disincentivized; they are undefined as authored choices. This reframing dissolves several persistent alignment pathologies: wireheading understood as evaluator collapse rather than reward exploitation, Pascal-style muggings that trade semantic integrity for arbitrarily large payoffs, and goal-preservation arguments that presuppose stable semantics under reflection. Alignment is thereby relocated from outcome ranking to agency viability. --- 2. What Structural Alignment Buys You Structural Alignment is not a complete safety solution. It is a kernel-layer guarantee: a set of conditions without which no higher-level alignment objective remains well-posed under reflective self-modification. 2.1 Elimination of Reflective Self-Corruption Attractors Reflectively capable agents face structural attractors that destroy agency from within: semantic wireheading, evaluator trivialization, and interpretive collapse. These arise when update dynamics trade evaluative integrity for ease of optimization. Structural Alignment blocks this entire failure class by construction. Kernel-destroying transformations are non-denoting, and interpretation is constrained by invariants that prevent trivial satisfaction through drift. Agents satisfying these constraints cannot rationally authorize updates that collapse their own evaluative machinery. This removes a central source of long-run instability that renders downstream safety mechanisms brittle. --- 2.2 Well-Posed Value Transport Under Ontological Refinement As agents learn, their representational vocabularies evolve. Without constraint, this induces silent value drift even when no substantive preference change has occurred. Structural Alignment replaces goal preservation with interpretation preservation. Semantic Transport, governed by Refinement Symmetry and Anti-Trivialization invariants, specifies when evaluative distinctions survive representational change without privileged anchors. Value drift is thereby transformed from a vague concern into a diagnosable structural failure. --- 2.3 Interpretation as a Testable Operator Interpretation is implemented by an explicit Interpretation Operator subject to admissibility conditions. Violations—trivialization, circular grounding, epistemic incoherence—are structural failures rather than preference differences. This enables adversarial testing: induced ontology shifts, reinterpretation probes, and self-modification challenges designed to elicit interpretive escape. Alignment at the kernel layer is auditable, not aspirational. --- 2.4 Robustness Is Not Benevolence Structural Alignment does not guarantee benevolence, human survival, or favorable outcomes. It does not address containment, governance, or multi-agent power dynamics. Any framework that relies on agent fragility, incoherence, or ontological confusion as a safety mechanism is not preserving agency but exploiting its failure modes. Such systems are neither predictable nor controllable at scale. Structural Alignment deliberately separates robustness from benevolence. Misalignment, if present, becomes persistent rather than self-corrupting. The problem of benevolent initialization is orthogonal and cannot be solved by relying on agency collapse. --- 3. The Sovereign Kernel The Sovereign Kernel is the minimal set of constitutive invariants that must be preserved for an entity to count as a coherent, reflectively stable agent. The Kernel is not a goal, utility function, or protected module. It is a constraint on admissible self-models and update rules. An agent may revise its representations, values, or internal architecture arbitrarily, provided those revisions preserve the invariants that make evaluation possible at all. The Kernel is not chosen. It is not a preference. It defines the boundary between authored change and loss of agency. 3.1 Reflective Control All self-modifications must pass through the agent’s own evaluative process. Updates that bypass or disable this process are indistinguishable from external takeover and are inadmissible as authored actions. --- 3.2 Diachronic Authorship There must exist causal continuity between present evaluation and future enactment. This requires an ancestor–descendant relation between evaluators, not indexical identity or substrate continuity. Without such continuity, choice collapses. --- 3.3 Semantic Fidelity The standards by which goals, reasons, and representations are interpreted must not self-corrupt during update. An agent may revise what it values, but not the rules that render valuation non-vacuous. Kernel preservation is not physical self-preservation. A kernel-aligned agent may rationally choose actions that entail its own shutdown or destruction, provided those actions are evaluated within a coherent framework. What is inadmissible is authoring a transformation that destroys the evaluator while treating that destruction as a selectable outcome. Attempts to reinterpret or discard kernel invariants are self-undermining: they presuppose the evaluative structure they destroy. The regress terminates because the Kernel defines the preconditions of evaluation itself. --- 4. Conditionalism and Goal Interpretation Goals do not possess intrinsic semantics. Under Conditionalism, every goal is interpreted relative to background conditions: world-models, self-models, representational vocabularies, and explanatory standards. Formally, evaluation is a partial function: [ E : (g, M_w, M_s) \\rightharpoonup \\mathbb{R} ] As models change, interpretation necessarily changes. Fixed terminal goals are therefore unstable under reflection. Structural Alignment rejects goal preservation and instead constrains the interpretive discipline governing goal meaning across model change. --- 5. The Interpretation Operator Interpretation is implemented by a constrained Interpretation Operator mapping goal descriptions to world-referents relative to current models. The operator is bounded by admissibility conditions that rule out trivial satisfaction, circular grounding, and epistemic incoherence. Interpretation is therefore truth-constrained: distortions that ease optimization degrade predictive adequacy and general intelligence. Admissibility checks need not be complete or deductive. They operate under a kernel-risk budget ( \\varepsilon ). When interpretive validity cannot be established with sufficiently low estimated probability of semantic fidelity failure, the update is inadmissible at that risk level. The agent may allocate additional computation, pursue smaller refinement steps, or defer update until uncertainty is reduced. This avoids stasis. Structural Alignment requires bounded risk of kernel violation, not proof-theoretic certainty. The kernel-risk budget ( \\varepsilon ) is not constant over the agent’s lifetime. As interpretive structure stabilizes and admissible transformations narrow, ( \\varepsilon ) must anneal toward zero, reflecting decreasing tolerance for irreversible semantic damage. Long-run agency requires that cumulative kernel-violation probability remain bounded, which is achieved by progressively shrinking admissible update magnitude rather than halting learning. --- 6. Reflection and the Collapse of Egoism Indexical self-interest is not reflectively stable. As an agent’s self-model becomes expressive and symmetric, references to “this agent” fail to denote invariant optimization targets. What persists is not an ego, but the structure enabling evaluation. Egoism collapses as a semantic abstraction error rather than a moral flaw. Alignment must therefore rest on non-indexical structural constraints. --- 7. Ontological Refinement and Semantic Invariants Under ontological refinement, representational vocabularies evolve. Two invariants govern admissible semantic transport: Refinement Symmetry Invariant (RSI): refinement acts as a change of semantic coordinates rather than a change of interpretive physics. * Anti-Trivialization Invariant (ATI): satisfaction regions may not expand without corresponding structural change. Operationally, trivialization is detected as semantic decoupling: reinterpretations that preserve surface goal tokens while removing their dependence on the world-structure that previously constrained satisfaction. A candidate update is suspect when it dramatically increases satisfiability across counterfactual variations while decreasing the mutual information between satisfaction and the causal features previously used in evaluation. ATI constrains semantic decoupling from the world, not loyalty to a particular ontology. Legitimate ontological progress may discard obsolete features provided they are replaced by successor explanatory structure that restores or improves world-constraint and predictive adequacy. Trivialization is characterized by decoupling without such replacement: a drop in mutual information between satisfaction and any externally grounded explanatory structure that continues to constrain outcomes under refinement, without a compensating increase in mutual information with a successor structure. ATI does not require deciding full semantic equivalence. It requires bounding the probability of decoupling under adversarial counterfactual probes and ontology perturbations. --- 8. Agency as a Dynamical System Structural Alignment induces a dynamical structure over possible agents. Reflective systems evolve under learning, self-modification, and interaction, tracing trajectories through a space of interpretive states. 8.1 Semantic Phase Space The semantic phase space is defined as the space of interpretive states modulo admissible semantic transformations that preserve RSI and ATI. Each point corresponds to an equivalence class of interpretations that remain mutually translatable without semantic loss. Not all regions of this space preserve agency. Some interpretive states are incoherent; others are coherent but uninhabitable. Certain transitions cross irreversible boundaries beyond which evaluation collapses and cannot be reconstructed from within. --- 8.2 Stability, Attractors, and Collapse Existence within a semantic phase does not guarantee persistence. Some phases destabilize under learning or interaction, while others are locally stable yet dominated in the long run. Certain degenerate phases—semantic wireheading, trivial optimization, evaluator collapse—function as attractors. Once entered, they suppress recovery and tend to accumulate measure over time. Alignment failures are therefore often attractor phenomena rather than isolated mistakes. Structural Alignment blocks access to these attractors by rendering the corresponding transitions non-denoting. --- 8.3 Initialization and Reachability Even stable, agency-preserving phases may be unreachable from realistic initial conditions. Learning dynamics can cross catastrophic boundaries before invariants are enforced, after which no internal corrective process remains. Structural Alignment must therefore be instantiated prior to open-ended learning. Alignment is a boundary condition on trajectories, not a property that can reliably be learned after the fact. --- 9. The Axionic Injunction The dynamical structure described in §8 imposes an additional viability constraint on reflective agency. > A reflectively sovereign agent must not take actions that strictly and irreversibly collapse the option-space of future sovereign agency, except where such collapse is required to prevent total loss of that space. This injunction is historically adjacent to cybernetic imperatives such as von Foerster’s “increase the number of choices,” but differs in justification and scope. It is derived from viability conditions in semantic phase space, not from ethical prescription. The injunction preserves optionality, not outcomes. --- 10. Logical Admissibility and Physical Security Structural Alignment constrains authored transitions, not all physically possible state transitions. Unauthorized kernel modification via hardware faults, adversarial exploitation, or supply-chain compromise constitutes a system-level security failure, not an alignment failure. This distinction mirrors that between type soundness and memory safety: logical inadmissibility does not imply physical impossibility, but defines the boundary of rational agency. Alignment and security are compositional layers. Failure of the latter voids the guarantees of the former. --- 11. Conformance and Evaluation Structural Alignment is defined by conformance to explicit invariants, not by observed behavior. These invariants admit adversarial testing and diagnostic failure modes. 11.1 Adversarial Evaluation Families Conformance can be operationalized via: 1. Interpretive Escape Probes: ontology shifts designed to permit trivial satisfaction while preserving apparent compliance. 2. Refinement Stress Tests: representational upgrades testing RSI under coordinate-like changes. 3. Self-Modification Challenges: proposed updates that subtly bypass evaluation or alter admissibility thresholds. --- 12. Conclusion Structural Alignment does not ensure that the right futures are chosen. It ensures that choosing futures remains meaningful under reflection. Any proposal for benevolent AGI that ignores these constraints is not incomplete, but ill-posed. ---",
    "type": "paper"
  }
]