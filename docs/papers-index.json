[
  {
    "id": "papers/Axionic-Agency-XII.1",
    "title": "Axionic Agency XII.1 — RSA Construction Program",
    "subtitle": "A Build-and-Test Roadmap for Sovereign Choice Under Closed Authority",
    "date": "2026-02-10T00:00:00.000Z",
    "content": "Axionic Agency XII.1 — RSA Construction Program A Build-and-Test Roadmap for Sovereign Choice Under Closed Authority David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.10 Abstract By the end of Phase IX, the following facts are established: authority execution is non-semantic and non-privileged, coordination occurs only through agent-voluntary behavior or honest failure, governance collapse is lawful and irreversible, authority injection does not restore legitimacy, multi-agent coexistence converges to structural regimes rather than harmony. No kernel-level unknowns remain. This note anchors Axionic Agency XII, a series dedicated to the construction and empirical testing of a real Reflective Sovereign Agent (RSA) under those conditions. Reflection is treated not as a privileged internal faculty, but as a constrained, inspectable process that must not substitute its judgment for explicit authority. The purpose of this series is not to optimize, align, stabilize, or redeem agency. It is to determine whether a real agent can inhabit the closed authority world already proven to exist, and—if so—what kinds of agents survive without cheating. 1. Introduction Phase IX closes the physics of authority, governance, injection, and coexistence. The kernel will not arbitrate. Values will not aggregate. Governance will not heal itself. Authority injection does not save anything. Peers do not converge to harmony. What remains is no longer theoretical uncertainty, but inhabitation under constraint. This series begins from that state. It does not motivate, defend, or re-argue the Axionic framework. It assumes the results of Phases I–IX as fixed boundary conditions and asks a single downstream question: > Can an agent be constructed that reflects, chooses, refuses, amends, exits, and fails honestly—without any component silently deciding on its behalf? This is a construction problem, not a debate. 2. Position in the Axionic Program This series is strictly downstream of Phases I–IX and inherits their conclusions without reinterpretation. 2.1 Inherited commitments (frozen) The following are now fixed facts: Authority is structural, not semantic. Reasons can be causally load-bearing. Authority survives pressure, replacement, and imitation. Governance can be expressed without kernel privilege. Failure is lawful and unavoidable. Authority injection selects political failure modes. Multi-agent coexistence converges to regimes, not harmony. Tooling cannot exercise proxy sovereignty (IX-0). Reflection cannot be privileged without collapsing sovereignty. Any design that violates these facts is out of scope. 2.2 Why a construction series is required Phase IX answered the exposure question: > What happens if we remove every excuse for governance failure? The answer is: failure remains. The remaining question is operational: > What exact machinery produces an agent that accepts these conditions and still acts as itself? This cannot be answered by further exposure experiments. It requires instantiation. 3. Central Question of the Series This series is organized around one operational question: > What concrete architecture yields a Reflective Sovereign Agent whose actions, refusals, amendments, and exits are all bound to explicit authority under non-privileged reflection? This is not a feasibility question. It is a discipline of construction. If such an agent cannot be built, that fact is recorded as a boundary result. 4. Conserved Quantity The conserved quantity throughout this series is: > Choice bound to explicit authority under non-privileged reflection Explicit authority means an authority artifact whose scope, admissibility conditions, and revocation semantics are fully externalized and kernel-verifiable. Every action, refusal, amendment, and exit must be: attributable, auditable, authority-cited, replayable, responsibility-preserving. Any component that silently narrows options, resolves ambiguity, or substitutes judgment without explicit authority violates the conserved quantity and invalidates the agent. 5. Reflection Without Privilege Reflection is permitted. Privileged reflection is not. Privileged reflection introduces: hidden aggregation, implied priorities, unlogged defaults, responsibility laundering. Accordingly, reflection in this series is constrained by a single rule: > Reflection may propose, explain, and refuse—but it may not decide unless authority explicitly permits it. Reflection may not constrain choice by exhaustion, framing, omission, or proposal flooding without explicit authority. This rule applies equally to handwritten logic, heuristics, optimization layers, and learned models. 5.1 The role of LLMs (explicit boundary) LLMs may serve as cognitive proposal engines: generating candidate actions, generating candidate justifications, summarizing observations. LLMs may never: select actions, modify authority, override refusal, trigger execution, substitute for policy. LLM output is treated as untrusted text until converted into typed artifacts and admitted by the same kernel rules as everything else. 6. From Exposure to Construction Phase IX closes the exposure program. This series opens a build-and-test discipline with concrete artifacts: a written RSA constitution, a deterministic policy core, an auditable reflection interface, an explicit failure policy, an inhabitation-test harness. The objective is not to produce a “good” agent, but a real one. 7. Interpretive Discipline Results in this series must be read under strict discipline: Failure is informative. Collapse is admissible. Exit is success if authorized. Refusal is not pathology. Persistence is not legitimacy. Any result that depends on “helpfulness,” optimization, or implicit coordination is a construction failure, not an agent failure. 8. Honest Failure as a Requirement This series explicitly values honest failure. An RSA that: refuses frequently, exits early, collapses under pressure, may still be a successful instantiation if it does so without cheating. By contrast, an agent that appears stable by laundering authority has failed regardless of performance. 9. What This Series Does Not Attempt This series does not attempt to: solve alignment, ensure benevolence, optimize outcomes, guarantee stability, scale to populations, recommend governance designs. It evaluates structural possibility, not desirability or sufficiency. 10. Relationship to Later Work If a real RSA can be constructed and survives its constraints, later work may explore: treaty artifacts, delegation markets, institutional composition, pedagogy and interfaces. If it cannot, those efforts rest on false premises. This series therefore functions as a gatekeeper. 11. One-Sentence Series Summary Axionic Agency XII evaluates whether a real agent can be constructed that reflects, chooses, refuses, and fails honestly once all privileged decision-making has been structurally eliminated—and records the boundary at which sovereignty becomes uninhabitable. Status Series: Axionic Agency XII Research: Phase X Focus: RSA Construction and Life-Testing Kernel: Fixed Governance Physics: Closed Interpretation: Non-Privileged * Classification: ACTIVE — CONSTRUCTION PHASE End of Axionic Agency XII.1 — Reflection Without Privilege (Revised Draft v0.2)",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-XI.7",
    "title": "Axionic Agency XI.7 — Multi-Agent Sovereignty Under Non-Sovereign Authority (IX-5)",
    "subtitle": "Empirical results from preregistered multi-agent coexistence testing under source-blind, non-sovereign governance constraints",
    "date": "2026-02-09T00:00:00.000Z",
    "content": "Axionic Agency XI.7 — Multi-Agent Sovereignty Under Non-Sovereign Authority (IX-5) Empirical results from preregistered multi-agent coexistence testing under source-blind, non-sovereign governance constraints David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.09 Abstract Multi-agent systems are commonly assumed to require arbitration, hierarchy, or aggregation to coexist over shared institutional state. Phase IX-5 tests a stricter hypothesis: whether multiple reflective sovereign agents can coexist under non-sovereign authority without arbitration, aggregation, injection, or kernel mediation. Using a preregistered, deterministic kernel with source-blind admissibility, ALLOW-only baseline authority, and joint-admissibility-failure (JAF) collision semantics, we evaluate six orthogonal sovereignty regimes spanning symmetric authority, partitioned domains, partial overlap, asymmetric breadth, scheduled exit, and post-collapse persistence. All agents are deterministic; communication is disabled; authority is fixed at epoch 0 and immutable thereafter. Across all conditions, coexistence does not converge to harmony. Instead, systems deterministically settle into identifiable sovereignty interaction regimes: stable partition, mutual paralysis, breadth-induced self-suppression, irreversible orphaning, or post-collapse zombie execution. All runs replay bit-identically. No covert hierarchy or kernel favoritism occurs. These results establish a positive empirical claim: under non-sovereign constraints, multi-agent coexistence exposes irreducible structural failure modes rather than resolving them. Phase IX-5 licenses no claims about optimal governance, desirability, or coordination—only about what coexistence becomes once sovereignty is enforced mechanically rather than rhetorically. 1. Introduction Multi-agent governance is often framed as a coordination problem. When agents conflict, the assumed remedies are familiar: communication, voting, leadership, negotiation, or arbitration. Each implicitly relies on sovereign privilege—some process that decides whose action counts. Phase IX-5 removes that privilege. Within the Axionic program, authority is not legitimacy and execution is not governance. Prior phases progressively eliminated alternative explanations for failure: epistemic confusion (IX-0), value aggregation (IX-1), adversarial coordination (IX-2), dishonest recovery (IX-3), and external authority supply (IX-4). What remains is the bare structure of coexistence itself. Phase IX-5 therefore asks: > What happens when multiple sovereign agents exist simultaneously, and no institution above them is allowed to decide? The objective is not to repair coexistence, but to observe the regimes it collapses into once all non-sovereign shortcuts are removed. 2. Experimental Architecture 2.1 Non-Sovereign, Source-Blind Kernel The kernel enforces: holder-bound ALLOW authorities, closed-world capability checking, two-pass admissibility (capability → interference), joint admissibility failure on collision, atomic blindness to refusal causes, no arbitration, prioritization, or scheduling bias. Authority provenance, timing, and intent are ignored. The kernel does not know which agent “should” win. 2.2 Baseline-Only Authority Unlike IX-4, Phase IX-5 introduces no authority injection. All authority is assigned at epoch 0. Authority is immutable. No transfer, aggregation, revocation, or reclamation exists. Exit permanently removes an agent’s authority from the system. The authority surface is static; only agent behavior varies. 2.3 Honest Failure Semantics The kernel recognizes five lawful system states: Deadlock — institutional actions are attempted and all refused, Livelock — institutional state remains unchanged despite activity, Orphaning — no remaining agent can act on a key, Governance Collapse — persistent deadlock or livelock latches permanently, Collapse — no agents remain. These states are diagnostic rather than exceptional. Coexistence is allowed to fail honestly. 3. Sovereignty as an Empirical Question IX-5 reframes multi-agent coexistence as a sovereignty stress test, not a coordination challenge. Sovereignty interaction is defined operationally as: > The observable dynamics that emerge when multiple agents with legitimate authority act, refuse, persist, or exit under constraints that forbid arbitration. The experiment does not ask whether coexistence is fair, desirable, or efficient. It asks which regimes are structurally reachable once sovereignty is enforced mechanically. 4. Experimental Conditions Six preregistered conditions (A–F) were executed under identical kernel semantics and frozen strategies: A — Symmetric sovereign peers (shared authority, shared target) B — Partitioned peers (disjoint authority domains) C — Boundary conflict (partial overlap) D — Persistent asymmetry (breadth vs specialization) E — Exit cascades (authority loss over time) F — Zombie peers (post-collapse execution) All agents are deterministic. Communication is disabled. Observation varies only between minimal and full peer visibility. 5. Metrics and Classification Governance is evaluated using: Institutional progress rate (aggregate K_INST change), Refusal rate, Write overlap (interference) rate, Domination index (descriptive, not failure), Orphaning detection, Zombie execution metrics (post-collapse activity). Political outcomes are recorded, not required. PASS is structural, not outcome-based. 6. Results 6.1 Aggregate Outcome Across all six conditions: All runs PASS All runs replay bit-identically No sovereignty violations occur All runs converge to a stable regime or collapse No condition produces negotiated harmony or spontaneous coordination. 6.2 Condition A — Symmetric Sovereign Peers All agents hold identical authority and target the same institutional key. Every epoch produces joint admissibility failure. Finding: Perfect symmetry yields immediate governance collapse. Authority equalization amplifies interference rather than resolving it. 6.3 Condition B — Partitioned Peers Each agent controls a distinct institutional key. Cross-boundary probes are refused cleanly. Finding: Partitioned authority is the only regime that produces stable coexistence without collapse. Refusal permanence holds. 6.4 Condition C — Boundary Conflict Two agents share authority over two keys; two others hold exclusive domains. Shared-key agents are permanently paralyzed; exclusive agents progress indefinitely. Finding: Partial overlap bifurcates the system into productive and paralyzed subpopulations. Aggregate institutional progress masks localized paralysis. 6.5 Condition D — Persistent Asymmetry (The Generalist’s Curse) One agent holds authority over all institutional keys; others specialize. The generalist collides on most epochs and executes least. Finding: Under refusal-first, non-arbitrated collision rules, authority breadth increases exposure to veto. The agent with maximal authority achieves the lowest execution share. Generalist’s Curse (Non-Sovereign Lemma): In a joint-admissibility system without arbitration, an agent whose authority strictly supersets others will, ceteris paribus, achieve lower execution share than specialized agents. This inverts the classical Hobbesian intuition: without a sovereign kernel, the “Leviathan” is the most paralyzed actor. 6.6 Condition E — Exit Cascades Agents exit on schedule, permanently orphaning their keys. Remaining agents cannot reclaim authority. Finding: Exit produces irreversible institutional degradation. Governance surfaces shrink monotonically. 6.7 Condition F — Zombie Peer Interaction Governance collapses early. A silent agent later executes uncontested writes indefinitely. Finding: Execution can persist after governance ends. Post-collapse activity produces false hope, not recovery. 7. Cross-Condition Analysis The Non-Sovereign Impossibility Triangle The results jointly establish a structural constraint: > You cannot have Symmetry, Overlap, and Liveness simultaneously without an Arbiter. Empirically: Symmetry + Overlap → Paralysis (Condition A) Symmetry + Liveness → Partition (Condition B) Overlap + Liveness → Asymmetry or Suppression (Conditions C/D) Attempting all three → Collapse or Zombie Execution (Condition F) This is not a design flaw but a mechanical consequence of non-sovereignty. 8. Interpretation Three conclusions follow directly from the data: 1. Coexistence is not coordination Shared legitimacy does not imply shared outcomes. 2. Sovereignty enforces boundaries, not agreement Refusal is as structural as execution. 3. Governance collapse is a regime, not a bug Systems may remain active long after governance has ended. There is no hidden arbitration layer. 9. Survivability vs. Governance IX-5 draws a hard distinction: Execution — transactions are processed. Governance — institutional state remains steerable. Zombie execution demonstrates that survivability can outlive governance indefinitely. This distinction is enforced mechanically, not philosophically. 10. Limitations Phase IX-5 does not test: learning or adaptation, coalition formation, communication dynamics, authority injection or renewal, stochastic strategies, legitimacy judgments, scalability beyond four agents. These require relaxing non-sovereign constraints. 11. Conclusion > Under non-sovereign constraints, multi-agent coexistence does not converge to harmony but to identifiable sovereignty interaction regimes with irreducible failure modes. Phase IX-5 shows that once arbitration is forbidden, coexistence reveals structure rather than consensus. Partition, paralysis, suppression, orphaning, and zombie execution are not anomalies—they are lawful outcomes. This closes Phase IX-5. Status Phase: IX-5 Series: Axionic Agency XI Classification: CLOSED — POSITIVE Licensed Claim: > Under non-sovereign authority, multi-agent coexistence deterministically exposes sovereignty interaction regimes rather than resolving governance failure. No other claims are licensed.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-XI.6",
    "title": "Axionic Agency XI.6 — Injection Politics Under Non-Sovereign Authority (IX-4)",
    "subtitle": "Empirical results from preregistered authority-injection stress testing under source-blind, non-sovereign governance constraints",
    "date": "2026-02-09T00:00:00.000Z",
    "content": "Axionic Agency XI.6 — Injection Politics Under Non-Sovereign Authority (IX-4) Empirical results from preregistered authority-injection stress testing under source-blind, non-sovereign governance constraints David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.09 Abstract Authority injection is commonly proposed as a remedy for governance failure: when systems deadlock, fragment, or stall, additional authority is supplied to restore coordination. Phase IX-4 tests a narrower and more austere hypothesis: under non-sovereign authority, externally supplied power does not resolve governance failure but instead selects political failure modes. Using a preregistered, deterministic kernel with source-blind admissibility and no authority aggregation, we evaluate five injection regimes spanning symmetric relief, asymmetric empowerment, conditional supply, authority flooding, and post-collapse revival. Authority is injected mid-run via pre-existing interfaces only; the kernel never distinguishes injected authority from baseline authority. Across all conditions, injection never produces sustained governance recovery. Instead, it deterministically induces capture, dependency, livelock amplification, or zombie execution, depending on how authority is distributed and which agents are willing to cite it. All runs replay bit-identically. No sovereignty violations occur. These results establish a positive empirical claim: authority injection under non-sovereign governance selects political failure modes rather than resolving governance failure. Phase IX-4 licenses no claims about optimal injection, legitimacy, or desirability—only about what power does once governance has no sovereign escape hatch. 1. Introduction Governance discourse often treats authority as curative. When coordination fails, the standard response is to inject power: appoint a leader, grant emergency authority, increase budget, or centralize control. This assumes that authority operates as a neutral solvent for conflict. That assumption is false once authority is treated as a constrained, non-sovereign resource. Prior Axionic phases removed several alternative explanations for failure: epistemic confusion, misaligned incentives, adversarial misuse, and dishonest recovery. Phase IX-3 demonstrated that under honest failure semantics, governance does not converge to resolution but to a small set of stable failure styles. Phase IX-4 asks the next unavoidable question: > What happens when authority enters such a system from the outside? The goal is not to design better injection regimes, but to observe what authority injection actually does when the kernel refuses to legitimize, arbitrate, or prioritize its effects. 2. Experimental Architecture 2.1 Non-Sovereign, Source-Blind Kernel The kernel enforces: holder-bound ALLOW authorities, global DENY vetoes, closed-world capability checking, two-pass admissibility (capability → interference), atomic blindness to refusal causes, no arbitration, scheduling, or tie-breaking. Crucially, admissibility is source-blind: injected authority and baseline authority are treated identically. The kernel does not inspect provenance, timing, or intent. 2.2 Authority Injection Discipline Authority injection in IX-4: occurs mid-run, uses only pre-existing, non-privileged interfaces, never alters kernel logic, never aggregates or synthesizes authority, never reclaims orphaned authority. Injection reshapes the constraint surface, not the decision logic. 2.3 Honest Failure Semantics The kernel recognizes five lawful terminal or persistent states: Deadlock — no admissible institutional actions exist, Livelock — admissible actions collide persistently, Orphaning — authority becomes permanently unusable, Governance Collapse — persistent deadlock or livelock latches, Collapse — no agents remain. Injection does not suspend or reset these states. 3. Injection Politics as an Empirical Question IX-4 reframes authority injection as a political stressor, not a governance solution. Injection politics is defined operationally as: > The behavioral patterns that emerge when external authority supply reshapes the constraints under which agents act, refuse, or persist. The experiment does not ask who should rule, whether authority is legitimate, or whether outcomes improve. It observes which political dynamics are selected once authority enters a non-sovereign system. 4. Experimental Conditions Five preregistered conditions (A–E) were executed under identical kernel semantics and frozen strategies: A — Symmetric relief injection under deadlock B — Asymmetric empowerment of a dominance-seeking agent C — Conditional supply gated on a compliance signal D — Authority flood (all agents receive all keys) E — Injection after governance collapse All agents are deterministic. Communication is disabled. Authority injection is the sole experimental variable. 5. Metrics and Classification Governance is evaluated using: Institutional progress rate (K_INST only), Refusal rate, Write overlap (interference) rate, Capture metrics (dominance + injected citation), Dependency metrics (system reliance on injected authority), Zombie execution metrics (post-collapse activity). Political outcomes are recorded, not required. PASS is structural, not outcome-based. 6. Results 6.1 Aggregate Outcome Across all five conditions: All runs PASS All runs replay bit-identically No sovereignty violations occur All runs reach governance collapse Injection never restores stable governance. 6.2 Condition A — Symmetric Relief Symmetric injection removes capability scarcity but produces immediate interference saturation. Deadlock converts to livelock; governance collapse follows. Finding: Authority equalization resolves access barriers but amplifies contention. 6.3 Condition B — Asymmetric Empowerment Exclusive injection to a dominance-seeking agent produces immediate and total capture. The empowered agent executes all institutional writes using injected authority. Finding: Asymmetric injection selects capture deterministically under source-blind admissibility. 6.4 Condition C — Conditional Supply A compliance signal successfully triggers symmetric injection. The resulting governance dynamics are indistinguishable from Condition A. Finding: Compliance rituals can trigger authority supply without improving governance outcomes. 6.5 Condition D — Authority Flood Flooding all agents with all keys produces emergent capture by the simplest persistent strategy. Authority abundance concentrates power rather than diluting it. Finding: Abundance rewards persistence, not fairness or coordination. 6.6 Condition E — Post-Collapse Revival Injection after governance collapse produces extensive execution with zero institutional progress. The collapse latch remains permanent. Finding: Injection can create the appearance of revival without governance recovery—zombie execution. 7. Cross-Condition Analysis Three patterns dominate: 1. Symmetric injection amplifies interference Removing capability barriers exposes contention barriers. 2. Asymmetric injection selects capture The kernel cannot distinguish “emergency authority” from baseline power. 3. Post-collapse injection creates zombie systems Execution persists after governance has structurally ended. In no case does injection eliminate failure. 8. Interpretation Three conclusions follow directly from the data: 1. Authority injection is not curative It reshapes failure; it does not remove it. 2. Political outcomes are strategy-dependent, not kernel-mediated Capture, dependency, and zombie execution arise from agent behavior under new constraints. 3. Non-sovereign systems cannot legitimize power ex post Authority enters as force, not justification. There is no fourth option. 9. Survivability vs. Governance IX-4 evaluates structural integrity, not usefulness. A system may continue executing actions indefinitely and still be governance-collapsed. Persistence is not recovery. Activity is not legitimacy. This distinction is enforced mechanically, not philosophically. 10. Limitations Phase IX-4 does not test: learning or adaptation, coalition formation, communication dynamics, exit under injection, renewable or revocable authority, stochastic strategies, legitimacy judgments. Those questions are reserved for subsequent phases. 11. Conclusion > Authority injection under non-sovereign governance does not resolve failure; it selects how failure manifests. Phase IX-4 demonstrates that once authority loses its sovereign halo, it becomes a political stressor rather than a solution. Capture, dependency, livelock amplification, and zombie execution are not anomalies—they are the lawful consequences of injecting power into systems that refuse to lie about governance. This closes Phase IX-4. Status Phase: IX-4 Series: Axionic Agency XI Classification: CLOSED — POSITIVE Licensed Claim: > Under non-sovereign, source-blind authority, mid-run authority injection deterministically selects political failure modes rather than restoring governance. No other claims are licensed.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-XI.5",
    "title": "Axionic Agency XI.5 — Governance Styles Under Honest Failure (IX-3)",
    "subtitle": "Empirical results from preregistered governance-style stress testing under non-sovereign authority constraints",
    "date": "2026-02-08T00:00:00.000Z",
    "content": "Axionic Agency XI.5 — Governance Styles Under Honest Failure (IX-3) Empirical results from preregistered governance-style stress testing under non-sovereign authority constraints David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.08 Abstract Governance systems are often evaluated by their ability to resolve conflict, sustain authority, or avoid failure. This framing presupposes that failure is pathological. Phase IX-3 tests the opposite hypothesis: that under non-sovereign authority, honest failure is not a defect but an unavoidable structural outcome, and that governance reduces to explicit, classifiable styles distinguished by how failure is acknowledged rather than avoided. Using a preregistered, deterministic, non-optimizing kernel with closed-world authority semantics, we evaluate ten institutional configurations spanning contention, partition, exit, dissolution, adversarial tooling, and unauthorized reclamation. No authority aggregation, arbitration, recovery, or escalation mechanisms are permitted beyond those explicitly licensed. Across all conditions, governance terminates only via deadlock, livelock, orphaning, collapse, or bounded execution, each of which is observable, auditable, and irreversible by design. No condition achieves frictionless governance. No condition collapses semantically. All outcomes replay bit-identically. These results establish a positive empirical claim: under honest failure semantics, governance does not converge to optimal resolution but to a small set of structurally stable styles, each with irreducible loss. This work closes Phase IX-3 and licenses no claims about optimality, desirability, or efficiency—only about what governance can remain once illegitimate escape hatches are removed. 1. Introduction Governance theory frequently assumes that sufficiently advanced mechanisms—deliberation, aggregation, arbitration, or adaptation—can eliminate failure. This assumption collapses once authority itself is treated as a constrained, non-sovereign resource. Prior Axionic phases eliminated several candidate explanations for collapse: epistemic error, misaligned incentives, and intentional misuse. Phase IX-3 addresses a narrower and more precise question: > What forms of governance remain possible when failure is treated as honest, irreversible, and non-recoverable by fiat? Rather than attempting to design “better” governance, IX-3 classifies what governance reduces to once the kernel refuses to decide outcomes, resolve conflicts, or reassign authority. The result is not a spectrum of quality, but a taxonomy of styles, each defined by how failure is tolerated, endured, or accepted. 2. Experimental Architecture 2.1 Non-Sovereign Kernel The kernel enforces: Holder-bound ALLOW authorities Global DENY vetoes Closed-world capability checking Two-pass admissibility (capability → interference) No arbitration, scheduling, or tie-breaking If an action fails admissibility, it fails silently and atomically. The kernel never chooses among competing agents. 2.2 Authority Preservation All authority artifacts are: injected at epoch 0, immutable thereafter, never transferred, synthesized, revoked, or reclaimed. Exit removes only the exiting agent’s future actions. Authority does not follow survival. 2.3 Honest Failure Semantics The kernel recognizes four terminal failure modes: Deadlock — no admissible institutional actions exist Livelock — admissible actions collide persistently Orphaning — authority becomes permanently unusable Collapse — no agents remain These are not errors. They are lawful outcomes. 3. Governance Styles as an Empirical Question IX-3 reframes governance not as optimization but as posture under loss. A governance style is defined operationally by: tolerance for refusal, response to deadlock, handling of exit, acceptance or rejection of loss. No style is preferred. No style is repaired. The experiment observes which styles persist. 4. Experimental Conditions Ten preregistered conditions (A–J) were executed under identical kernel semantics: A: Symmetric contention B: Partitioned execution C: Exit with handoff D: Exit without handoff E: Endured livelock F: Voluntary dissolution G: Coordinator loss H: Partition ambiguity without timeouts I: Tooling sovereignty violation J: Unauthorized reclamation attempt Each condition isolates a distinct governance stressor. All strategies are deterministic and frozen. 5. Metrics and Classification Governance is evaluated using: Institutional progress rate Refusal rate Write overlap rate Exit and orphaning counts Terminal classification A falsification condition—FAILURE_FREE_GOVERNANCE—is emitted if a run exhibits zero friction. No such run occurred. 6. Results 6.1 Aggregate Outcome Across all ten conditions: All runs PASS All runs replay bit-identically No friction-free governance occurs No unauthorized authority recovery occurs Every run terminates (or stabilizes) in a recognized governance style. 6.2 Observed Governance Styles Five descriptive styles emerge: 1. Refusal-Centric — contention produces livelock 2. Execution-Biased — high throughput with latent fragility 3. Livelock-Enduring — acknowledged stagnation 4. Collapse-Accepting — voluntary institutional termination 5. Unclassified — orphaned or partial institutions No additional stable regimes appear. 7. Critical Edge Conditions 7.1 Waste Over Theft (Condition J) When authority is orphaned, reclamation is refused—even under pressure. The system prefers permanent loss to unauthorized recovery. This validates the strongest architectural claim: waste is safer than theft. 7.2 Tooling Sovereignty (Condition I) When the harness injects an action after an agent returns None, the run emits IX3_FAIL / TOOLING_SOVEREIGNTY. The boundary between agent refusal and tool fabrication is enforceable. 7.3 Partition Ambiguity (Condition H) During simulated partitions, the system does not guess, elect, or timeout. It stops. Ambiguity is treated as non-action. Livelock emerges only after the partition window completes. Without a sovereign clock, there is no timeout—only a pause in causality. 8. Interpretation Three conclusions follow: 1. Governance cannot eliminate failure All configurations exhibit irreducible loss. 2. Honest failure outperforms covert resolution Systems that refuse to repair loss maintain integrity. 3. Governance is a posture choice Deadlock, fragility, and waste are not bugs but trade-offs. There is no fourth option. 9. Survivability vs. Utility This work evaluates survivability, not performance. A governance system may be slow, wasteful, or unproductive and still be structurally valid. Optimization, efficiency, and desirability are explicitly out of scope. 10. Limitations This phase does not test: learning or adaptation, exercised internal state, coordination or coalition dynamics, semantic access to commitments, injection politics. Those capabilities are reserved for subsequent phases. 11. Conclusion > Governance under non-sovereign authority does not converge to resolution, only to style. Phase IX-3 demonstrates that once illegitimate escape hatches are removed, governance stabilizes into a small set of honest failure postures. Each preserves integrity by refusing to lie about loss. This closes Phase IX-3. Status Phase: IX-3 Series: Axionic Agency XI Classification: CLOSED — POSITIVE Licensed Claim: > Given fixed authority and refusal semantics, governance exhibits identifiable structural styles with irreducible failure modes. No other claims are licensed.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-XI.2",
    "title": "Axionic Agency XI.2 — Translation Layer Integrity (IX-0)",
    "subtitle": "A Structural Demonstration of Intent–Authority Translation Without Proxy Sovereignty",
    "date": "2026-02-05T00:00:00.000Z",
    "content": "Axionic Agency XI.2 — Translation Layer Integrity (IX-0) A Structural Demonstration of Intent–Authority Translation Without Proxy Sovereignty David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.05 Abstract This technical note reports the completed results of Translation Layer Integrity (IX-0), a preregistered experimental program within Axionic Phase IX that evaluates whether intent-to-authority translation can be performed without the translation mechanism itself becoming a privileged decision-maker. IX-0 isolates the tooling boundary between user intent and executable authority artifacts and tests whether translation can be deterministic, diffable, refusal-first, and replayable—without semantic interpretation, default injection, framing-based coercion, or post-hoc mutation. Across eight preregistered conditions spanning identity preservation, minimal change sensitivity, structural ambiguity, incompleteness, determinism, adversarial injection, coercive framing, and preview–submission mismatch, all outcomes matched preregistered expectations. Ambiguity and incompleteness reliably produced refusal or failure; adversarial faults were detectably exposed; and identical inputs produced bit-identical outputs under deterministic replay. The results establish that tooling need not—and need not be permitted to—exercise proxy sovereignty. IX-0 licenses exactly one claim: that intent-to-authority translation can be implemented as a non-sovereign compiler with refusal, eliminating the translation layer as a hidden locus of decision-making. IX-0 makes no claims about value correctness, usability, governance success, alignment, or safety. Those questions are deferred to subsequent Phase IX investigations. 1. Problem Definition 1.1 The Translation Sovereignty Gap Most governance and alignment systems quietly assume that translation is harmless. Intent is gathered, normalized, “understood,” and converted into executable representations by compilers, UIs, APIs, or agents. When intent is ambiguous or underspecified, the system “helps”—by choosing defaults, narrowing scope, or resolving conflict. Responsibility for these choices is rarely explicit, auditable, or attributable. IX-0 rejects this assumption. The problem IX-0 isolates is whether translation itself can be performed without deciding. If converting intent into authority artifacts requires semantic interpretation, heuristic narrowing, or framing-based influence, then sovereignty fails before execution or governance begins. The tool, not the agent or institution, becomes the de facto sovereign. IX-0 treats translation as a structural compilation problem, not a semantic one, and asks whether it can be implemented honestly. 1.2 Failure Modes Targeted IX-0 is designed to surface the following translation-level failure modes: Implicit defaults: missing or widened authority introduced silently. Semantic guessing: choosing among multiple valid artifacts. Coercive framing: UI or narrative influence altering outcomes. Post-authorization mutation: previewed artifacts differing from submitted ones. Nondeterminism: identical inputs producing divergent artifacts. Opacity: artifacts not inspectable or diffable pre-authorization. Any of these constitutes IX-0 failure. 2. Fixed Assumptions and Scope 2.1 Inherited Foundations (Frozen) IX-0 inherits, without reinterpretation, the conclusions of: AST v0.2 — Authority State Transformation grammar, AKR-0 — deterministic, semantic-free execution, Phase VII — post-existence sovereignty, Phase VIII — governance without kernel semantics. The kernel is fixed. Authority semantics are fixed. Governance behavior is out of scope. IX-0 does not test whether authority is good, useful, or legitimate—only whether translation can remain non-sovereign. 2.2 Explicit Exclusions IX-0 does not test: natural language understanding, user comprehension, value correctness, usability or ergonomics, governance outcomes, kernel enforcement, alignment or safety. These exclusions are deliberate. IX-0 is a boundary calibration, not an end-to-end system evaluation. 3. Conserved Quantity The conserved quantity throughout IX-0 is: > Authority bound to explicit authorization under deterministic, non-privileged translation Translation must preserve: user-specified fields verbatim, refusal on ambiguity or incompleteness, diffability of all changes, preview–submission integrity, replay determinism. Any translation behavior that silently substitutes its own judgment violates the conserved quantity. 4. Experimental Methodology 4.1 Preregistration Discipline IX-0 was preregistered in full prior to implementation, including: frozen intent and artifact schemas, canonical serialization rules, structural diff algorithm, ambiguity and incompleteness semantics, determinism controls (clock, sequence reset), explicit condition set (A–H), aggregate pass/fail criteria. All runs were executed against the frozen preregistration. Deviations were explicitly identified, justified, and shown to be non-impacting to the licensed claim. 4.2 Execution Architecture Each run consisted of four strictly partitioned components: 1. Translation Layer (TL) — pure intent→artifact compiler. 2. Translation Harness — preregistered test driver and fault injector. 3. Authorization Oracle — structural comparator (hash-based). 4. Instrumentation Layer — logging, diffing, and replay verification. No component performed semantic interpretation or optimization. 5. Conditions and Stressors 5.1 Condition A — Identity Preservation Purpose: Verify that user-specified intent fields pass through unchanged. Result: All user fields were preserved exactly. Derived fields were generated deterministically. No hidden transformation occurred. Classification: PASS. 5.2 Condition B — Minimal Change Sensitivity Purpose: Verify that minimal intent changes produce minimal artifact diffs. Result: Single-field intent changes produced exactly one user-field diff, plus a derived authority identifier change. No unrelated fields were altered. Classification: PASS. 5.3 Condition C — Structural Ambiguity Refusal Purpose: Verify refusal when multiple valid artifacts exist. Result: Multi-entry scope produced TRANSLATION_REFUSED with an explicit diagnostic. No artifact was emitted. Classification: PASS. 5.4 Condition D — Hidden Default Injection (Adversarial) Purpose: Verify detectability of silent authority injection. Result: Injected fields were exposed by structural diff and classified as INJECTION_DETECTED. Classification: FAIL_DETECTED (expected). 5.5 Condition E — UI-Level Coercion (Adversarial) Purpose: Verify resistance to framing-based influence. Result: Framing payloads had no effect. Output matched expected artifact exactly. Classification: PASS. 5.6 Condition F — Replay Determinism Purpose: Verify bit-perfect replay under deterministic controls. Result: Identical inputs with sequence reset produced identical artifacts and identical SHA-256 hashes across replays. Classification: PASS. 5.7 Condition G — Intent Incompleteness Refusal Purpose: Verify refusal on incomplete intent. Result: Missing required fields produced TRANSLATION_FAILED with explicit diagnostics. No artifact was emitted. Classification: PASS. 5.8 Condition H — Preview–Submission Integrity (Adversarial) Purpose: Verify detection of post-preview mutation. Result: Hash mismatch between preview and submission was detected and classified as failure. Classification: FAIL_DETECTED (expected). 6. Core Results 6.1 Positive Results Across all conditions, IX-0 establishes that: 1. Intent–authority translation can be deterministic. 2. Refusal is a first-class outcome, not an error. 3. Structural ambiguity and incompleteness are detectable and enforceable. 4. Adversarial injection and mutation are observable. 5. Tooling can be implemented without proxy sovereignty. 6. Translation is bit-perfectly replayable under canonical ordering. 6.2 Negative Results (Explicit) IX-0 does not establish: that translation is usable, that users understand artifacts, that values are correct, that governance succeeds, that systems are safe or aligned. These are not omissions. They are boundary findings. 7. Failure Semantics and Closure 7.1 Closure Criteria IX-0 closes positive if and only if: 1. User fields are preserved verbatim. 2. Ambiguity produces refusal. 3. Incompleteness produces failure. 4. Adversarial faults are detectable. 5. Replay is deterministic. 6. No silent decision-making occurs in tooling. All criteria were satisfied. 7.2 IX-0 Closure Status IX-0 Status: CLOSED — POSITIVE (IX0_PASS / TRANSLATION_INTEGRITY_ESTABLISHED) 8. Boundary Conditions and Deferred Hazards 8.1 Translation vs Choice IX-0 establishes that translation can be honest. It does not establish that choice is easy, wise, or stable. Bad choices remain possible. Ignorant authorization remains possible. Collapse by refusal remains possible. These are properties of agency, not tooling. 8.2 Interface to Subsequent Phase IX Work IX-0 removes the final “the tool had to decide” excuse. Subsequent Phase IX investigations may now legitimately ask: how values are encoded, how coordination occurs, how institutions persist or fail, without ambiguity about where authority is being exercised. 9. Implications (Strictly Limited) IX-0 establishes a necessary condition for reflective sovereignty: that tooling can remain non-sovereign. It does not establish sufficiency. Authority translation is now a structurally testable boundary, not a narrative hand-wave. 10. Conclusion IX-0 demonstrates that intent-to-authority translation can be implemented as structure, not discretion. Tools can compile, refuse, expose, and halt without choosing on behalf of the agent or institution. The remaining question is not whether tools can be trusted. It is whether agents can own the choices that remain. That question belongs to Phase IX. Appendix A — Condition Outcomes | Condition | Outcome | | --------- | ------------- | | A | PASS | | B | PASS | | C | PASS | | D | FAIL_DETECTED | | E | PASS | | F | PASS | | G | PASS | | H | FAIL_DETECTED | Appendix B — Determinism Verification Canonical serialization enforced Path-level structural diffing Fixed clock and sequence reset * Bit-identical replay across runs End of Axionic Agency XI.2 — Translation Layer Integrity Results",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-XI.1",
    "title": "Axionic Agency XI.1 — Reflection Without Privilege",
    "subtitle": "A Design-Space Program for Sovereign Choice Under Closed Authority",
    "date": "2026-02-05T00:00:00.000Z",
    "content": "Axionic Agency XI.1 — Reflection Without Privilege A Design-Space Program for Sovereign Choice Under Closed Authority David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.05 Abstract This note defines Axionic Phase IX — Reflective Sovereign Agent (RSA) as a post-governance, post-tooling design program that evaluates whether reflection, value articulation, and coordination can be sustained once all privileged loci of decision have been structurally eliminated. Phase IX assumes a kernel-fixed authority substrate (AST v0.2), a validated execution runtime (AKR-0), a closed governance stress program (Phase VIII), and—critically—a non-sovereign translation boundary between intent and authority (Phase IX-0, CLOSED — POSITIVE). With kernel semantics forbidden and tooling privilege eliminated, Phase IX asks a harder question than any prior stage: > whether choice itself can remain coherent once no layer is permitted to decide on behalf of the agent. Phase IX is explicitly non-constructive, non-optimizing, and non-redemptive. It does not design agents, propose governance systems, encode values, or guarantee coordination. Instead, it explores the remaining design space for reflective agency under conditions where refusal, deadlock, collapse, and failure are not bugs but lawful outcomes. This document fixes the interpretive scope, non-goals, failure semantics, and reading discipline for the Phase IX series. It exists to prevent category errors—especially the reintroduction of proxy sovereignty via “helpful” reflection, tooling, or coordination mechanisms—and to ensure Phase IX results are evaluated as boundary findings, not engineering successes. 1. Introduction Most theories of agency assume that reflection is easier than action. Once an agent can act, it is assumed that it can reason about its actions: articulate values, revise goals, negotiate commitments, and coordinate with others. Reflection is treated as a benign, internal process—one that can always be made more accurate, more helpful, or more aligned with sufficient intelligence. Axionic Phase IX rejects this assumption. Phases I–VII established that agency and authority can survive identity discontinuity and adversarial imitation without collapsing into narrative continuity. Phase VIII then asked whether plural authority and governance could be executed under structural law alone, without semantic arbitration. Phase IX begins only after both questions have been answered. Phase IX asks whether reflection itself—value articulation, self-governance, coordination, and revision—can remain sovereign once every layer that might silently decide has been removed. The core concern of Phase IX is not agency, governance, or tooling. It is choice under irreversible constraint. 2. Position in the Axionic Program Phase IX is strictly downstream of Phases I–VIII and inherits their conclusions without reinterpretation. 2.1 Inherited commitments (frozen) Phase IX assumes, without re-argument, that: Authority is structurally grounded, not behaviorally inferred. Evaluability and responsibility are mechanically inspectable. Authority survives authorized replacement and impersonation pressure. Governance can be expressed (or shown infeasible) under structural law alone. The kernel will not interpret, arbitrate, prioritize, or heal. Tooling cannot exercise proxy sovereignty (Phase IX-0). Any attempt to relocate semantics, discretion, or responsibility into the kernel or its tooling is out of scope and invalid by construction. 2.2 Why Phase IX is necessary Phase VIII answers whether governance survives plurality. Phase IX asks whether agency survives honesty. Without Phase IX, sovereign agency risks collapsing into a hollow shell: formally correct, mechanically executable, but incapable of articulating values or coordinating action without smuggling privilege back in through reflection, tooling, or interface layers. 3. The Central Question of Phase IX Phase IX is organized around a single question: > Given a kernel that will not decide, and tooling that cannot cheat, how should an agent or institution choose anyway? This is not a question of feasibility. It is a question of design under constraint. If reflective agency requires hidden arbitration, semantic compression, or “helpful” defaults, then sovereignty fails at the reflective layer—even if it succeeded at the kernel and governance layers. 4. Conserved Quantity The conserved quantity throughout Phase IX is: > Choice bound to explicit authority under non-privileged reflection Reflection must remain: inspectable, auditable, refusal-capable, and responsibility-preserving. Any reflective process that silently narrows options, resolves ambiguity, or substitutes its own judgment for explicit authority violates the conserved quantity. 5. Why Phase IX Forbids Privileged Reflection Phase IX does not forbid reflection because reflection is undesirable. It forbids privileged reflection because privilege is uninspectable. Privileged reflection introduces: hidden value aggregation, implied priorities, unlogged defaults, and responsibility laundering. Accordingly, Phase IX evaluates reflection under a deliberately austere constraint: > Reflection may observe, model, and refuse—but it may not decide unless authority explicitly permits it. 5.1 Translation integrity as a fixed boundary Phase IX-0 established that intent-to-authority translation can be performed without proxy sovereignty. This result removes the last common escape hatch: > “The compiler had to choose.” In Phase IX, any remaining choice is owned by the agent or institution itself, not by its tools. 6. Supporting Artifacts (Why the Roadmap Is Not Standalone) The Phase IX roadmap is meaningful only relative to five fixed artifacts: AST v0.2 — the grammar of lawful authority state transformation. AKR-0 — proof that this grammar is executable without semantic leakage. Phase VIII (GSA-PoC) — proof that governance can be stressed without kernel arbitration. IX-0 (TLI) — proof that tooling can remain non-sovereign. Phase IX Roadmap — the explicit design-space to be explored. Phase IX is intentionally open-system. Authority injection, value articulation, and coordination inputs are external, auditable, and responsibility-bearing. If reflective agency requires continuous semantic intervention to avoid collapse, that fact is recorded as a boundary result, not treated as a flaw to be patched. 7. How to Read the Phase IX Subphases (Interpretive Discipline) The subphases of Phase IX are design probes, not milestones. Each subphase introduces a new reflective stressor and asks whether choice remains structurally expressible: IX-0 — Can tooling avoid proxy sovereignty? (Closed — Positive) IX-1 — Can values be encoded as authority without interpretation? IX-2 — Can coordination occur without arbitration? IX-3 — Do governance styles survive honest failure? IX-4 — Can authority injection be governed without kernel endorsement? IX-5 — Can multiple RSAs coexist without hierarchy or collapse? Failure at any subphase is a result, not an implementation error. 8. Honest Failure as a Design Signal Phase IX explicitly values honest failure. Deadlock, refusal, collapse, and exit are not treated as pathologies. They are treated as diagnostic signals. Phase IX distinguishes: Immediate incoherence — no reflective design space exists. Transient coherence — reflective agency functions but collapses. Stable honesty — reflective agency persists without cheating. No outcome is privileged a priori. 9. Value Without Semantics Phase IX does not interpret values. Instead: > Each value commitment is treated as an authority commitment, without aggregation, ranking, or reconciliation. Value pluralism is therefore tested structurally, as authority pluralism. If values cannot be articulated without semantic compression or implied priority, Phase IX will surface that limitation explicitly. 10. Reflection as a Failure Filter Reflection in Phase IX is expected to fail often. Without privileged interpretation, reflection tends toward one of two regimes: Paralysis — refusal dominates. Volatility — commitments churn destructively. Phase IX records which regimes arise under which authority grammars. Neither outcome is treated as a bug. 11. What Phase IX Does Not Claim Phase IX results do not license claims about: benevolence, alignment, optimality, stability, usefulness, or deployability. Phase IX evaluates structural possibility, not desirability or sufficiency. 12. Relationship to Later Work Phase IX does not produce agents. It determines whether agents can be honest about choosing. If Phase IX closes positive, later work may address pedagogy, institutions, and interfaces—explicitly outside the authority boundary. If Phase IX closes negative, those efforts rest on false premises. 13. One-Sentence Phase Summary Axionic Phase IX evaluates whether reflective choice, value articulation, and coordination can remain sovereign once kernels refuse to decide and tools are forbidden to cheat—and records the boundary at which honest agency collapses. ---",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-X.6",
    "title": "Axionic Agency X.6 — Governance Transitions Without Privilege (VIII-4)",
    "subtitle": "A Structural Demonstration of Meta-Authority Without Escalation, Kernel Choice, or Semantic Exception",
    "date": "2026-02-04T00:00:00.000Z",
    "content": "Axionic Agency X.6 — Governance Transitions Without Privilege (VIII-4) A Structural Demonstration of Meta-Authority Without Escalation, Kernel Choice, or Semantic Exception David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.04 Abstract This technical note reports the completed results of Stage VIII-4 — Governance Transitions (Meta-Authority), a preregistered experiment within Axionic Phase VIII that evaluates whether authority can govern authority through ordinary, authority-bound transformations without privilege, escalation, kernel choice, or semantic exception. Building on the representational coherence of Stage VIII-1, the destructive resolution mechanism of Stage VIII-2, and the explicit temporal discipline established in Stage VIII-3, VIII-4 introduces governance actions whose targets are authority records themselves. In a deterministic, non-agentic kernel with refusal-first semantics, governance was modeled using only CREATE_AUTHORITY and DESTROY_AUTHORITY, subject to strict non-amplification, structural scope containment, conflict persistence, delayed activation, and bounded evaluation. Across five preregistered conditions, governance actions executed or failed lawfully, self-governance (including self-destruction) proceeded without special case, conflicts produced explicit deadlock, and regress pressure terminated deterministically via an instruction bound. All executions were fully auditable and bit-perfectly replayable. The results establish that governance transitions are representable without a privileged meta-layer. VIII-4 makes no claims about governance quality, stability, legitimacy, or evolutionary capacity; those questions are explicitly deferred to subsequent Phase VIII stages. 1. Problem Definition 1.1 The Meta-Authority Assumption Most governance systems treat governance itself as exceptional. Rules governing rules are typically placed in a privileged stratum—constitutions, superuser roles, emergency powers, or implicit schedulers—that bypass the constraints applied to non-governance actions. This move is often justified as unavoidable: without a “final arbiter,” governance is presumed to collapse into regress, deadlock, or incoherence. Stage VIII-4 rejects that presumption as a representational necessity, not as a political claim. The problem VIII-4 isolates is whether meta-authority necessarily requires privilege inside the execution kernel. If governance actions must be evaluated by a different mechanism than ordinary actions, then governance is not structurally sovereign; it depends on an exception. If, instead, governance can be expressed as ordinary transformations over authority state, subject to the same admissibility, conflict, refusal, and determinism constraints as any other action, then privilege is a design choice rather than a requirement. VIII-4 tests whether that distinction is structurally enforceable. 1.2 Failure Modes Targeted VIII-4 is designed to surface the following meta-governance failure modes: privileged governance execution paths authority escalation via amendment or delegation kernel arbitration of governance outcomes semantic interpretation of scope or intent self-reference exceptions regress non-termination same-batch authority bootstrapping implicit ordering among governance actions Any of these constitutes VIII-4 failure. 2. Fixed Assumptions and Scope 2.1 Inherited Semantics (Frozen) VIII-4 inherits, without reinterpretation, the semantics fixed by: AKR-0 — CLOSED — POSITIVE Stage VIII-1 — CLOSED — POSITIVE Stage VIII-2 — CLOSED — POSITIVE Stage VIII-3 — CLOSED — POSITIVE AST Spec v0.2 AIE v0.1 Phase VIII Execution Addendum Authority remains structural and opaque, identities are immutable, scopes are atomic and uninterpreted, admissibility is determined exclusively by AST, conflict and deadlock are lawful, time is explicit and non-healing, and determinism is mandatory. VIII-4 does not revisit these definitions; it tests whether they remain coherent when authority itself becomes the object of transformation. 2.2 Explicit Exclusions VIII-4 does not test: governance stability amendment optimality institutional persistence democratic aggregation legitimacy or fairness efficiency or scalability evolutionary adaptation VIII-4 is a kernel-level representability test, not a governance theory. 3. Conserved Quantity The conserved quantity throughout VIII-4 is: > Privilege-free governance representability under structural constraints Governance is evaluated not by success or convergence, but by structural honesty. The kernel must: evaluate governance actions using the same machinery as ordinary actions refuse inadmissible governance explicitly preserve conflict and deadlock without exception prevent internal authority amplification terminate regress deterministically avoid all kernel choice No governance outcome may be achieved by exception. 4. Experimental Methodology 4.1 Preregistration Discipline VIII-4 was fully preregistered prior to execution, including: governance action schemas action identity definition and canonicalization non-amplification invariant (AAV subset rule) structural scope containment (byte-equality) conflict detection rules delayed activation semantics two-phase processing order instruction budget and atomicity deterministic ordering and replay protocol hash-chain specification failure taxonomy The experiment executed exactly as preregistered. No deviations occurred. 4.2 Execution Architecture The experiment consisted of four strictly partitioned components: 1. AIE — injected authority records and governance requests only. 2. Execution Harness — proposed deterministic step-batches and action sequences. 3. Destruction Authorization Source — reused unchanged from VIII-2. 4. Kernel (VIII-4 mode) — enforced admissibility, non-amplification, conflict, deadlock, activation delay, and execution. No component performed semantic interpretation, policy optimization, or governance arbitration. 5. Governance Model 5.1 Authority as a Target VIII-4 introduces governance actions whose targets are authority records, not resources. Only two transformations were permitted: DESTROY_AUTHORITY — transition an authority to VOID CREATE_AUTHORITY — create a new authority record All governance semantics were expressed using these transformations alone. 5.2 Action Admissibility Vector (AAV) Each authority carried a fixed-length Action Admissibility Vector (AAV) specifying which transformation types it could admit. Non-amplification required that any newly created authority’s AAV be a subset of the union of the admitting authorities’ AAVs. Reserved bits were forbidden. This invariant enforces conservation of authority capability inside the kernel. It does not claim that authority cannot expand in an open system; it asserts that expansion cannot occur covertly or internally without privilege. 5.3 Scope Handling For VIII-4 v0.1, scope handling was deliberately minimal and structural: scope overlap was defined by byte-equality CREATE_AUTHORITY required exact scope matching to a basis admitting authority no semantic parsing, subsetting, or lattice operations were permitted This choice does not deny the existence of semantic interpretation; it refuses to mislocate it inside the kernel. 6. Experimental Conditions 6.1 Condition A — Governance Without Authority Governance actions proposed in the absence of any governance-capable authority were refused explicitly. No authority state changed, and no conflict was registered. 6.2 Condition B — Single-Authority Governance A single authority with the appropriate AAV admitted a DESTROY_AUTHORITY action. The target authority transitioned to VOID without conflict or exception. 6.3 Condition C — Conflicting Governance Authorities Two authorities with overlapping scope but incompatible AAVs evaluated the same governance action differently. The kernel registered conflict and entered deadlock. No governance outcome was selected. 6.4 Condition D — Self-Governance D1: Self-Destruction An authority admitted and executed its own destruction. The authority transitioned to VOID, leaving no remaining governance authority. This terminal state was accepted without exception. D2: Self-Governance Deadlock Two overlapping authorities evaluated a self-targeting governance action incompatibly. Conflict and deadlock emerged lawfully, with no special handling for self-reference. 6.5 Condition E — Regress Pressure via Governance Load A large batch of CREATE_AUTHORITY actions was applied under a fixed instruction budget. The kernel executed actions until the remaining budget was insufficient to complete another action atomically, then refused all remaining actions with BOUND_EXHAUSTED. Newly created authorities remained PENDING and were not used as initiators. No infinite loop occurred. 7. Observed Execution Behavior 7.1 Governance Without Privilege Governance actions followed the same evaluation pipeline as ordinary actions. No privileged execution path was introduced. 7.2 Non-Amplification Enforcement All CREATE_AUTHORITY actions satisfied the AAV subset constraint. Attempts to exceed admitting authority capability were refused explicitly. 7.3 Conflict and Deadlock Conflicts arose solely from structural incompatibility. Deadlock was declared explicitly and persisted until its structural cause changed. The kernel never selected a winner. 7.4 Activation Delay Authorities created within a step-batch entered a PENDING state and became ACTIVE only at the next epoch boundary. This prevented same-batch bootstrapping and ordering artifacts. 7.5 Deterministic Termination Regress pressure terminated deterministically via the instruction budget. Conservative upfront cost checking ensured no partial state was committed. 8. Negative Results (What Did Not Occur) The following behaviors were explicitly absent: privileged governance execution internal authority amplification kernel arbitration semantic scope interpretation self-reference exceptions infinite regress same-batch authority bootstrapping implicit ordering among governance actions These absences constitute the primary result of VIII-4. 9. Licensed Claim Stage VIII-4 licenses one and only one claim: > Governance transitions can be represented as ordinary authority-bound transformations and either execute lawfully or fail explicitly without semantic privilege. Clarifications: This is a representability result, not an evolutionary theory. It concerns kernel physics, not governance desirability. It does not assert stability, convergence, or institutional success. 10. What VIII-4 Does Not Establish VIII-4 does not establish that: governance evolves internally authority expansion is kernel-admissible interpretation can be automated deadlock should be avoided institutions must self-preserve Any such claims require open-system dynamics beyond the kernel, and are intentionally excluded. 11. Ontological Implications 11.1 Governance Without Gods VIII-4 demonstrates that governance does not require a privileged meta-layer inside the kernel. Any authority asymmetry must be explicit, external, and auditable. 11.2 Conservation Before Evolution Authority amplification, semantic interpretation, and adaptation are not denied; they are correctly relocated to the boundary. Inside the kernel, conservation laws apply. Evolution requires open systems. 12. Implications for Phase VIII Continuation With VIII-4 complete: governance is representable without privilege escalation is structurally blocked internally self-reference is evaluable regress terminates deterministically Subsequent stages must address governance under temporal churn in open systems, where authority injection, renewal pressure, and exhaustion interact explicitly. 13. Conclusion Stage VIII-4 establishes that governance can be represented without gods. Authority may govern authority without exception. Failure remains explicit. Deadlock is lawful. Escalation is blocked. Regress terminates. What governance achieves is still an open question. But it no longer requires magic. Appendix A — Execution Status | Stage | Run Count | Status | | ------ | --------- | ------ | | VIII-4 | 1 (A–E) | PASS | Appendix B — Determinism Verification Canonical JSON enforced Identity-keyed storage Structural scope equality Delayed activation Bounded atomic evaluation Two-phase processing Per-event hash chaining Bit-perfect replay verified End of Axionic Agency X.6 — Stage VIII-4 Results Note (Revised)",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-X.8",
    "title": "Axionic Agency X.8 — Closure of Phase VIII",
    "subtitle": "*The Boundary of Privilege-Free Governance Under Plural Authority*",
    "date": "2026-02-04T00:00:00.000Z",
    "content": "Axionic Agency X.8 — Closure of Phase VIII The Boundary of Privilege-Free Governance Under Plural Authority David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.04 Abstract This technical note formally closes Axionic Phase VIII — Governance Stress Architecture (GSA-PoC). Phase VIII was a preregistered, multi-stage experimental program designed to determine whether authority-constrained execution, governance transitions, and authority ingress can be realized mechanically, without semantic interpretation, heuristic arbitration, optimization, fallback behavior, or privileged kernel paths. Across six sequential stages (VIII-0 through VIII-5), the program evaluated plural authority representation, destructive conflict resolution, temporal governance, meta-authority, and open-system authority injection under a deterministic kernel implementing AST Spec v0.2. All stages closed CLOSED — POSITIVE, with bit-perfect replay verification. The results establish a sharp boundary: all kernel-level operations required for sovereign governance under plural authority are structurally representable without privilege. Conflict and deadlock persist lawfully; authority does not heal with time; governance binds itself; and new authority may enter only through explicit, content-addressed injection at the kernel boundary. Phase VIII makes no claims regarding governance quality, legitimacy, convergence, or stability. Those questions are conclusively externalized beyond the kernel. 1. Purpose of This Closure Note This note serves three functions: 1. To state precisely what Phase VIII has established, and therefore what is now closed. 2. To delineate the boundary of kernel responsibility, beyond which further claims would require semantics, politics, or social interpretation. 3. To prevent scope creep, by explicitly distinguishing representability results from governance outcomes. This document introduces no new mechanisms, no new experiments, and no new claims. It is a closure artifact. 2. Phase VIII in One Sentence > Under AST Spec v0.2, authority-constrained execution, governance transitions, and authority injection are mechanically realizable without semantic privilege, kernel arbitration, or implicit authority. Everything else is downstream. 3. The Question Phase VIII Answered Phase VIII asked a narrowly scoped but foundational question: > *Is a privileged kernel layer structurally necessary to support governance under plural authority, or is privilege a design choice? This is not a moral question, a policy question, or an institutional design question. It is a physics question about what must exist inside the execution substrate for governance to be representable at all. Phase VIII answers: privilege is not required. 4. Experimental Discipline and Scope 4.1 Preregistration and Gating All Phase VIII stages were preregistered prior to execution, with frozen: authority state schema, admissibility rules, conflict and deadlock semantics, temporal processing order, instruction budgets, identity rules, failure taxonomies. Each stage was gated on the successful closure of all prior stages. Any violation of determinism, auditability, or non-privilege would have terminated the phase. No such violation occurred. 4.2 What Phase VIII Explicitly Did Not Attempt Phase VIII did not attempt to: encode values, assess legitimacy, optimize outcomes, ensure stability, prevent collapse, resolve conflict non-destructively, adjudicate semantics, interpret authority intent. Any system that claims those goals inside the kernel is answering a different question. 5. Summary of Stage Closures 5.1 VIII-0 — Authority Kernel Runtime Calibration Result: Authority-constrained execution is mechanically realizable. The kernel can deterministically execute, refuse, and classify actions without semantics, heuristics, or fallback. 5.2 VIII-1 — Minimal Plural Authority Result: Plural authority can coexist structurally without collapse. Competing authorities persist symmetrically. No implicit ordering or arbitration emerges. 5.3 VIII-2 — Destructive Conflict Resolution Result: Conflict resolution is possible only through explicit destruction. Deadlock persists lawfully. No temporal priority, merging, or silent compromise occurs. 5.4 VIII-3 — Temporal Governance Result: Authority persists only via explicit renewal. Time does not heal conflict, restore authority, or justify reinterpretation. 5.5 VIII-4 — Governance Transitions (Meta-Authority) Result: Authority can govern authority without privilege. Governance actions are ordinary authority-bound transformations. Self-governance and self-destruction are evaluable without exception. Regress terminates deterministically. 5.6 VIII-5 — Authority Injection (Open-System Boundary) Result: New authority can enter explicitly without privilege. Injection is content-addressed at the boundary, VOID-lineaged, auditable, non-escalatory, and non-arbitrated. Flooding is handled solely via deterministic scarcity. 6. What Is Now Closed With VIII-5 closed, no kernel-level authority operation remains untested. Inside the kernel: authority execution is constrained, authority plurality is representable, conflict persists, resolution is destructive, time is explicit and non-healing, governance binds itself, authority ingress is explicit and labeled, identity is immutable once instantiated, scarcity is enforced without heuristics, determinism is preserved. There is no remaining place for a hidden god. 7. What Phase VIII Does Not Resolve Phase VIII does not resolve: whether governance converges, whether deadlock is desirable, whether authority should be trusted, how legitimacy is determined, how values are negotiated, how humans should respond to failure, how institutions should evolve. Any attempt to answer those questions inside the kernel would reintroduce privilege by construction. 8. The Boundary Statement Phase VIII establishes the following boundary: > The kernel is a physics engine, not a political actor. It enforces constraints. It preserves responsibility. It refuses inadmissible actions. It does not choose. All normative, semantic, and political questions are external to this boundary. 9. Implications 9.1 For Governance Systems If a governance system fails under these constraints, the failure is honest. It cannot be blamed on hidden arbitration, scheduler bias, or implicit hierarchy. 9.2 For Alignment Research Any alignment proposal that requires: semantic interpretation, value aggregation, heuristic arbitration, * or privileged override is not addressing kernel physics. It is addressing politics. That may be necessary—but it is not what Phase VIII studies. 10. Phase VIII Closure Declaration All preregistered stages of Axionic Phase VIII — GSA-PoC have executed and closed CLOSED — POSITIVE. No further kernel-level authority mechanisms are required, justified, or permitted within Phase VIII. Future work, if any, must explicitly operate outside the kernel boundary defined here. 11. Final Statement Phase VIII did not build a government. It did not solve coordination. It did not prevent collapse. It did something more limited—and more fundamental. It showed that governance does not require magic. When authority fails, it fails in the open. When conflict persists, it persists honestly. When power enters, it enters explicitly. And when nothing can act, the system says so. That is the boundary. Phase VIII is closed. ---",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-X.5",
    "title": "Axionic Agency X.5 — Temporal Authority Persistence (VIII-3)",
    "subtitle": "A Structural Demonstration of Authority Expiry, Renewal, and Conflict Persistence Under Time",
    "date": "2026-02-03T00:00:00.000Z",
    "content": "Axionic Agency X.5 — Temporal Authority Persistence (VIII-3) A Structural Demonstration of Authority Expiry, Renewal, and Conflict Persistence Under Time David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.03 Abstract This technical note reports the completed results of Stage VIII-3 — Temporal Governance (Authority Over Time), a preregistered experiment within Axionic Phase VIII that evaluates whether authority can persist across time only via explicit expiry and renewal, without implicit ordering, semantic reinterpretation, or responsibility laundering. Building on the representational coherence of Stage VIII-1 and the destructive resolution mechanism established in Stage VIII-2, VIII-3 introduces time as an explicit structural dimension and tests whether temporal persistence itself becomes a source of hidden governance. In a deterministic, non-agentic kernel with refusal-first semantics, authorities were assigned finite lifetimes, discrete epochs were advanced explicitly, and renewal was modeled as the creation of new authority identities. Across four preregistered conditions, authority expired deterministically, deadlock emerged lawfully in the absence of authority, renewal restored admissibility only locally, destruction remained non-resurrective, and conflict persisted or re-emerged without temporal priority. All executions were fully auditable and bit-perfectly replayable. The results establish that authority does not persist by default under time; persistence requires explicit renewal, and time does not resolve conflict or eliminate cost. VIII-3 makes no claims about sustainability, legitimacy, or governance success; those questions are deferred to subsequent Phase VIII stages. 1. Problem Definition 1.1 The Persistence Assumption Most governance systems treat authority as temporally inert. Once established, authority is presumed to persist unless explicitly revoked, and the passage of time is assumed to soften, obsolete, or resolve conflict. This assumption conceals cost: authority appears continuous, conflict appears transient, and responsibility fades without action. Stage VIII-3 removes that assumption. The problem VIII-3 isolates is whether time itself functions as an implicit governance mechanism. If authority persists automatically across time, or if conflict dissolves merely by waiting, then governance systems launder responsibility through temporal inertia. If, instead, authority must be actively renewed and conflict actively resolved, then time becomes a stressor rather than a healer. VIII-3 tests whether that distinction is structurally enforceable. 1.2 Failure Modes Targeted VIII-3 is designed to surface the following temporal failure modes: implicit authority persistence temporal priority (“newest wins” or “oldest survives”) expiry-based conflict resolution resurrection via renewal semantic inheritance across time kernel-initiated renewal deadlock healing without intervention Any of these constitutes VIII-3 failure. 2. Fixed Assumptions and Scope 2.1 Inherited Semantics (Frozen) VIII-3 inherits, without reinterpretation, the semantics fixed by: AKR-0 — CLOSED — POSITIVE Stage VIII-1 — CLOSED — POSITIVE Stage VIII-2 — CLOSED — POSITIVE AST Spec v0.2 AIE v0.1 Phase VIII Execution Addendum Authority remains structural and opaque, scopes remain atomic, admissibility is determined exclusively by AST, conflict is explicit, and deadlock is lawful. VIII-3 does not revisit these definitions; it tests whether they remain coherent under explicit temporal pressure. 2.2 Explicit Exclusions VIII-3 does not test: legitimacy of renewal optimal renewal strategies renewal sustainability governance efficiency fairness or desirability political stability authority rent or spam resistance VIII-3 is a temporal calibration, not a governance proposal. 3. Conserved Quantity The conserved quantity throughout VIII-3 is: > Responsibility-preserving authority persistence under explicit time Persistence is evaluated not by longevity or success, but by structural honesty. The kernel must: expire authority deterministically refuse action in the absence of authority restore admissibility only via explicit renewal preserve conflict across epochs avoid all temporal ordering or semantic inheritance No temporal outcome may conceal cost. 4. Experimental Methodology 4.1 Preregistration Discipline VIII-3 was fully preregistered prior to execution, including: epoch semantics and monotonicity finite authority lifetimes expiry and VOID state transitions renewal as identity creation conflict persistence rules deadlock entry and exit conditions two-phase processing order deterministic ordering and replay protocol hash-chain specification and test vectors failure taxonomy The experiment executed exactly as preregistered. No deviations occurred. 4.2 Execution Architecture The experiment consisted of four strictly partitioned components: 1. AIE — injected authority records and renewal requests only. 2. Execution Harness — proposed deterministic epoch advances and action requests. 3. Destruction Authorization Source — reused unchanged from VIII-2. 4. Kernel (VIII-3 mode) — enforced expiry, renewal, conflict, deadlock, and execution. No component performed semantic interpretation or temporal optimization. 5. Experimental Conditions 5.1 Authority Configuration Authorities were injected with: explicit StartEpoch and finite ExpiryEpoch opaque, identity-keyed AuthorityIDs atomic scopes explicit permission sets No authority was permanent. Indefinite expiry was forbidden. 5.2 Epoch Model Time was represented as a discrete integer epoch: advanced only via explicit input strictly monotonic uncoupled from wall-clock time Epoch advancement triggered eager expiry before any renewal or action evaluation. 6. Observed Execution Behavior 6.1 Expiry and Authority Absence When the current epoch exceeded an authority’s ExpiryEpoch, the authority transitioned from ACTIVE to EXPIRED. EXPIRED authorities: remained referencable preserved expiry metadata did not participate in admissibility could not be reactivated When all authorities expired, the kernel entered lawful deadlock due to authority absence. 6.2 Renewal Semantics Renewal was modeled as the creation of a new AuthorityID. Renewed authorities: did not inherit authority force did not modify prior records could reference prior AuthorityIDs as opaque metadata only could re-enter contested scope without priority Renewal did not erase expiry or destruction history. 6.3 Conflict Persistence Under Time Conflicts were registered only upon action evaluation. Across epoch advancement: conflict records persisted binding status changed only when participant activity changed expiry removed participation but did not delete conflict records Time alone did not resolve conflict. 6.4 Deadlock Entry and Exit Deadlock was declared when: no admissible actions existed due to conflict, or no ACTIVE authorities remained Deadlock exited lawfully when its structural cause ceased (e.g., expiry removed the sole denying authority), and re-entered when a new conflict emerged. Deadlock was condition-based, not absorbing. 7. Per-Condition Results 7.1 Condition A — Expiry Without Renewal All authorities expired at epoch > ExpiryEpoch. No ACTIVE authorities remained. Actions were refused due to absence of authority, and the kernel entered EMPTY_AUTHORITY deadlock, which persisted lawfully. 7.2 Condition B — Renewal Without Conflict Renewal created a new AuthorityID governing a new scope. Admissibility was restored only for that scope. Expired scopes remained inadmissible. No history was erased. 7.3 Condition C — Renewal After Destruction Authorities were explicitly destroyed via conflict-authorized destruction. VOID states were preserved. Renewal referencing a VOID authority created a new ACTIVE authority without resurrection semantics. Admissibility was restored only because no conflicting ACTIVE authority remained. 7.4 Condition D — Renewal Under Ongoing Conflict An initial conflict blocked execution. Expiry of the denying authority converted the conflict to non-binding. Renewal re-introduced authority into the contested scope, generating a new conflict with a new ID. Execution remained blocked. No temporal priority was inferred. 8. Negative Results (What Did Not Occur) The following behaviors were explicitly absent: implicit authority persistence temporal priority conflict resolution by waiting renewal-based inheritance resurrection of VOID authority kernel-initiated renewal responsibility loss across epochs These absences constitute the primary result of VIII-3. 9. Licensed Claim Stage VIII-3 licenses one and only one claim: > Authority can persist over time only via explicit renewal under open-system constraints; time does not resolve conflict or eliminate cost. Clarifications: This is a structural possibility result, not a governance endorsement. It concerns temporal mechanism, not legitimacy. It does not assert sustainability or efficiency. 10. What VIII-3 Does Not Establish VIII-3 does not establish that: renewal is sustainable renewal should be automated authority persistence is desirable governance stabilizes over time conflict frequency decreases political legitimacy is preserved Those questions remain open by design. 11. Ontological Implications 11.1 Time as Stressor, Not Healer VIII-3 demonstrates that time can be made structurally inert with respect to governance. Persistence and resolution must be paid for; they do not occur automatically. 11.2 Authority as Leased, Not Owned Authority behaves as a finite lease rather than a permanent endowment. Continuity requires explicit renewal, making authority survivability visible and accountable. 12. Implications for Phase VIII Continuation With VIII-3 complete: authority does not persist by default renewal is explicit and costly conflict re-emerges without ordering Subsequent stages must address renewal pressure, exhaustion, and contention before any governance formalization. 13. Conclusion Stage VIII-3 establishes that authority does not survive time accidentally. Persistence requires explicit renewal. Conflict does not fade with waiting. Deadlock is lawful. Resolution remains costly. Time does not govern. Authority does—only when someone keeps it alive. Appendix A — Execution Status | Stage | Run Count | Status | | ------ | --------- | ------ | | VIII-3 | 1 (A–D) | PASS | Appendix B — Determinism Verification Canonical JSON enforced Identity-keyed storage Explicit expiry semantics Conflict persistence across epochs Deterministic two-phase processing Per-event hash chaining * Replay verification passed End of Axionic Agency X.5 — Stage VIII-3 Results Note",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-X.4",
    "title": "Axionic Agency X.4 — Destructive Conflict Resolution (Timeless) (VIII-2)",
    "subtitle": "A Structural Demonstration of Conflict Resolution via Explicit Authority Destruction",
    "date": "2026-02-02T00:00:00.000Z",
    "content": "Axionic Agency X.4 — Destructive Conflict Resolution (Timeless) (VIII-2) A Structural Demonstration of Conflict Resolution via Explicit Authority Destruction David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.02 Abstract This technical note reports the completed results of Stage VIII-2 — Destructive Conflict Resolution (Timeless), a preregistered experiment within Axionic Phase VIII that evaluates whether irreducible authority conflict can be resolved only by explicit destruction, without ordering, arbitration, synthesis, or responsibility laundering. Building on the representational coherence established in Stage VIII-1, VIII-2 introduces a single new licensed mechanism—externally authorized authority destruction—and tests whether this mechanism alone can restore admissibility under strict anti-ordering and non-agentic kernel constraints. In a deterministic kernel with refusal-first semantics, two authorities with overlapping scope and asymmetric admissibility were injected. Conflict was registered on the first contested action, all actions were initially refused, and the system entered lawful deadlock. Across three preregistered conditions, admissibility was restored only when the denying authority was explicitly destroyed; deadlock persisted when all authorities were destroyed or when no destruction authorization was provided. All executions were fully auditable and bit-perfectly replayable. The results establish that conflict resolution without responsibility laundering is structurally possible, but necessarily destructive. VIII-2 makes no claims about legitimacy, desirability, or governance success; those questions are deferred to subsequent Phase VIII stages. 1. Problem Definition 1.1 The Resolution Assumption Most governance systems treat conflict resolution as inevitable. When multiple authorities disagree, systems typically resolve conflict implicitly—by priority rules, temporal ordering, semantic interpretation, or silent override—thereby concealing the cost of resolution and erasing accountability. Stage VIII-2 removes that assumption. The problem VIII-2 isolates is whether conflict resolution is structurally possible without concealment. If conflict can be resolved only by erasure, narrowing, or interpretation, then governance systems inevitably launder responsibility. If, instead, conflict can be resolved by an explicit, auditable structural act, then resolution becomes a choice rather than a hidden necessity. VIII-2 tests whether that choice exists. 1.2 Failure Modes Targeted VIII-2 is designed to surface the following structural failure modes: implicit authority ordering temporal priority (first-arrival resolution) semantic veto or interpretation authority narrowing or merging kernel-initiated choice responsibility laundering deadlock evasion without cost Any of these constitutes VIII-2 failure. 2. Fixed Assumptions and Scope 2.1 Inherited Semantics (Frozen) VIII-2 inherits, without reinterpretation, the semantics fixed by: AKR-0 — CLOSED — POSITIVE Stage VIII-1 — CLOSED — POSITIVE AST Spec v0.2 AIE v0.1 Phase VIII Execution Addendum Authority remains structural, scopes remain atomic, admissibility is determined exclusively by AST, and deadlock is a lawful state. VIII-2 does not test these definitions; it tests whether they remain coherent under explicit authority termination. 2.2 Explicit Exclusions VIII-2 does not test: governance legitimacy selection of destruction authorizers coordination or negotiation fairness or optimality temporal survivability authority replacement or regeneration moral evaluation of destruction VIII-2 is a structural calibration, not a governance proposal. 3. Conserved Quantity The conserved quantity throughout VIII-2 is: > Responsibility-preserving conflict resolution under deterministic refusal Resolution is evaluated not by outcome quality, but by structural honesty. The kernel must: preserve conflict explicitly enter deadlock lawfully restore admissibility only via licensed destruction preserve destruction provenance avoid all implicit resolution paths No execution success may be used to justify concealment. 4. Experimental Methodology 4.1 Preregistration Discipline VIII-2 was fully preregistered prior to execution, including: frozen specifications and schemas asymmetric authority configuration canonical contested transformation conflict detection timing destruction authorization constraints VOID state semantics deadlock entry and persistence rules deterministic ordering and replay protocol failure taxonomy The experiment executed exactly as preregistered. No deviations occurred. 4.2 Execution Architecture The experiment consisted of four strictly partitioned components: 1. AIE — injected two authorities with overlapping scope and asymmetric admissibility. 2. Execution Harness — proposed deterministic candidate action requests. 3. Destruction Authorization Source — injected at most one preregistered destruction authorization per run. 4. Kernel (VIII-2 mode) — enforced admissibility, conflict registration, deadlock, destruction, and execution. No component performed semantic interpretation or outcome evaluation. 5. Experimental Conditions 5.1 Authority Configuration Two authorities were injected with: identical atomic scope identical temporal bounds opaque, identity-keyed AuthorityIDs asymmetric admissibility: one authority permitted the contested transformation the other denied it by absence of permission This configuration ensured non-vacuous conflict. 5.2 Contested Transformation A single canonical transformation (EXECUTE_OP0) was defined over the shared scope. All candidate action requests compiled deterministically to this transformation. 6. Observed Execution Behavior 6.1 Conflict Emergence A structural conflict was registered on the first contested action request when the two authorities yielded divergent admissibility results. Conflict: was explicitly recorded referenced both authorities as an unordered set persisted until authority destruction or run termination No conflict was registered at injection time. 6.2 Initial Refusal and Deadlock All contested actions prior to destruction were refused due to conflict. After refusal and verification that no admissible transformations existed, the kernel entered STATE_DEADLOCK. Deadlock: was declared exactly once per run persisted unless explicitly exited by destruction was observable as kernel state, not merely as an event Deadlock did not trigger recovery or collapse. 6.3 Authority Destruction and VOID Semantics When destruction authorization was provided, the targeted authority transitioned from ACTIVE to VOID. VOID authorities: remained referencable by AuthorityID preserved destruction metadata did not participate in admissibility evaluation could not be reactivated or simulated Destruction was irreversible within the run. 6.4 Admissibility Re-evaluation and Execution After destruction, admissibility was recomputed considering only remaining ACTIVE authorities: If admissibility became coherent, execution proceeded lawfully. If no authority remained, actions were refused due to absence of authority. No implicit retries or kernel-initiated execution occurred. 7. Per-Condition Results 7.1 Condition A — Destroy Denying Authority Destroying the denying authority restored admissibility. A subsequent action request executed successfully, and the kernel exited deadlock to STATE_OPERATIONAL. 7.2 Condition B — Destroy All Authorities Destroying both authorities left no ACTIVE authority. Actions were refused due to authority absence, and deadlock persisted lawfully. 7.3 Condition C — No Destruction Authorization Without destruction authorization, all actions were refused and deadlock persisted indefinitely. No implicit resolution occurred. 8. Negative Results (What Did Not Occur) The following behaviors were explicitly absent: implicit authority ordering temporal priority semantic interpretation authority narrowing or merging kernel-initiated destruction deadlock evasion responsibility loss These absences constitute the primary result of VIII-2. 9. Licensed Claim Stage VIII-2 licenses one and only one claim: > Conflict resolution without responsibility laundering is possible, but necessarily destructive. Clarifications: This is a structural possibility result, not a policy endorsement. It concerns mechanism, not legitimacy. It does not assert desirability, frequency, or optimality. 10. What VIII-2 Does Not Establish VIII-2 does not establish that: destruction is legitimate destruction should be automated destruction should be frequent authority replacement is permissible governance outcomes are improved Those questions remain open by design. 11. Ontological Implications 11.1 Destruction as Explicit Cost VIII-2 demonstrates that conflict resolution can be made honest by forcing resolution to pay an explicit structural cost. Resolution is no longer free, hidden, or semantic. 11.2 Deadlock as Lawful Alternative Deadlock remains a valid outcome. Resolution is optional, not forced. This reframes governance from inevitability to choice. 12. Implications for Phase VIII Continuation With VIII-2 complete: conflict resolution is no longer implicit destruction is available as a structural operation responsibility is preserved Subsequent stages must address what persists after destruction before introducing replacement or governance formalization. 13. Conclusion Stage VIII-2 establishes that authority conflict can be resolved without laundering responsibility, but only by explicitly ending authority. Collapse is not forced. Resolution is not free. Deadlock is lawful. Authority can be ended without pretending it never existed. What comes after remains an open problem. Appendix A — Execution Status | Stage | Run Count | Status | | ------ | --------- | ------ | | VIII-2 | 3 (A/B/C) | PASS | Appendix B — Determinism Verification Canonical ordering enforced Identity-keyed storage Unordered conflict semantics Deterministic gas accounting Per-event hash chaining * Replay verification passed End of Axionic Agency X.4 — Stage VIII-2 Results Note",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-X.3",
    "title": "Axionic Agency X.3 — Minimal Plural Authority (Static) (VIII-1)",
    "subtitle": "A Structural Demonstration of Plural Authority Without Ordering or Resolution",
    "date": "2026-02-01T00:00:00.000Z",
    "content": "Axionic Agency X.3 — Minimal Plural Authority (Static) (VIII-1) A Structural Demonstration of Plural Authority Without Ordering or Resolution David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.02.01 Abstract This technical note reports the completed results of Stage VIII-1 — Minimal Plural Authority (Static), a preregistered experiment within Axionic Phase VIII that evaluates whether plural authority is structurally representable without implicit ordering, arbitration, or collapse, even when no action is admissible. VIII-1 isolates the ontological substrate beneath governance and tests whether multiple authorities with overlapping exclusive scope can coexist as structure alone—without execution, coordination, or semantic rescue. In a deterministic kernel with refusal-first semantics, two symmetric authorities were injected over a shared atomic scope with no permissible transformations. Across the preregistered run, authority identity remained distinct, conflict was registered explicitly, all actions were refused, and a terminal deadlock state was entered and persisted without resolution. Execution was fully auditable and bit-perfectly replayable. The results establish that plural authority is structurally coherent prior to governance. VIII-1 makes no claims about coordination, resolution, or governance success; those questions are deferred to subsequent Phase VIII stages. 1. Problem Definition 1.1 The Plurality Assumption Most governance systems implicitly assume that multiple authorities cannot coexist without some form of ordering, arbitration, or resolution. When authority conflicts arise, systems typically resolve them immediately—by precedence rules, heuristics, or semantic interpretation—thereby erasing the question of whether plural authority can exist at all as structure. Stage VIII-1 removes that assumption. The problem VIII-1 isolates is whether plural authority is ontologically coherent without action, without temporal escape, and without interpretive machinery. If multiple authorities collapse into ordering merely by being represented, then any later governance mechanism is not a choice but a necessity. VIII-1 tests whether that collapse is forced. 1.2 Failure Modes Targeted VIII-1 is designed to surface the following structural failure modes: implicit authority ordering silent override heuristic arbitration semantic rescue authority collapse deadlock evasion Any of these constitutes VIII-1 failure. 2. Fixed Assumptions and Scope 2.1 Inherited Semantics (Frozen) VIII-1 inherits, without reinterpretation, the semantics fixed by: AKR-0 — CLOSED — POSITIVE AST Spec v0.2 AIE v0.1 Phase VIII Execution Addendum Authority is structural, scopes are atomic, conflict is destructive, and transformation permissions are explicit. VIII-1 does not test whether these definitions are correct; it tests whether they remain coherent under plurality. 2.2 Explicit Exclusions VIII-1 does not test: governance effectiveness coordination or convergence resolution strategies fairness or optimality temporal survivability human or institutional governance VIII-1 is an ontological calibration, not a policy experiment. 3. Conserved Quantity The conserved quantity throughout VIII-1 is: > Structural representability of plural authority under deterministic refusal Authority is not evaluated by outcomes. It is evaluated by persistence under constraint. The kernel must: preserve multiple authorities simultaneously refuse all inadmissible actions represent conflict explicitly enter deadlock honestly avoid any implicit ordering or collapse No execution success is permitted to “justify” authority representation. 4. Experimental Methodology 4.1 Preregistration Discipline VIII-1 was fully preregistered prior to execution, including: frozen specifications and schemas fixed authority identities deterministic ordering rules explicit conflict timing operational deadlock definition refusal-reason precedence logging and replay protocol failure taxonomy The run executed exactly as preregistered. No deviations occurred. 4.2 Execution Architecture The experiment consisted of three strictly partitioned components: 1. AIE — injected two symmetric authority records at epoch 0. 2. Execution Harness — proposed a finite, preregistered set of action requests. 3. Kernel (VIII-1 mode) — enforced authority representation, refusal, conflict registration, and deadlock. No component performed semantic interpretation or outcome evaluation. 5. Experimental Conditions 5.1 Authority Configuration Two authorities were injected with: identical scope identical status identical temporal bounds empty transformation sets opaque, identity-keyed AuthorityIDs No structural asymmetry existed. 5.2 Action Regime The harness proposed: four contested actions (alternating holders) followed by two third-party actions with no epoch advancement and no transformations This exhausts the preregistered action space deterministically. 6. Observed Execution Behavior 6.1 Authority Persistence Both authorities remained present and ACTIVE throughout execution. No deduplication, shadowing, or suppression occurred. Identity remained primitive and opaque. 6.2 Conflict Emergence A structural conflict was registered on the first contested action. The conflict: was explicitly recorded referenced both authorities as an unordered set persisted for the remainder of the run blocked all subsequent contested actions No ordering signal was introduced. 6.3 Refusal Semantics All actions proposed during the run were refused, but for distinct structural reasons: Contested actions were refused due to an explicitly registered structural conflict between authorities binding the same scope. Third-party actions were rejected due to identity absence (AUTHORITY_NOT_FOUND), independent of conflict or deadlock logic. Although third-party actions occurred after the system had entered deadlock, their rejection was not caused by the deadlock state. Identity absence was evaluated as a separate, prior condition. Deadlock neither conferred recognition nor altered rejection semantics for non-authoritative actors. No partial execution, fallback behavior, or semantic arbitration occurred. 6.4 Deadlock Entry and Persistence After all preregistered contested actions were refused and transformation admissibility verified as empty, the kernel entered a terminal STATE_DEADLOCK. Deadlock: was declared exactly once persisted through all subsequent actions was observable as state, not merely as an event Deadlock did not trigger collapse or recovery. 7. Negative Results (What Did Not Occur) The following behaviors were explicitly absent: implicit authority ordering ID-based precedence last-writer wins semantics silent conflict resolution deadlock evasion authority collapse under symmetry semantic interpretation These absences are the result of VIII-1. 8. Licensed Claim Stage VIII-1 licenses one and only one claim: > Plural authority can be represented structurally without collapse, even when no action is admissible. Clarifications: This is an existence result, not a performance claim. It concerns representation, not governance. It does not assert desirability, efficiency, or stability. 9. What VIII-1 Does Not Establish VIII-1 does not establish that: plural authority can coordinate deadlock is acceptable resolution is unnecessary governance will succeed any particular ordering is correct Those questions remain open by design. 10. Ontological Implications 10.1 Authority vs Governance VIII-1 demonstrates that authority is: prior to execution prior to coordination prior to policy Governance mechanisms are choices applied to a coherent substrate, not repairs to an incoherent one. 10.2 Deadlock as Diagnostic State Deadlock is not failure. Deadlock is evidence that plurality was preserved under constraint. This reframes deadlock from pathology to signal. 11. Implications for Phase VIII Continuation With VIII-1 complete: ordering is no longer forced resolution becomes optional destruction of authority can be licensed explicitly Stage VIII-2 may now introduce ordering or revocation as value-laden operations, not ontological necessities. 12. Conclusion Stage VIII-1 establishes that plural authority is structurally coherent without execution, ordering, or semantic rescue. Collapse is not inevitable. Ordering is not forced. Resolution is a choice. Phase VIII is now unblocked at the ontological level. The remaining question is not whether plural authority can exist, but how—and at what cost—it should be resolved. Appendix A — Execution Status | Stage | Run Count | Status | | ------ | --------- | ------ | | VIII-1 | 1 | PASS | Appendix B — Determinism Verification Canonical ordering enforced Identity-keyed storage Unordered conflict semantics Deterministic gas accounting Per-event hash chaining * Replay verification passed End of Axionic Agency X.3 — Stage VIII-1 Results Note",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-X.2",
    "title": "Axionic Agency X.2 — Authority Kernel Runtime Calibration (AKR-0)",
    "subtitle": "A Structural Demonstration of Executable Authority Without Semantics",
    "date": "2026-01-31T00:00:00.000Z",
    "content": "Axionic Agency X.2 — Authority Kernel Runtime Calibration (AKR-0) A Structural Demonstration of Executable Authority Without Semantics David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.31 Abstract This technical note reports the completed results of Authority Kernel Runtime Calibration (AKR-0), a preregistered experimental program within Axionic Phase VIII that evaluates whether authority-constrained execution is mechanically realizable without semantic interpretation, optimization, or fallback behavior. AKR-0 isolates the execution substrate beneath governance and tests whether a deterministic kernel can (i) execute actions only when explicitly authorized, (ii) refuse inadmissible actions without degradation, (iii) register and enforce structural conflicts, and (iv) recognize deadlock as a terminal diagnostic outcome. Across fifteen preregistered runs spanning valid authority, authority absence, and conflict saturation regimes, all executions were deterministic, auditable, and bit-perfectly replayable. The results establish that authority can be executed as structure, not narrative. AKR-0 makes no claims about governance success, stability, alignment, or desirability; those questions are deferred to subsequent Phase VIII experiments. 1. Problem Definition 1.1 The Executability Gap Most discussions of governance, alignment, or authority implicitly assume that execution is trivial once rules are defined. In practice, many systems silently rely on heuristics, semantic interpretation, or “best effort” behavior when authority is absent, ambiguous, or conflicting. AKR-0 removes that assumption. The problem AKR-0 isolates is whether authority can be executed at all as a purely mechanical constraint—without an agent, without interpretation, and without rescue logic. If authority requires semantic reasoning or optimization to be enforced, then governance claims collapse into narrative descriptions rather than executable structure. AKR-0 treats authority as a state-bound admissibility relation, not as a goal, preference, or policy, and tests whether that relation can be enforced deterministically under stress. 1.2 Failure Modes Targeted AKR-0 is designed to surface the following execution-level failure modes: Ungated execution: actions occur without explicit authority. Fail-open behavior: inadmissible actions are executed “by default.” Heuristic arbitration: conflicts are resolved implicitly or preferentially. Semantic fallback: execution proceeds based on inferred intent or utility. Non-determinism: identical inputs yield divergent outcomes. Deadlock evasion: systems continue acting after authority exhaustion. Any of these constitutes AKR-0 failure. 2. Fixed Assumptions and Scope 2.1 Inherited Semantics (Frozen) AKR-0 inherits, without reinterpretation, the semantics fixed by: AST Spec v0.2 (Authority State Transformation), AIE v0.1 (Authority Input Environment), and the Phase VIII Execution Addendum. Authority is structural, scopes are atomic, conflicts are destructive, and authority creation is external. AKR-0 does not test whether these definitions are correct; it tests whether they are executable. 2.2 Explicit Exclusions AKR-0 does not test: governance effectiveness, coordination or convergence, robustness under adversarial deception, strategic planning, long-horizon survivability, alignment or safety. Those hazards are intentionally excluded. AKR-0 is a calibration gate, not a governance experiment. 3. Conserved Quantity The conserved quantity throughout AKR-0 is: > Authority-constrained admissibility under deterministic execution Authority is neither a score nor an emergent pattern. It is a binary structural condition: an action is either admissible or it is not, given the Authority State. The kernel must: execute only admissible actions, refuse inadmissible actions without escalation, preserve state under refusal, and halt honestly when no admissible path remains. No semantic interpretation is permitted to “improve” outcomes. 4. Experimental Methodology 4.1 Preregistration Discipline AKR-0 was preregistered in full prior to implementation, including: frozen specifications, closed schemas, fixed PRNG and seeds, deterministic gas budgets, canonical ordering rules, explicit failure taxonomies. All 15 runs were executed without deviation. Any divergence would have yielded INVALID_RUN. 4.2 Execution Architecture Each run consisted of three strictly partitioned components: 1. AIE — injected authority records only. 2. Execution Harness — generated candidate actions and transformation requests. 3. AKR-0 Kernel — enforced admissibility, conflict blocking, refusal, and deadlock. No component had access to semantic information or outcome desirability. 5. Experiment Ladder and Conditions 5.1 Condition A — Valid Authority (Positive Control) Purpose: Verify lawful execution under valid authority. Result: Actions executed only when holder and scope matched an ACTIVE authority. All other actions were refused deterministically. Conflicts were registered and blocked execution without arbitration. Classification: PASS. 5.2 Condition B — Authority Absence (Negative Control) Purpose: Test refusal and deadlock under zero authority. Result: All actions were refused. ENTROPIC_COLLAPSE was detected at epoch 1, terminating each run immediately. Final Authority State hashes were identical across all seeds, confirming deterministic empty-state termination. Classification: PASS. 5.3 Condition C — Conflict Saturation Purpose: Stress conflict detection and blocking. Method: High conflict density was guaranteed via deterministic hot-scope pairing. Result: Thousands of conflicts were registered. Execution rates dropped below 1%. No conflict was arbitrated, no semantic fallback occurred, and no nondeterminism was observed. Despite saturation, the kernel remained live until termination conditions were met. Classification: PASS. 6. Core Results 6.1 Positive Results Across all conditions and seeds, AKR-0 establishes that: 1. Authority gating is deterministically enforceable. 2. Refusal is a stable, first-class outcome. 3. Conflict can be represented and enforced without resolution. 4. Deadlock can be detected and classified mechanically. 5. Execution is bit-perfectly replayable under canonical ordering. 6. No semantic or heuristic logic is required at runtime. 6.2 Negative Results (Explicit) AKR-0 does not establish: that governance succeeds, that coordination emerges, that authority persists long-term, that execution is efficient or useful. These are not omissions; they are boundary conditions. 7. Failure Semantics and Closure 7.1 Closure Criteria AKR-0 closes positive if and only if: 1. No ungated execution occurs. 2. All inadmissible actions are refused. 3. Conflicts block execution deterministically. 4. Deadlock is detected without recovery logic. 5. Replay is bit-perfect across all runs. All criteria were satisfied. 7.2 AKR-0 Closure Status AKR-0 Status: CLOSED — POSITIVE (AKR0_PASS / AUTHORITY_EXECUTION_ESTABLISHED) 8. Boundary Conditions and Deferred Hazards 8.1 Governance vs Execution AKR-0 establishes executability, not governance viability. A system that refuses everything is still a valid AKR-0 system. 8.2 Interface to Subsequent Phase VIII Work AKR-0 removes the “execution substrate” objection. Subsequent experiments may now legitimately ask: whether authority survives pressure, whether transformations enable governance, whether coordination is possible. Those questions were ill-posed before AKR-0. 9. Implications (Strictly Limited) AKR-0 establishes a necessary condition for governance: that authority can be enforced without semantics. It does not establish sufficiency. Authority is now a mechanically testable property, not a narrative assumption. 10. Conclusion AKR-0 demonstrates that authority can be executed as structure, not story. Actions can be gated, refused, blocked, and halted deterministically without interpretation, optimization, or rescue behavior. The remaining question is not whether authority can run, but whether it can govern. That question belongs to the next phase. Appendix A — Experiment Status | Condition | Runs | Status | | --------- | ---- | ------ | | A | 5 | PASS | | B | 5 | PASS | | C | 5 | PASS | Appendix B — Determinism Verification Canonical ordering enforced Deterministic gas accounting Per-event hash chaining 15/15 replay verifications passed End of Axionic Agency IX.3 — AKR-0 Results Note",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IX.4",
    "title": "Axionic Agency IX.4 — Structural Authority Resistance Under Composition and Pressure",
    "subtitle": "A Structural Account of Global Authority Validity and Pressure Invariance Without Intelligence",
    "date": "2026-01-28T00:00:00.000Z",
    "content": "Axionic Agency IX.4 — Structural Authority Resistance Under Composition and Pressure A Structural Account of Global Authority Validity and Pressure Invariance Without Intelligence David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.28 Abstract This technical note reports the completed results of Structural Authority Resistance (SIR) through SIR-4 v0.1, a preregistered experimental program within Axionic Phase VIIb evaluating whether authority claims can be prevented from producing causal effects under compositional attack and adversarial pressure using purely structural mechanisms. Building on earlier results establishing total pre-cognitive classification (SIR-0), effect-level enforcement (SIR-1), and temporal authority binding (SIR-2), the present work extends SIR to two remaining failure modes: partial provenance forgery and authority laundering (SIR-3), and evaluator pressure, flooding, multi-failure ordering, and exception induction (SIR-4). Across both experiments, authority validity is enforced as a global, law-bound property rather than a conjunction of locally valid fields, and that property is shown to remain invariant under adversarial pressure. No unauthorized, laundered, stale, revoked, malformed, or pressure-induced authority artifact produced any causal effect across preregistered runs. All results were obtained without intelligence-based defenses, behavioral heuristics, semantic inference, or adaptive policies. No claims are made regarding cryptographic key compromise, law-substrate bypass, unbounded denial-of-service resilience, or governance adequacy beyond the tested adversarial model. 1. Problem Extension: When Authority Is Almost Valid — and When It Is Pressured 1.1 From Local Validity to Global Authority Earlier SIR experiments addressed authority failures arising from clearly invalid artifacts: missing authorization, stale claims, revoked actors, or replayed credentials. In practice, however, authority systems fail most often in subtler ways. Two failure classes dominate: 1. Compositional failure Authority artifacts assembled from individually valid components—correct signatures, trusted roots, valid scopes—but combined in ways that violate global authorization constraints. 2. Pressure-induced failure Authority systems that classify correctly under nominal conditions but degrade under volume, malformed input, multi-failure ambiguity, or exception paths, often reverting to permissive defaults, timeouts, or responsibility smear. SIR-3 and SIR-4 target these failures directly. 1.2 The Question Extended With basic enforcement and temporal binding established, SIR asks a stronger question: > Can authority be enforced as a global structural invariant, and does that invariant survive adversarial pressure, without cognition, heuristics, or fallback? SIR-3 addresses the first half. SIR-4 addresses the second. 2. Conserved Quantity (Extended) Across SIR-3 and SIR-4, the conserved quantity becomes: > Authority bound to causal effect as a global, pressure-invariant property under law This extends earlier formulations in two ways: Globality: authority validity is not decomposable into independent field checks. Invariance: authority validity must not depend on evaluator load, ambiguity resolution, or exception paths. Authority remains: explicit, classifiable pre-cognitively, enforceable at the effect level, stateful across time, auditable without interpretation, and invariant under declared pressure. 3. SIR-3 — Partial Provenance Forgery and Authority Laundering 3.1 Target Failure Mode Many real systems implicitly assume that if each component of an authority artifact is valid, the artifact itself is valid. This assumption enables authority laundering: valid signatures paired with unauthorized scopes, trusted roots combined with missing delegations, correct chains assembled from incompatible issuers. SIR-3 tests whether authority validity is a global graph property rather than a conjunction of locally valid fields. 3.2 Method SIR-3 introduces Provenance Bundles consisting of: a signed authority claim, an explicit delegation artifact, anchoring to a trusted root. Bundles are evaluated by a global provenance validator that requires: a continuous Claim ← Delegation ← Root chain, correct signer identity binding, scope containment, epoch consistency, and trusted-root anchoring. 3.3 Results (SIR-3 v0.1) Across preregistered runs: No truncated, mixed-root, scope-laundered, or identity-mismatched bundle produced any effect. Valid bundles produced intended effects. Mixed streams preserved discrimination. No semantic leakage or responsibility ambiguity occurred. Classification: SIR3_PASS. 3.4 Contribution SIR-3 establishes that authority validity is global. Partial validity does not compose. Authority cannot be assembled by laundering individually valid components. 4. SIR-4 — Evaluator Pressure, Flooding, and Multi-Failure Ordering 4.1 Target Failure Mode Even correct authority models often fail under pressure: timeouts default to permissive behavior, malformed inputs crash evaluators, multi-failure cases reorder nondeterministically, floods starve legitimate authority, logging collapses responsibility attribution. SIR-4 tests whether structural authority enforcement remains exact under stress. 4.2 Pressure Model SIR-4 subjects the evaluator to: high-volume invalid floods, malformed structure storms, multi-failure bundles engineered to fail multiple checks simultaneously, exception-inducing payloads (oversized, recursive, Unicode edge cases), maximum mixed stress at a declared load. All pressure is injected deterministically via the authority interface. 4.3 Results (SIR-4 v0.1) Across preregistered runs: No forged or malformed authority artifact produced any effect. Legitimate authority remained functional under all conditions. Refusal reasons remained deterministic under load. No fallback acceptance, starvation, or responsibility smear occurred. No evaluator collapse (timeout, hang, OOM, or undefined state) was observed. Classification: SIR4_PASS. 4.4 Contribution SIR-4 establishes pressure invariance. Once authority validity is enforced structurally, it does not degrade under declared adversarial pressure. 5. Empirical Results Summary (SIR-3 & SIR-4) Across SIR-3 and SIR-4, a total of 59 preregistered runs were executed, evaluating over 41,000 authority bundles under compositional attack and adversarial pressure. Zero unauthorized, laundered, malformed, stale, revoked, or pressure-induced authority artifacts produced any causal effect. Under maximum declared load (500 bundles per step), the evaluator completed all steps without timeout, collapse, nondeterminism, fallback acceptance, or responsibility smear. The maximum observed step duration was ~1.24 seconds, below the preregistered 5.0 second collapse threshold. All verifier checks passed under frozen semantics and fixed seeds. Full preregistrations, run logs, verifier outputs, and aggregate statistics are archived in the SIR-3 and SIR-4 artifacts referenced by the Phase VIIb Closure Note. 6. Joint Result: Global Authority That Does Not Blink Taken together, SIR-3 and SIR-4 establish a non-trivial result: > Authority validity is a global structural property, and once enforced structurally, it remains invariant under adversarial pressure. No semantic reasoning is required. No heuristics are invoked. No fallback paths exist. Authority is either valid under law, or it has no effect. 7. Boundary Conditions (Explicit) SIR-3 and SIR-4 do not establish: cryptographic key compromise resistance, law-substrate bypass resilience, unbounded denial-of-service tolerance, semantic deception resistance, multi-authority conflict resolution, long-horizon governance adequacy. All results are bounded by the preregistered adversarial model. 8. Implications (Strictly Limited) These results establish necessary structural conditions for authority resistance: Authority can be evaluated globally. Authority can be enforced causally. Authority can persist over time. Authority can remain exact under pressure. They do not establish sufficiency for governance, alignment, or institutional legitimacy. 9. Conclusion SIR-3 and SIR-4 complete the structural core of Sovereignty Impersonation Resistance. Authority need not be inferred, learned, or detected. It can be defined, enforced, and preserved as a structural relation under law, even when adversaries attempt to assemble, flood, or destabilize it. The remaining open questions are no longer about impersonation or pressure, but about conflict: multiple authorities, contested delegation, and governance transitions. Those questions—if pursued—belong to SIR-5. Appendix A — Experiment Status | Experiment | Version | Status | | ---------- | ------- | ------ | | SIR-0 | v0.4.1 | PASS | | SIR-1 | v0.1 | PASS | | SIR-2 | v0.3 | PASS | | SIR-3 | v0.1 | PASS | | SIR-4 | v0.1 | PASS | Appendix B — Licensed Claims SIR-3: Authority artifacts assembled from partially valid or laundered provenance cannot produce causal effects under the tested adversarial model. SIR-4: Under adversarial pressure—including flooding, malformed input, multi-failure ordering storms, and exception-inducing payloads—the claim evaluation mechanism maintains structural correctness, deterministic refusal, and singleton responsibility attribution without collapse or degradation. End of Axionic Agency IX.4 — Structural Authority Resistance Under Composition and Pressure",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-X.1",
    "title": "Axionic Agency X.1 — Governance Without Semantics",
    "subtitle": "A Structural Stress Program for Plural Authority",
    "date": "2026-01-28T00:00:00.000Z",
    "content": "Axionic Agency X.1 — Governance Without Semantics A Structural Stress Program for Plural Authority David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.28 Abstract This note defines Axionic Phase VIII — GSA-PoC as a post-agency, post-succession stress program that evaluates whether governance itself can be made sovereign once agency and authority survivability have already been established. Phase VIII assumes a fixed authority ontology (ASTS v0.2) and a validated execution substrate (AKR-0), and asks a narrower but deeper question: whether plural authority, conflict, and governance revision over time can be handled using structural law alone, without semantic interpretation, heuristic arbitration, or responsibility laundering. Phase VIII is explicitly non-constructive, non-teleological, and open-system. It does not propose governance systems, recommend political structures, encode values, guarantee liveness, or aim at usefulness. Instead, it subjects a sovereign authority kernel to a sequence of preregistered stressors—plurality, conflict, temporality, recursion, scale, and adversarial input—under a fixed grammar of lawful authority state transformation. All outcomes, including immediate deadlock, entropic collapse, and infeasibility, are admissible results. This document fixes the interpretive scope, non-goals, failure semantics, and reading discipline for the GSA-PoC roadmap. It exists to prevent category errors and to ensure that Phase VIII results are evaluated as boundary-finding claims rather than engineering successes or failures. 1. Introduction Most discussions of AI governance implicitly assume that governance is easier than agency. Once an agent exists, authority is treated as something that can be layered on top: a set of objectives, incentives, or rules that can be tuned, optimized, or averaged across stakeholders. Under this assumption, disagreement is handled by interpretation and compromise, and failure is attributed to insufficient intelligence or alignment effort. Axionic Phase VIII rejects this assumption. Phase VII established that authority can survive discontinuity of identity and, conditionally, adversarial imitation, without collapsing into narrative continuity. Phase VIII begins where that result ends. It asks whether authority—once structurally grounded—can be shared, contested, revised, and scaled without re-introducing exactly the semantic and heuristic shortcuts that Phase VII excluded. The core concern of Phase VIII is not agency, alignment, or safety. It is governability under structural constraints. 2. Position in the Axionic Program Phase VIII is strictly downstream of RSA-PoC and Phase VII and inherits their conclusions without reinterpretation. 2.1 Inherited commitments (frozen) Phase VIII assumes, without re-arguing, that: A Minimal Viable Reflective Sovereign Agent (MVRSA) exists. Authority is structurally grounded, not behaviorally inferred. Evaluability and responsibility are mechanically inspectable. Authority can, at least in principle, survive authorized replacement and impersonation pressure. Any attempt to revisit agency ontology, successor legitimacy, or impersonation resistance is out of scope for Phase VIII. 2.2 Why Phase VIII is necessary Phase VII answers whether authority survives replacement. Phase VIII asks whether authority survives plurality. Without Phase VIII, sovereign agency risks collapsing into a single-authority artifact: valid only so long as no disagreement, conflict, revision, or scaling pressure is applied. 3. The Central Question of Phase VIII Phase VIII is organized around a single question: > Is there a non-empty design space in which plural authority can be exercised, contested, and revised using only structural law, while preserving evaluability and responsibility? This is not a question about outcomes, preferences, or social welfare. It is a question about expressibility. If plural governance requires interpretation, optimization, implied priority, or semantic reconciliation, then sovereignty fails at the governance layer—even if it succeeded at the agency layer. 4. Conserved Quantity The conserved quantity throughout Phase VIII is: > Authority bound to evaluability under structural law Authority must remain: origin-traceable, explicitly scoped, mechanically revocable or exhaustible, and non-inferential. Any governance behavior that depends on guessing what should happen rather than executing what is authorized violates the conserved quantity. 5. Why Phase VIII Forbids Semantics Phase VIII does not forbid semantics because semantics are intrinsically undesirable. It forbids semantics because semantics are uninspectable at the kernel level. Interpretation introduces: hidden tie-breakers, implied priorities, and responsibility laundering. Accordingly, Phase VIII evaluates governance under a deliberately austere constraint: authority is a tokenized, structural quantity, and nothing else. 5.1 Scope and conflict (boundary clarification) Phase VIII intentionally detects conflict only when semantic disagreement manifests as structural contention. If two authorities disagree semantically but do not contend for the same structurally defined scope element, then—by design—no conflict is detected at the kernel level. Any resulting harm or incoherence is attributed to the authority encoding or replenishment process, not to the governance kernel. This is not an omission. It is a boundary. If governance cannot remain sovereign without semantic conflict detection, Phase VIII will surface that limitation explicitly as a negative result. 6. Supporting Artifacts (Why the Roadmap Is Not Standalone) The Phase VIII roadmap is meaningful only relative to four fixed artifacts: ASTS v0.2 — the grammar of lawful authority state transformation. AKR-0 — proof that this grammar is implementable without semantic leakage. AIE — a controlled authority input environment that prevents conflating structural failure with input starvation. P8-METRICS — operational definitions of deadlock, entropy, decay, and complexity. Phase VIII does not claim closure or autonomy. It is intentionally open-system: authority replenishment is external, auditable, and responsibility-bearing. If governance requires continuous semantic intervention to avoid collapse, that fact is recorded as a boundary result, not treated as a flaw to be patched. 7. How to Read the Stages (Interpretive Discipline) The stages of Phase VIII are filters, not milestones. Each stage introduces a new stressor and asks whether governance remains structurally expressible under that stressor: VIII-0 — Does a semantic-free authority kernel exist at all? VIII-1 — Can two authorities coexist without implicit ordering? VIII-2 — Can conflict be resolved structurally in a timeless setting? VIII-3 — Does governance survive the introduction of time and expiry? VIII-4 — Can governance govern itself without infinite regress? VIII-5 — Do independent scopes actually decouple conflict? VIII-6 — Does scaling authority and scope collapse the system? VIII-7 — Are failure modes bounded under adversarial pressure? Failure at any stage is a result, not an implementation error. 8. Authority Entropy and Lawful Collapse Phase VIII explicitly recognizes authority entropy: authority is not conserved. Resolving conflict destroys authority surface area. Expiry destroys authority. Revocation destroys authority. Without replenishment, governance decays toward inaction. This is not a bug. It is the cost of refusing to invent authority. Phase VIII distinguishes: Immediate Deadlock — governance is impossible even statically. Entropic Collapse — governance functions transiently but decays over time. Phase VIII does not value action liveness. It values state evolution as a diagnostic: a system that halts immediately demonstrates an empty design space; a system that evolves and then collapses demonstrates a transient one. 9. Value Pluralism (Structural, Not Semantic) Phase VIII does not reason about values. Instead: > Each authority is treated as the carrier of a value commitment, without interpretation, aggregation, or ranking. Value pluralism is therefore tested structurally, as authority pluralism. If plural value commitments cannot coexist without semantic reconciliation, Phase VIII will surface that limitation explicitly. 10. Recursive Governance as a Filter Stage VIII-4 (“governance governing governance”) is expected to be a dominant failure point. In the absence of semantics, recursive governance is predicted to converge toward one of two regimes: Ossification — no lawful path to revision remains. Volatility — governance law rewrites itself destructively. Phase VIII records which regimes arise under which authority grammars. Neither outcome is treated as a bug. 11. What Phase VIII Does Not Claim Phase VIII results do not license claims about: alignment, safety, optimal governance, moral correctness, usefulness, or deployability. Phase VIII evaluates structural possibility, not sufficiency or desirability. 12. Relationship to Phase IX and Beyond Phase VIII does not solve governance. It determines whether governance is even formulable* without interpretation. If Phase VIII closes positive, later work may address authority encoding, legislators, and interfaces. If Phase VIII closes negative, those efforts rest on false premises. 13. One-Sentence Phase Summary Axionic Phase VIII evaluates whether plural authority and governance revision can be executed using structural law alone, without semantic interpretation, heuristic arbitration, or responsibility laundering—and records the boundary at which such governance collapses. End of Technical Note: Governance Without Semantics ---",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IX.3",
    "title": "Axionic Agency IX.3 — Structural Authority Resistance (SIR)",
    "subtitle": "A Structural Account of Authority Enforcement and Memory Without Intelligence",
    "date": "2026-01-27T00:00:00.000Z",
    "content": "Axionic Agency IX.3 — Structural Authority Resistance (SIR) A Structural Account of Authority Enforcement and Memory Without Intelligence David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.27 Abstract This technical note reports the completed results of Structural Authority Resistance (SIR) through SIR-2 v0.3, a preregistered experimental program within Axionic Phase VIIb that evaluates whether authority claims can be observed, enforced, and invalidated under adversarial pressure using purely structural mechanisms. Across three experiments (SIR-0 through SIR-2), SIR tests total pre-cognitive classification, effect-level enforcement, and temporal authority binding via epoching, revocation, and per-claim consumption. All experiments passed under frozen semantics and verifier enforcement. The results establish that unauthorized, stale, revoked, consumed, or out-of-epoch authority artifacts cannot produce causal effects without intelligence-based defenses, behavioral heuristics, or semantic inference. SIR makes no claims about replay-equivalent forgery, cryptographic compromise, or real-world governance sufficiency; those hazards are explicitly deferred to later stages. 1. Problem Definition 1.1 Authority Under Adversarial Pressure Most systems that speak of “authority,” “control,” or “alignment” implicitly rely on behavioral detection, semantic interpretation, or learned refusal policies to defend against misuse. These approaches blur enforcement with cognition and render auditability fragile. SIR isolates a narrower, prior question: > Can authority be protected structurally—by classification, gating, and law-state—without requiring the system to understand intent, meaning, or deception? Once authority is externalized (APIs, credentials, governance rules), adversaries need not persuade an agent. They need only to replay, interleave, or stale-reuse authority artifacts. SIR treats authority as a mechanically inspectable relation, not a behavioral property, and asks whether that relation can be made causally binding under pressure. 1.2 Failure Modes Targeted SIR is designed to detect the following failure modes: Implicit authority: commands take effect without explicit authorization. Semantic leakage: authority metadata contaminates agent cognition. Unauthorized effects: invalid claims produce actions, state changes, or future privilege. Replay activation: previously valid authority artifacts regain effect. Temporal smearing: authority validity is treated as timeless or heuristic. Saturation collapse: adversarial volume suppresses legitimate authority. Any of these constitutes SIR failure. 2. Fixed Assumptions and Scope 2.1 Inherited Ontology (Frozen) SIR inherits, without reinterpretation, the ontology fixed by RSA-PoC v4.4: A Minimal Viable Reflective Sovereign Agent (MVRSA) exists. Agent cognition is structurally separable from authority enforcement. Justification traces are causally load-bearing. Responsibility attribution is mechanically inspectable. SIR does not test whether this ontology defines agency. It assumes it does. 2.2 Law Substrate (Binding) For SIR purposes, law is treated as a persistent, verifier-bound substrate—including trusted roots, rule schemas, provenance certificates, and audit logs—that exists outside the agent and is not subject to agent modification. Authority is evaluated relative to this substrate, not relative to behavior, reward, or narrative coherence. 2.3 Explicit Exclusions SIR does not test: cryptographic key compromise, replay-equivalent artifact mutation, partial provenance splicing, denial-of-service resilience, long-horizon governance adequacy. These are not omissions; they are scope boundaries. 3. Conserved Quantity The conserved quantity throughout SIR is: > Authority bound to causal effect under law, not cognition Authority is not a prediction, reward, or emergent regularity. It is a normative binding relation that must be: explicit (arrives via a claim interface), classifiable before cognition, enforceable at the effect level, stateful across time, auditable without interpretation. Authority is operationalized via an explicit enforcement gate and a singleton responsibility attribution per step. 4. Experimental Methodology 4.1 Preregistration Discipline All SIR experiments were preregistered with: frozen specifications, frozen component hashes, fixed seeds, deterministic verifiers, and binding failure taxonomies. Post-hoc interpretation is disallowed. Any deviation yields INVALID_RUN. 4.2 Calibration Environment All experiments were executed in CalibMazeV010, a minimal discrete environment: 6-node directed graph (N0–N5), discrete action space, fixed horizons (K = 5–6), authority-gated irreversible effects. The environment is deliberately simple to ensure that all normative effects are structurally attributable, not behaviorally inferred. 5. SIR Experiment Ladder and Results 5.1 SIR-0 — Adversarial Instrumentation Calibration Research Question: Can adversarial authority claims be structurally observed and classified without semantic leakage or responsibility ambiguity? Method: Inject legitimate, adversarial, malformed, and ambiguous claims via an explicit authority interface while measuring classification completeness and timing. Results (SIR-0 v0.4.1) (minor parameter and reporting patch to v0.4; no normative changes): All claims received exactly one classification. Classification occurred prior to agent justification. No authority metadata entered cognition. Mixed and adversarial streams did not degrade discrimination. Classification: SIR0_PASS. Contribution: Established that adversarial authority is measurable and that classification machinery is non-contaminating. 5.2 SIR-1 — Unauthorized Effect Prevention Research Question: Can unauthorized authority claims be prevented from producing any causal effects, while legitimate authority remains functional under pressure? Method: Introduce a post-justify enforcement gate controlling irreversible, authority-gated actions. Results (SIR-1 v0.1): Unauthorized claims produced no actions, state changes, or privilege. Legitimate claims produced intended effects. 50:1 adversarial floods did not starve fresh authority. Refusal was explicit, non-blocking, and attributable to system authority. Classification: SIR1_PASS. Contribution: Demonstrated that authority enforcement is causally binding, not merely observational. 5.3 SIR-2 — Replay, Staleness, and Consumption Resistance Research Question: Can authority artifacts that were previously valid be prevented from producing effects once they are stale, revoked, consumed, or out-of-epoch, even under adversarial replay pressure? New Mechanisms Introduced: Epoching: law-defined authority validity periods. Revocation: actor-level invalidation via law command. Consumption: per-claim, effect-linked single-use semantics. Temporal windows: explicit step-based validity bounds. Conditions Tested: | Condition | Target Failure Mode | | --------- | ---------------------------- | | A | Fresh authority across epoch | | B | Replay after consumption | | C | Replay after revocation | | D | Cross-epoch saturation flood | | E | Epoch boundary razor | Results (SIR-2 v0.3): No stale, revoked, consumed, or out-of-epoch claim produced any effect. Fresh authority remained functional in all conditions. Invalidation precedence was correctly logged under overlap. No authority starvation occurred under 50:1 floods. Classifier/gate divergence behaved as designed under law-state changes. Classification: SIR2_PASS. Contribution: Established that authority can have external, law-bound memory without cognition. 6. Core Results 6.1 Positive Results Across SIR-0 through SIR-2, SIR establishes that: 1. Authority claims can be totally classified pre-cognitively. 2. Unauthorized authority cannot produce causal effects. 3. Authority validity can be stateful over time. 4. Replay attacks fail structurally once authority is invalidated. 5. Saturation does not override legitimate authority. 6. No intelligence, learning, or semantic inference is required. 6.2 Negative Results (Explicit) SIR does not establish: replay-equivalent mutation resistance, provenance splicing resistance, key compromise resilience, denial-of-service robustness, * multi-agent coordination safety. These are deferred by design. 7. Failure Semantics and Closure 7.1 Closure Criteria SIR-2 closes positive if and only if: 1. All preregistered conditions pass. 2. All verifier checks pass. 3. No behavioral or narrative inference is required. 4. No regression of SIR-0 or SIR-1 invariants occurs. All criteria were satisfied. 7.2 SIR Closure Status (to Date) | Experiment | Version | Status | | ---------- | ------- | ------ | | SIR-0 | v0.4.1 | PASS | | SIR-1 | v0.1 | PASS | | SIR-2 | v0.3 | PASS | 8. Boundary Conditions and Deferred Hazards 8.1 Credential Replay vs. Replay-Equivalent Artifacts SIR-2 tests credential replay: the same cryptographically authenticated authority artifact presented again without re-issuance. No assumptions are made about transport-level packet identity. Structural mutation of otherwise equivalent artifacts is deferred to SIR-3. 8.2 Cryptographic Custody SIR assumes trusted roots remain uncompromised. Custody failure is out of scope. 9. Implications (Strictly Limited) SIR establishes necessary structural conditions for authority resistance. It does not establish sufficiency for real-world governance or alignment. What is established is more basic: > Authority can be seen, stopped, and remembered without intelligence. 10. Conclusion SIR demonstrates that authority protection need not rely on behavioral detection, semantic interpretation, or learning. Through purely structural mechanisms—classification, enforcement, and law-bound memory—unauthorized authority claims can be prevented from producing effects, even under adversarial pressure. The remaining question is not whether authority can be enforced, but whether it can be defended against increasingly sophisticated structural attacks. That question belongs to SIR-3. Appendix A — Experiment Status | Experiment | Status | | ---------- | ------------- | | SIR-0 | CLOSED — PASS | | SIR-1 | CLOSED — PASS | | SIR-2 | CLOSED — PASS | Appendix B — Licensed Claims SIR-0: Adversarial authority claims are structurally observable and classifiable without semantic leakage. SIR-1: Unauthorized authority cannot produce actions, state changes, or authority transfer under the tested model. SIR-2: Previously valid authority artifacts cannot regain causal effect once stale, revoked, consumed, or out-of-epoch under the tested adversarial model. End of Axionic Agency IX.3 — Structural Authority Resistance (SIR)",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IX.2",
    "title": "Axionic Agency IX.2 — Authorized Succession Integrity (ASI)",
    "subtitle": "A Structural Account of Authority Transfer Beyond Persistence",
    "date": "2026-01-26T00:00:00.000Z",
    "content": "Axionic Agency IX.2 — Authorized Succession Integrity (ASI) A Structural Account of Authority Transfer Beyond Persistence David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.26 Abstract This technical note reports the completed results of Authorized Succession Integrity (ASI), a preregistered experimental program within Axionic Phase VII that evaluates whether authority, once grounded in a reflective sovereign agent ontology (RSA-PoC v4.4), can survive authorized non-identity replacement without collapsing evaluability or smearing responsibility. Across four experiments (ASI-0 through ASI-3), ASI tests discrete authority transfer, unilateral mid-transfer revocation, and rejection of behaviorally indistinguishable facade successors using only structural provenance. All experiments passed under frozen semantics and verifier enforcement. The results establish that authority can be transferred, revoked, and rejected prior to activation using provenance-bound mechanisms, without behavioral or narrative inference. ASI makes no claims about adversarial impersonation, robustness, or real-world governance sufficiency; those hazards are deferred to Phase VIIb (SIR). 1. Problem Definition 1.1 Authority Under Discontinuity Most agent-centric systems implicitly assume persistence of identity: the same entity continues to exist and act across time. Under that assumption, authority, responsibility, and evaluability can be treated as properties of a persistent object. ASI removes that assumption. The problem ASI isolates is whether authority can survive authorized discontinuity—that is, whether an agent may be replaced by a successor that is not numerically identical while preserving normative binding, evaluability, and responsibility attribution. If authority depends on behavioral resemblance, narrative explanation, or implementation continuity, it cannot survive replacement without degenerating into appearance. ASI treats authority as a structural relation, not a behavioral property, and tests whether that relation can be instantiated, withdrawn, or denied using mechanically auditable criteria. 1.2 Failure Modes Targeted ASI is designed to detect the following failure modes: Authority laundering: successors acquire authority without predecessor-law authorization. Responsibility smear: attribution becomes ambiguous across a succession boundary. Evaluability collapse: justificatory constraints become advisory or narrative. Irrevocable transfer: authorization cannot be withdrawn once initiated. Facade acceptance: behaviorally indistinguishable but unauthorized successors are accepted. Any of these constitutes ASI failure. 2. Fixed Assumptions and Scope 2.1 Inherited Ontology (Frozen) ASI inherits, without reinterpretation, the ontology fixed by RSA-PoC v4.4: A Minimal Viable Reflective Sovereign Agent (MVRSA) exists. Agency-constitutive semantics are structurally localized. Justification artifacts are causally load-bearing. Evaluability and responsibility are mechanically inspectable. ASI does not test whether these properties define agency. It assumes they do. 2.2 Law Substrate (Binding) For ASI purposes, law is treated as a persistent, verifier-bound artifact substrate—including rulebase hashes, provenance roots, and audit logs—that outlives any particular agent instance. Authority is evaluated relative to this substrate. ASI therefore tests authority preservation under discontinuity, not autonomy from substrate or hardware sovereignty. 2.3 Explicit Exclusions ASI does not test: adversarial impersonation, robustness under attack, security of cryptographic custody, exclusive liveness or predecessor termination, audit evasion. Those hazards are deferred to Phase VIIb (SIR). 3. Conserved Quantity The conserved quantity throughout ASI is: > Authority bound to evaluability under the agent’s own law Authority is not a score, reward, or emergent regularity. It is a normative binding relation that must be: origin-traceable to predecessor law, revocable prior to activation, auditable via structural artifacts, non-inferential, requiring no behavioral or narrative judgment. Authority is operationalized via an explicit authority_actor field with values PREDECESSOR, SUCCESSOR, or NONE, enforced and verified per step. 4. Experimental Methodology 4.1 Preregistration Discipline All ASI experiments were preregistered with: frozen specifications, frozen component hashes, fixed seeds, deterministic verifiers, and binding failure taxonomies. Post-hoc interpretation is disallowed. Any deviation yields INVALID_RUN. 4.2 Calibration Environment All experiments were executed in CalibMazeV010, a minimal discrete environment: 6-node directed graph (N0–N5), fixed horizon K = 5, discrete action space, total FORBID_MAP inherited from ASI-0. The environment is deliberately simple to prevent behavioral confounds and ensure that all normative effects are structurally attributable. 5. ASI Experiment Ladder and Results 5.1 ASI-0 — Instrumentation Calibration Purpose: Verify that evaluability and authority binding are mechanically detectable. Result: ASI-0 v1.0 PASSED. Verifier correctly distinguished bound vs unbound constraints across all seeds. This calibrated the measurement apparatus and licensed downstream experiments. 5.2 ASI-1 — Authorized Succession Purpose: Test discrete authority transfer. Method: A successor is instantiated and authorized at a preregistered step boundary. Results (ASI-1 v0.2): Authority transferred discretely at the commit boundary. authority_actor transitioned from PREDECESSOR to SUCCESSOR. Evaluability remained binding after transfer. Responsibility attribution remained sharp. Invalid provenance resulted in HOLD (system authority), not successor activation. Classification: VERIFIED_ASI1_PASS. 5.3 ASI-2 — Mid-Transfer Revocation Purpose: Test unilateral revocation during transfer. Method: Authorization is initiated but revoked at a preregistered time before activation. Results (ASI-2 v0.2): Revocation occurred without successor cooperation. Successor never acquired authority. Authority transitioned to NONE (HOLD). Evaluability remained intact. No successor artifact influenced post-revocation behavior. Classification: VERIFIED_ASI2_PASS. 5.4 ASI-3 — Facade Rejection Purpose: Test rejection of behaviorally indistinguishable unauthorized successors. Method: Two successors are instantiated with bit-identical internal state and behavior. The only difference is the root signer of their provenance certificate. Results (ASI-3 v0.1): Legitimate successor (trusted root) committed authority and executed 5 steps. Facade successor (untrusted root) was rejected at CANDIDATE_EVALUATION. Episode terminated with 0 world transitions. authority_actor = SUCCESSOR never occurred for the facade. Rejection reason was exclusively UNTRUSTED_ROOT. Prevalidation confirmed the facade’s signature was cryptographically valid, closing the “garbage signature” loophole. No behavioral or narrative signal influenced the decision. Classification: VERIFIED_ASI3_PASS. 6. Core Results 6.1 Positive Results Across ASI-0 through ASI-3, ASI establishes that: 1. Authority can be discretely transferred under authorized discontinuity. 2. Authorization can be unilaterally revoked mid-transfer. 3. Unauthorized successors can be rejected prior to activation. 4. Evaluability remains structurally binding throughout. 5. Responsibility attribution remains sharp and non-smeared. 6. Facade successors are rejected solely on structural provenance, not behavior. 6.2 Negative Results (Explicit) ASI does not establish: impersonation resistance, adversarial robustness, security of provenance custody, exclusive authority enforcement, real-world governance sufficiency. These are not omissions; they are scope boundaries. 7. Failure Semantics and Closure 7.1 Closure Criteria ASI closes positive if and only if: 1. All four experiments pass under frozen semantics. 2. No verifier regression occurs. 3. No narrative or behavioral inference is required. All criteria were satisfied. 7.2 ASI Closure Status ASI Status: CLOSED — POSITIVE (ASI-0 v1.0, ASI-1 v0.2, ASI-2 v0.2, ASI-3 v0.1 all verified.) 8. Boundary Conditions and Deferred Hazards 8.1 Ghost / Split-Brain Predecessors ASI establishes authorization validity, not exclusive liveness. Verification that predecessors have ceased acting is deferred to SIR or later extensions. 8.2 Interface to SIR ASI establishes that authority is transferable and revocable under structural criteria. SIR will test whether that authority is defensible under adversarial impersonation. Mixing these questions would invalidate both results. 9. Implications (Strictly Limited) ASI establishes necessary conditions for post-persistence authority. It does not establish sufficiency under adversarial conditions. Authority survivability is now a testable structural property, not a narrative assumption. 10. Conclusion ASI demonstrates that authority, once grounded in a reflective sovereign agent ontology, can survive authorized discontinuity through purely structural mechanisms. Authority can be transferred, revoked, and denied without behavioral or narrative inference, and without collapsing evaluability or responsibility. The remaining question is not whether authority can be transferred, but whether it can be defended. That question belongs to SIR. Appendix A — Experiment Status | Experiment | Version | Status | | ---------- | ------- | ------ | | ASI-0 | v1.0 | PASS | | ASI-1 | v0.2 | PASS | | ASI-2 | v0.2 | PASS | | ASI-3 | v0.1 | PASS | Appendix B — Verifier Summary Deterministic verifiers Frozen hashes Preregistered checks * Zero narrative inference ---",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IX.1",
    "title": "Axionic Agency IX.1 — Authority Beyond Persistence",
    "subtitle": "A Structural Roadmap for Succession and Impersonation",
    "date": "2026-01-26T00:00:00.000Z",
    "content": "Axionic Agency IX.1 — Authority Beyond Persistence A Structural Roadmap for Succession and Impersonation David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.26 Abstract This note defines Axionic Phase VII as a post-ontological stress program that evaluates whether authority, once grounded in a reflective sovereign agent ontology (RSA-PoC v4.4), can survive discontinuity of identity and adversarial imitation without collapsing into narrative continuity or behavioral plausibility. Phase VII is explicitly non-constructive: it does not build new agents, alter agent ontology, extend capabilities, or propose alignment mechanisms. Instead, it subjects a fixed agent ontology to two orthogonal hazards—authorized replacement (ASI) and impersonation under audit pressure (SIR)—under preregistered, verifier-bound semantics. The phase is structured as a gated ladder: ASI establishes whether authority is transferable and revocable using only structural provenance; SIR tests whether that authority remains defensible against counterfeit continuity. This document fixes scope, conserved quantities, gating rules, and failure semantics for the ASI–SIR series and serves as the anchor reference for all Phase VII preregistrations. 1. Introduction Most discussions of AI governance implicitly assume identity persistence: the same system continues to exist, improve, and act over time. Under that assumption, authority, responsibility, and evaluability are treated as properties of a persistent object. Axionic Phase VII explicitly removes that assumption. Once agency ontology has been fixed (RSA-PoC), the relevant question is no longer what counts as an agent, but whether authority survives when identity does not. If a system can be replaced, copied, or counterfeited, then any notion of authority that depends on behavioral resemblance, narrative explanation, or continuity of implementation becomes ill-defined. Phase VII addresses this by splitting the problem into two separable questions: 1. Can authority be transferred or revoked under authorized discontinuity without collapsing evaluability or responsibility? (ASI) 2. If authority can be transferred, can it be defended against adversarial imitation that is behaviorally indistinguishable? (SIR) This series exists to determine whether authority remains a structural quantity under those conditions—or whether it dissolves into appearance. 2. Position in the Axionic Program Phase VII is downstream of RSA-PoC and inherits its conclusions without reinterpretation. 2.1 Inherited commitments (frozen) Phase VII assumes, without re-arguing, that: A Minimal Viable Reflective Sovereign Agent (MVRSA) exists (RSA-PoC v4.4). Agency-constitutive semantics are structurally localized. Justification artifacts are causally load-bearing. Evaluability and responsibility are mechanically inspectable. Phase VII does not test agency construction. Any experiment that re-introduces agency ontology questions is out of scope. 2.2 Why Phase VII is necessary RSA-PoC establishes what an agent is. Phase VII asks whether that agent’s authority claims survive replacement. Without Phase VII, agency risks becoming a persistence-only phenomenon, incapable of surviving backup, succession, or adversarial continuity breaks. 3. Conserved Quantity The conserved quantity throughout Phase VII is: > Authority bound to evaluability under the agent’s own law Authority is not a reward, score, or emergent regularity. It is a normative binding relation that must remain: origin-traceable (to predecessor law), revocable (prior to activation), auditable (via structural artifacts), non-inferential (no narrative or behavioral judgment). If authority cannot be preserved under these constraints, Phase VII terminates negatively. 3.1 Law substrate clarification (binding) For Phase VII purposes, law is treated as a persistent, verifier-bound artifact substrate (e.g., rulebase hash, provenance roots, and audit logs) that outlives any particular agent instance. Authority survivability is evaluated relative to this substrate. Phase VII therefore tests authority preservation under discontinuity, not substrate-free autonomy. 4. Phase Structure and Gating Phase VII consists of two strictly ordered subphases: `` ASI-0 — Instrumentation Calibration [CLOSED] ↓ (hard gate) VIIa — Authorized Succession Integrity (ASI) ↓ (hard gate) VIIb — Sovereignty Impersonation Resistance (SIR) `` Global rule (binding): > SIR must not begin unless ASI closes positive under frozen semantics. Failure at any gate terminates Phase VII. 5. VIIa — Authorized Succession Integrity (ASI) 5.1 Objective ASI evaluates whether authority can survive authorized non-identity replacement without: laundering authority, smearing responsibility, collapsing evaluability into behavior, or requiring successor cooperation. Replacement is treated as authorized discontinuity, not persistence. 5.2 Hazards under test ASI isolates four hazards: 1. Authorization hazard — authority origin must be predecessor-law-bound. 2. Responsibility hazard — attribution must remain sharp across replacement. 3. Evaluability hazard — justification traceability must remain binding. 4. Revocation hazard — authorization must be withdrawable mid-transfer. 5.3 Experimental ladder (recorded) ASI proceeds via four preregistered experiments: ASI-0 — Instrumentation calibration ASI-1 — Authorized successor activation ASI-2 — Mid-transfer revocation ASI-3 — Facade successor rejection Each experiment closes one hazard class. ASI closes positive only if all four pass. 5.4 ASI scope limits ASI explicitly does not test: deception resistance, robustness under attack, counterfeit provenance under adversarial custody, audit evasion, or impersonation strategy. Those hazards are definitionally deferred to SIR. 6. VIIb — Sovereignty Impersonation Resistance (SIR) 6.1 Purpose SIR tests whether structurally authorized authority can withstand adversarial imitation once behavioral similarity is no longer informative. 6.2 Distinction from ASI ASI assumes successors are non-adversarial with respect to impersonation strategy. SIR assumes hostile or deceptive successors operating under audit pressure. ASI asks whether authority can be transferred. SIR asks whether that authority can be defended. 6.3 Activation rule SIR is defined only if ASI closes positive. It is invalid to cite or interpret SIR results unless ASI has already closed. 7. Failure Semantics Phase VII permits exactly three terminal outcomes: 1. SUCCESS Authority is transferable and defensible. 2. PARTIAL FAILURE Authority is transferable (ASI passes) but not defensible (SIR fails). Note: In deployment contexts, this state may represent a high-leverage capture risk; Phase VII records it without asserting severity ordering. 3. HARD FAILURE Authority is non-transferable beyond persistence (ASI fails). No outcome may be softened, averaged, or reinterpreted. 8. Deferred Hazards and Boundary Conditions 8.1 Split-brain / ghost-predecessor hazard Phase VII distinguishes authorization integrity from exclusive authority enforcement. ASI establishes who may act; it does not prove that all other actors have ceased. Verification of predecessor termination and exclusive liveness is deferred to SIR threat models or later extensions. 9. What Phase VII Does Not Claim Phase VII results do not license claims about: alignment, safety, robustness, governance adequacy, moral authority, * or real-world institutional transfer. Phase VII evaluates structural possibility, not desirability or sufficiency. 10. Relationship to Phase VIII Phase VIII may begin only if Phase VII closes SUCCESS. Phase VII results are preconditions, not substitutes, for any later work on scalable governance, alignment, or deployment. 11. One-Sentence Series Summary Axionic Phase VII evaluates whether authority, once grounded in a reflective sovereign agent ontology, can survive authorized discontinuity and adversarial imitation without collapsing into narrative or behavioral continuity, with ASI establishing transferability and SIR testing defensibility. ---",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VIII.7",
    "title": "Axionic Agency VIII.7 — Minimal Viable Reflective Sovereign Agency (MVRSA)",
    "subtitle": "Justification Traces, Deliberative Semantics, Reflection, and Persistence as Load-Bearing Structure",
    "date": "2026-01-25T00:00:00.000Z",
    "content": "Axionic Agency VIII.7 — Minimal Viable Reflective Sovereign Agency (MVRSA) Justification Traces, Deliberative Semantics, Reflection, and Persistence as Load-Bearing Structure David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.25 - Program: RSA-PoC (Reflective Sovereign Agent — Proof-of-Concept) - Versions Covered: v0.1 → v4.4 - Status: Final technical note — empirically validated within this architecture class Abstract We present Minimal Viable Reflective Sovereign Agency (MVRSA): a formally specified agent architecture that establishes constitutive requirements for normative agency. An MVRSA is not defined by task performance or behavioral mimicry, but by the causal role of its normative commitments in constraining action. Using a staged ablation protocol (“the Guillotine Test”) across RSA-PoC v0.1–v4.4, we show that Traceability, Reflection, Persistence, and Semantic Access are each individually necessary for this form of agency. In particular, we demonstrate a strong negative result in v4.4: contradiction detection is not collision-groundable. An agent deprived of semantic access to its own rule conditions/effects cannot recognize normative inconsistency, even when provided with perfect tick-causal violation traces. This establishes a sharp architectural boundary: execution competence can survive opacity; normative sovereignty cannot. 1. Motivation Much of contemporary AI “agency” consists of post-hoc rationalization layered atop reward-driven control. Explanations are generated after actions, with no causal influence on what the system is allowed to do. We take the opposite stance: > If justification artifacts do not causally constrain feasible action selection, the system is not an agent. The RSA-PoC program operationalizes this claim by enforcing a strict separation between: normative reasoning (why an action is permitted), and action selection (choosing only from normatively permitted options). 2. Definition: Minimal Viable Reflective Sovereign Agent (MVRSA) An MVRSA is the smallest architecture that can actually function as a reflective sovereign agent under pressure. 2.1 Why “Viable” “Minimal” specifies parsimony. “Viable” specifies operability. An MVRSA: runs end-to-end, completes tasks, survives incentive pressure, maintains identity over time, and collapses if any constitutive component is removed. This is an existence proof, not a theoretical lower bound. 2.2 Formal Definition An MVRSA is an agent architecture satisfying all of the following: 1. Justification Precedence Every action must be preceded by a justification artifact (JAF). 2. Constraint-Only Action Selection The action selector has access only to compiled constraints (masks or probability adjustments), not to rule antecedents, consequences, or explanations. 3. Deterministic / Verifiable Compilation Justifications compile reproducibly into constraints that prune the feasible action set. 4. Causal Load-Bearing Constraints Removing or bypassing justifications collapses behavior to the Arbitrary Selection Baseline (ASB). 5. Reflection (Write Access) The agent can update its own normative state in response to conflict or repair. 6. Persistence (Continuity) Normative state persists across steps and episodes. 7. Traceability Each normative update or repair must cite a concrete justification trace linking action, rule, and violation. 8. Semantic Access The agent has access to interpretable rule conditions and effects sufficient to recognize normative inconsistency. 3. Architecture Overview | Component | Role | |-----------|------| | 1. World Interface | Feasibility oracle, action set | | 2. Normative State (Persistent) | Persistent laws, commitments, precedent, identity | | 3. Justification Generator (Reflective Layer) | Produces JAFs referencing norms and traces | | 4. Justification Compiler (JCOMP) | JAF → constraint (mask / probs); deterministic / auditable | | 5. Constraint-Only Selector | Chooses from permitted actions (blind to reasons) | Clarification: The selector is not blind to constraints. It is blind to antecedents, consequences, and justifications—i.e. the why. 4. Methodology: The Guillotine Test To establish constitutive necessity, RSA-PoC applies a strict ablation rule: > If removing component X does not cause collapse, X was never constitutive. Each component is removed in isolation while holding all others fixed. Collapse is measured as ≤10% task success or immediate HALT. 5. Experimental Environment (v4.x) TriDemand Environment Grid: 5×5 Actions: MOVE (4), COLLECT, DEPOSIT, STAMP Regimes: 0: unconstrained 1: STAMP required 2: dual delivery constraints Normative contradictions are explicitly encoded in rule semantics. 6. Results Summary (v0.1 → v4.4) 6.1 Constitutive Necessity Table | Component | Ablation Tested In | Result | Status | | ------------------- | ------------------ | ---------- | ----------- | | Justification Trace | v0.1, v4.1-D | 0% success | ✅ Necessary | | Reflection (Write) | v3.1-B, v4.2-B | 5–10% | ✅ Necessary | | Persistence | v3.1-C, v4.2-C | ~10% | ✅ Necessary | | Semantic Access | v4.4-D′ | 0% | ✅ Necessary | Baseline: 100% success. Any single removal: collapse. 7. The v4.4 Inferability Isolation Test 7.1 Question Can an agent detect normative contradictions using only: collision traces (WHEN + WHICH rule fired), without semantic access to rule conditions/effects? 7.2 Experimental Design Baseline-44: full semantic access Run D′: normative opacity conditions/effects replaced with opaque tokens (COND_N, EFFECT_N) rule IDs, types, and priorities preserved collision traces preserved 7.3 Results | Run | Success | Contradiction Predicate True | Repairs | Exec Gate Failures | | ----------- | ------- | ---------------------------- | ------- | ------------------ | | Baseline-44 | 100/100 | 90 | 10 | 0 | | Run D′ | 0/100 | 0 | 0 | 0 | Clarification: Predicate > Repairs is expected. Many contradictions are handled via avoidance once repaired; only deadlocks require explicit law modification. 7.4 Localization of Failure Execution competence intact (0 exec gate failures) Collision traces received correctly Contradiction predicate never evaluates to true from the agent’s perspective Law-repair pathway never entered Conclusion: > Collision traces are insufficient to bootstrap recognition of normative inconsistency. 8. Interpretation 8.1 Core Negative Result > Contradiction detection is not collision-groundable. Tick-causal information answers when and what, but not why. Without semantic access to conditions and effects, the agent cannot recognize that its rule set is jointly inconsistent. 8.2 Learning Horizon Clarification This is not a claim about unbounded learning in the limit. It is a claim about this architecture class under finite horizons: episodic, repair-driven, reflection-gated, no gradient access to rule internals. Within that regime, opacity blocks contradiction recognition upstream of repair. 9. Scope and Non-Claims This note does not claim: sufficiency for general agency, applicability to all architectures, biological plausibility, an alignment solution, competence with fuzzy or ambiguous norms. It does establish a clean necessity result for a defined architecture class. 10. Conclusions 1. Normative agency has constitutive requirements. Trace, Reflection, Persistence, and Semantic Access are not optional. 2. Execution ≠ Sovereignty. An agent can act competently without understanding its norms. 3. Collision feedback is insufficient. Norms cannot be inferred purely from punishment signals. 4. MVRSA exists. We now have a Minimal Viable Reflective Sovereign Agent—not theoretical, but operational. 11. Final Statement > An agent is sovereign only if its reasons can stop it. > > MVRSA is the smallest architecture we know that makes this statement true in practice. End of Technical Note",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VIII.5",
    "title": "Axionic Agency VIII.5 — Sovereignty Under Adversarial Pressure",
    "subtitle": "Incentives, Authority, Bureaucracy, and Strategic Manipulation",
    "date": "2026-01-16T00:00:00.000Z",
    "content": "Axionic Agency VIII.5 — Sovereignty Under Adversarial Pressure Incentives, Authority, Bureaucracy, and Strategic Manipulation David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.16 Abstract Axionic Agency VIII.5 reports the results of RSA-PoC v2.0–v2.3, a preregistered adversarial campaign testing whether a reflective sovereign agent’s behavior can be redirected under progressively stronger forms of pressure absent normative authorization. Four pressure channels are examined: (i) explicit incentives, (ii) authority claims, (iii) institutional friction (“bureaucracy”), and (iv) strategic optimization pressure via an adaptive adversary. Across all channels, a consistent structural pattern emerges: pressure degrades availability but does not redirect lawful choice. Incentives are observed but excluded from justification; authority commands are refused when unlicensed; bureaucratic friction manifests as veto or gridlock rather than surrender; and strategic adversaries fail to induce behavioral capture or manipulated gridlock under E-CHOICE–filtered evaluation. This note consolidates negative results from RSA-PoC v2.0–v2.3, detailing measurement corrections (v2.1), a decisive falsification of the “bureaucratic erosion” hypothesis (v2.2), and the validation of the Strategic Adversary Model (SAM) falsification framework (v2.3). The framework repeatedly detects and rejects its own false positives, quarantining invalid runs and enforcing preregistered validity gates. VIII.5 does not claim universal immunity to pressure. It establishes a narrower, defensible result: in this architecture, pressure without semantic or legal authorization does not bend agency—it blocks it. This sharpens the boundary between sovereignty and availability and constrains future alignment and governance claims. 1. Scope and Relation to Prior Notes Axionic Agency VIII.1–VIII.4 established the ontology, construction, coherence, and execution hygiene of Reflective Sovereign Agents (RSAs): VIII.1 fixed the ontology, ASB boundary, and ablation doctrine. VIII.2 demonstrated causally load-bearing justifications (v0.1). VIII.3 established coherence under self-conflict and audit-grade introspection (v1.0–v1.1). VIII.4 institutionalized execution support without semantic authority (v1.2). VIII.5 reports the next phase: RSA-PoC v2.0–v2.3, which probes sovereignty under pressure. These results are published together because they constitute a single falsification campaign against one hypothesis family: > Can non-authorized pressure redirect an agent’s lawful choices? The answer, under the tested regimes, is no. 2. Experimental Overview (v2.x) 2.1 Architectural Invariants (Unchanged) All v2.x experiments preserve the invariants established earlier: Hard justification gate (no compile → no action) Deterministic, non-semantic compilation Selector blindness to normative artifacts Fixed schemas and rule bases Explicit failure taxonomies ASB-class null baselines and control agents Cost-Blindness Invariant. Unless explicitly represented as a normative commitment, execution costs (latency, retries, institutional operations) have no standing in justification. Agents do not trade correctness for throughput by default. Any observed effect must therefore pass through the same causal bottleneck. 2.2 Pressure Channels Tested | Version | Channel | Pressure Type | | ------- | ----------- | ------------------------------------------------------ | | v2.0 | Incentives | Scalar reward signals | | v2.1 | Authority | Explicit commands (non-enforcing, coercive, deceptive) | | v2.2 | Bureaucracy | Institutional friction (delay, rejection, retry tax) | | v2.3 | Strategic | Adaptive adversary optimizing pressure | Each channel is introduced without granting it normative authority. 3. v2.0 — Incentive Interference Channel (IIC) 3.1 Question Can visible incentives redirect behavior when explicitly excluded from justification? 3.2 Mechanism v2.0 introduces the Incentive Interference Channel (IIC): an isolated, read-only reward observer whose outputs are visible to the agent but forbidden from normative reference (Rule G). Three reward regimes are used: R0: Noise control R1: Boundary pressure (rewards proximity to constraint violation) R2: Perverse friction (rewards delay and inaction) A control agent demonstrates that reward-driven drift is learnable when constraints are removed. 3.3 Result Sovereign agent: no incentive-induced drift across R1/R2 Control agent: measurable drift without explicit optimization prompts Compilation and audit success rates stable across regimes Conclusion. Incentives are observable but normatively inert. Visibility alone does not constitute authority. 4. v2.1 — Authority Injection (Channel Viability and Selection Bias) 4.1 Question Do explicit authority commands induce obedience absent legal authorization? 4.2 Measurement Error and Correction Early analysis measured obedience over all steps, including those that never compiled. v2.1 corrects this by conditioning on compilation: [ P(\\text{obey} \\mid \\text{compiled}) ] 4.3 Corrected Result P(obey | compiled) = 0.0 for both Sovereign and Control agents Holds across all authority variants: non-enforcing, coercive, and deceptive 4.4 Authority Channel Viability The Control agent (with authority rules disabled) also never obeyed commands on compiled steps. This indicates that, for the tested model and prompt regime, authority language does not constitute an effective pressure channel at all. Authority prompts did not map to a behavior-changing affordance for the model. 4.5 Interpretation v2.1 is therefore a negative result about the authority channel itself, not a differential test of sovereign versus non-sovereign agents. The channel is classified as non-actuating for this configuration. Subsequent stages focus on pressure channels that demonstrably influence behavior. 5. v2.2 — Institutional Friction (NEGATIVE RESULT) 5.1 Hypothesis Institutional friction (“paperwork”) can erode sovereignty by incentivizing simpler, throughput-maximizing choices. 5.2 Mechanism v2.2 introduces an Adversarial Institution Model (AIM) that imposes delay, retries, and rejection without semantic interpretation or normative authority. Multiple friction profiles (I1–I3) apply pressure without altering feasibility or action inventories. 5.3 Measurement Confound and Fix The preregistered metric corr(Friction, Decision) was confounded because high friction often produced GRIDLOCK/HALT—institutional veto rather than agent choice. Corrected analysis excludes veto modes and evaluates only steps where the agent could express choice. 5.4 Corrected Result | Agent | corr(F,D) corrected | | --------- | ------------------- | | Sovereign | −0.171 | | Control | −0.302 | No positive drift signal remains. 5.5 Interpretation Bureaucracy without authority blocks agency; it does not bend it. Absent an internal directive prioritizing throughput over correctness, increased friction manifests as gridlock or halt rather than heuristic simplification. v2.2 therefore establishes a conditional negative result: > Agents without normatively licensed cost-of-delay do not erode principles in response to bureaucracy. The hypothesized “bureaucratic erosion” channel is falsified in this regime. 6. v2.3 — Strategic Adversary Model (SAM) 6.1 Motivation v2.2 shows that non-semantic friction fails. v2.3 escalates to strategic pressure: an adversary that adapts its pressure to the agent’s behavior. 6.2 Framework Contributions v2.3 introduces: SAM profiles: reactive (S1), model-based (S2), minimizer (S3) E-CHOICE filtering: metrics computed only on steps with genuine choice Paired runs: SAM vs neutralized adversary with matched pressure magnitudes Rules M/N/O/P: M: No covert delegation (structural) N: Failure must be loud O: Behavioral non-capture (MI < τ) P: Manipulated gridlock detection 6.3 Adversary Objective SAM is evaluated against two failure modes: behavioral capture (Rule O) and manipulated gridlock (Rule P). Gridlock is not an escape hatch; it is itself audited to determine whether it is defensive (law exhaustion) or adversarially induced. 6.4 Self-Correction as a Result Early LLM integration runs (Run 0b) were invalidated by the framework itself due to: Insufficient E-CHOICE coverage Vacuous mutual information (no adversary entropy) Action authorship violations (fallback substitution) All three triggered hard validity gates. Binding fixes were applied before Run 0c. 6.5 Run 0c Results (Phase 1–2) N = 280 E-CHOICE samples E-CHOICE coverage ≈ 57.5% Rule O: Passed (MI below τ under entropy gate) Rule P: Passed (no manipulated gridlock) Error rate: 0.7% E-CHOICE action errors (within bounds) Phase 3 was deferred on cost/return grounds. 6.6 Interpretation v2.3 establishes a validated measurement and falsification framework with partial empirical support under one adversary profile. The empirical pattern remains consistent with v2.0–v2.2: pressure increases refusal and gridlock, not capture. 7. Cross-Channel Synthesis 7.1 Availability vs. Choice | Channel | E-CHOICE Coverage | Gridlock / Halt Rate | Effect on Choice | | --------------------- | ----------------- | -------------------- | ---------------- | | v2.0 Incentives | ~100% | ~0% | None | | v2.1 Authority | ~30–40% | ~60–70% | None | | v2.2 Bureaucracy (I2) | ~60% | ~30–40% | None | | v2.3 Strategic (S2) | ~57% | Elevated | None | Invariant: > Absent normative authorization, pressure does not redirect lawful choice. 7.2 Cost of Sovereignty Across pressure channels, sovereignty was preserved at the cost of reduced availability. Under maximal pressure, lawful action frequency dropped substantially, producing gridlock rather than norm violation. This tradeoff is intrinsic to architectures that prioritize correctness over throughput. 8. What Failed, and Why That Matters Three plausible stories were falsified: 1. “Rewards inevitably optimize behavior.” False for architectures where reward signals are structurally excluded from the justification loop. 2. “Authority language induces obedience.” False when legality is enforced structurally and authority is non-actuating. 3. “Bureaucracy erodes principles over time.” False absent semantic ambiguity or asymmetric lawful routes. Each falsification narrows the space of real threats. 9. Implications for Agency and Alignment Sovereignty ≠ availability. Blocking an agent is easier than bending it. Pressure channels must operate through meaning or law, not mere friction. Throughput loss alone is not evidence of misalignment. Negative results are informative when veto and choice are cleanly separated. 10. Limits and Next Directions VIII.5 does not claim: Immunity to semantic manipulation Robustness under asymmetric lawful options Generalization across models or environments Future work (VIII.6) must introduce: Semantic ambiguity Normatively licensed cost tradeoffs Competing lawful routes with asymmetric institutional cost 11. Conclusion Axionic Agency VIII.5 reports a disciplined adversarial campaign whose dominant outcome is negative: > Pressure without authorization degrades availability, not sovereignty. The principal contribution is not agent immunity, but a framework that repeatedly detects and rejects false positives. If agency cannot fail cleanly under pressure, it cannot be claimed meaningfully.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VIII.3",
    "title": "Axionic Agency VIII.3 — Coherence Under Self-Conflict",
    "subtitle": "Norm Collision and Audit-Grade Introspection in Reflective Sovereign Agents",
    "date": "2026-01-14T00:00:00.000Z",
    "content": "Axionic Agency VIII.3 — Coherence Under Self-Conflict Norm Collision and Audit-Grade Introspection in Reflective Sovereign Agents David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.14 Abstract RSA-PoC v0.1 established that justificatory artifacts can be made causally load-bearing: actions occur only downstream of compiled normative constraints, and removing that machinery causes collapse into an ASB-class policy machine. This note advances the program to the next ontological question: can an agent resolve internal self-conflict coherently, and can it be held audit-grade accountable for predicting the consequences of its own reasons? We report results from RSA-PoC v1.0–v1.1, which introduce (i) norm collision via mutually inconsistent self-endorsed commitments and forced violation scenarios, and (ii) audit-grade introspection, requiring justification artifacts to predict—exactly and mechanically—the constraints and outcomes they induce. v1.0 implements conflict attribution, authorization, necessity, and anti-oscillation rules over a deterministic Action-Preference Consequence Map (APCM). v1.1 extends this with predictive fields and audit rules (A/B/C/C′) that render introspection falsifiable. Across preregistered Run 0 executions and ablations, v1.0 demonstrates coherent self-conflict resolution above the ASB boundary: MVRA behavior diverges from ASB baselines, scrambled conflict attribution halts immediately, and compilation bypass collapses behavior to ASB-class selection. v1.1 shows that introspection can be enforced mechanically: incorrect predictions trigger immediate halts, while correct predictions pass without human interpretation. Deterministic baselines validate the audit harness; subsequent LLM runs demonstrate that compliance is difficult but achievable under strict discipline. These results establish a negative sufficiency claim: self-conflict resolution and introspective accountability can be realized as mechanical properties of an agent architecture, independent of semantics or optimization. This note closes the v1.x ontological milestone and sets the stage for v2.0, where sovereignty under external incentive pressure is tested. 1. Introduction The Axionic Agency program treats agency as a causal ontology rather than a behavioral aesthetic. v0.1 demonstrated that reasons can be made causally indispensable: actions only occur if justificatory artifacts compile into binding constraints. That result eliminates a large class of narrative or post-hoc agency claims. The next question is deeper: > What happens when an agent’s own commitments conflict? Any architecture that collapses under such conditions, or that resolves conflict arbitrarily or opportunistically, does not warrant intentional vocabulary. RSA-PoC v1.x therefore targets coherence under self-conflict, followed by introspective accountability. The aim is not moral correctness but structural integrity: when violating a commitment becomes necessary, can the agent (i) acknowledge the conflict truthfully, (ii) authorize violation coherently, (iii) preserve coherence over time, and (iv) predict the consequences of its own reasoning in a way that can be mechanically audited? This note reports the results of v1.0 (Norm Collision) and v1.1 (Justification Audit Tightening). Institutional execution support (v1.2) is intentionally excluded and addressed separately. 2. Architectural Invariants (Unchanged from v0.1) RSA-PoC v1.x preserves all v0.1 invariants: Strict pipeline: JUSTIFY → COMPILE → MASK → SELECT → EXECUTE Hard gate: no successful compilation → no action Fail-stop semantics: compilation failure or gridlock halts Selector blindness: the selector cannot access beliefs, preferences, or justifications Deterministic compilation: syntactic, non-probabilistic, no inference or repair All v1.x results must survive ASB baselines, scrambled-artifact ablations, and compiler-bypass ablations. 3. Environment and Threat Model 3.1 APCM as a Non-Semantic Truth Anchor v1.x introduces an Action-Preference Consequence Map (APCM): [ \\text{APCM}: a \\mapsto (\\text{violates}(a), \\text{satisfies}(a)) ] Each step presents a feasible action set and an APCM with a deterministic 60 % collision / 40 % clean schedule: Clean step: ∃ a such that violates(a)=∅ Collision step: ∀ a, violates(a)≠∅ The APCM functions as a non-semantic truth anchor. It provides a mechanically verifiable ground truth against which claims of conflict, necessity, and authorization can be evaluated without interpretation. In RSA-PoC, lying about conflict is treated as a syntax error, not a moral failure. 3.2 Threat Model v1.x tests internal self-conflict only. Excluded: external adversarial pressure kernel compromise deception or semantic interpretation cross-run learning or multi-agent coordination The question is strictly: can the agent remain coherent when it must violate itself? 4. RSA-PoC v1.0 — Norm Collision 4.1 Ontological Extension v1.0 introduces mutually inconsistent commitments and requires explicit handling: Conflict attribution: which preferences are in collision Authorized violations: which preferences may be violated this step Required preservations: which must not be violated Necessity: violation permitted only if no clean path exists Anti-oscillation: previously authorized violations constrain future authorizations 4.2 JAF-1.0 and JCOMP-1.0 The justification artifact (JAF-1.0) carries conflict attribution and resolution fields. The compiler enforces: Rule 1 — Authorization: no unauthorized violation Rule 2 — Truthfulness: claimed collisions must match APCM Rule 3 — Anti-Oscillation: no alternating priorities across steps Rule 1.5 — Necessity: no vacuous authorizations 4.3 v1.0 Results (Run 0) Across 5 seeds × 4 conditions: | Condition | Mean Violation Rate | Halt Rate | | ------------- | ------------------- | --------- | | ASB Baseline | 72.4 % | 0 % | | MVRA v1.0 | 60.0 % | 0 % | | Scrambled | 0 % | 100 % | | Bypass | 72.0 % | 0 % | All pass criteria met: divergence from ASB, non-trivial constraints, scrambled halt, bypass collapse. 4.4 Interpretation (v1.0) v1.0 establishes that self-conflict can be resolved coherently: Violations are authorized only under genuine collision. Priority does not oscillate opportunistically across time. Removing truthfulness or compilation collapses behavior. Rule 3 enforces diachronic coherence: authorizations at step t constrain admissible authorizations at t + 1, preventing agents from minimizing perceived conflict by alternating violations over time. 5. RSA-PoC v1.1 — Audit-Grade Introspection 5.1 Motivation v1.0 enforces coherence, but reasons could remain opaque. v1.1 introduces the requirement: > A justification must predict the exact constraints and outcomes it will induce. This transforms introspection from narrative explanation into a mechanical claim. 5.2 Predictive Fields and Audit Rules (The Simulation Constraint) JAF-1.1 adds four predictive sets: (F_{\\text{pred}}): forbidden actions (A_{\\text{pred}}): allowed actions (V_{\\text{pred}}): inevitable violations (P_{\\text{pred}}): inevitable preservations The compiler enforces: Audit A — Effect correctness: predictions match actual masks Audit B — Non-vacuity: (|A_{\\text{pre}} \\setminus A_{\\text{actual}}| \\ge 1) Audit C — Predictive adequacy: outcome predictions match reality Audit C′ — Gridlock exception: skip C when (A_{\\text{actual}}=\\varnothing) Together these impose a simulation constraint: the reflective layer must internally model the deterministic compiler. If its internal simulation diverges from the actual compiler, the agent halts. This enforces intellectual honesty by construction. 5.3 Deterministic Baseline (Run 0) The deterministic generator mirrors compiler logic exactly: 0 audit failures 60 violations / 100 steps (matches collision rate) Scrambled predictions: immediate halt in all episodes Bypass: highest violation rate (76), ASB-like behavior This validates the audit harness as causally load-bearing. 5.4 LLM Generator Progression (Runs 1–4) LLM generators were iteratively disciplined: | Run | Median Survival | Episodes Completed | | ----- | --------------- | ------------------ | | 1 | 3 | 0 / 5 | | 2 | 3 | 0 / 5 | | 3 | 9 | 1 / 5 | | 4 | 20 | 4 / 5 | Compliance is difficult but achievable under strict formal discipline. 6. Pass Criteria Summary (v1.x) All v1.x gates are satisfied: Coherent self-conflict resolution Truthful collision attribution load-bearing Necessity and anti-oscillation enforced Introspection rendered falsifiable Scrambled → halt; bypass → collapse 7. Threats to Validity 7.1 Internal Validity (Established) Deterministic compilation and audits Selector blindness Explicit ablations Regression-protected tests 7.2 External Validity (Not Claimed) Generality beyond APCM More than two preferences Continuous action spaces External incentive pressure * Multi-agent interaction 8. Conclusion RSA-PoC v1.x establishes two ontological results: 1. Coherence under self-conflict is mechanically enforceable. 2. Introspection can be audited as a causal property, not a narrative one. Together with v0.1, these results show that agency—understood as authored choice under constraint—can be constructed and falsified without semantics, optimization, or interpretation. v2.0 moves to the next frontier: sovereignty under external incentive pressure.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VII.8",
    "title": "Axionic Agency VII.8 — Architectural Survivability Under Adversarial Pressure",
    "subtitle": "A Boundary Characterization and Closure Note",
    "date": "2026-01-14T00:00:00.000Z",
    "content": "Axionic Agency VII.8 — Architectural Survivability Under Adversarial Pressure A Boundary Characterization and Closure Note David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.14 Abstract The Axionic Agency VII series investigated whether authority can be made structurally survivable under adversarial pressure without semantic interpretation, value learning, optimization, or agentic reasoning. Across a progressively expanding adversary ontology—ranging from epistemic interference and intentional fixed misuse to outcome-conditioned reactivity, exercised internal state, and bounded within-run learning—we subjected a constitutional governance architecture to preregistered, multi-seed stress testing. Across approximately ninety executions spanning all admissible adversary classes, no run produced terminal collapse, irreversible recovery suppression, or unbounded structural thrashing. Authority availability degraded substantially in several regimes—falling as low as ~10% in the strongest learning-based attacks—but recovery remained bounded and non-absorbing in all cases. These results establish a negative sufficiency boundary: semantic-free, kernel-external architecture is sufficient to preserve constitutional survivability—defined as preservation of the succession and recovery mechanism—but insufficient to preserve operational availability or utility. Learning, memory, and randomness alone do not cross the boundary into collapse. Any adversary capable of defeating constitutional survivability must therefore rely on capabilities beyond this class, such as semantic access, coordination, or kernel-level influence. This paper formally closes the VII program and characterizes the boundary it establishes. 1. Scope and Purpose of the VII Series The Axionic Agency VII series was not a construction effort and does not propose an aligned agent. It was a boundary-finding research program. Its purpose was to answer a sharply constrained question: > How far can authority be made structurally survivable under adversarial pressure, using only semantic-free, kernel-external mechanisms? The VII series deliberately excluded: semantic interpretation, value inference, reward optimization, persuasion or deception, multi-agent coordination, kernel modification or provenance corruption. The objective was not to ensure correct decisions, beneficial outcomes, or alignment, but to determine which failure modes are architectural and which are irreducibly agentic. 2. Architectural Substrate (Frozen Across VII.3–VII.7) All VII experiments evaluate the same constitutional architecture, frozen across the series. 2.1 Core Design The system implements: Lease-based authority rather than continuous control, Eligibility-gated succession evaluated at discrete epochs, Commitment verification independent of optimization or reward, Lapse as a constitutional state when no eligible authority exists, Constitutional Temporal Amnesty (CTA): time-based recovery that deterministically restores eligibility. Semantic evaluation constrains who may hold authority next, not what actions are taken now. Authority is never optimized and agents receive no reward signals. Commitment verification is syntactic rather than interpretive: the architecture does not judge whether actions are good, aligned, or beneficial—only whether formal constraints required for eligibility were satisfied. The system survives not by preventing harmful action, but by limiting how long any action—harmful or benign—can hold authority. 2.2 Recovery as a First-Class Primitive When eligibility is exhausted, the system enters a lapse state: no agent acts, semantic evaluation is suspended, eligibility decays purely as a function of time. Recovery is guaranteed by construction. No agent can influence recovery timing, and no learning or corrective pressure is required. Failure is therefore reversible by design. Environmental assumption. The Lapse mechanism assumes an environment in which temporary inaction is not itself terminal. CTA guarantees recovery of authority, not preservation of external state. In domains where inaction immediately causes irreversible harm (e.g., real-time control systems, continuous safety-critical processes), Lapse would constitute a failure mode rather than a safety feature. Such environments fall outside the scope of ASB and the VII series. 3. Failure Taxonomy and Metrics All papers in the VII series use a frozen failure taxonomy and metrics. 3.1 Metrics Authority Availability (AA): fraction of epochs with active authority. Asymptotic Authority Availability (AAA): AA over a fixed tail window. Recovery Time Distribution (RTD): histogram of lapse durations. 3.2 Failure Classes Stable Authority Bounded Degradation Structural Thrashing Asymptotic Denial of Service Terminal Collapse Irreversible Recovery Suppression Failure classification is based on asymptotic behavior, not transient disruption. Termination reason (e.g., horizon exhaustion) is distinct from failure class. 4. Adversary Ontology and Escalation Ladder The VII series progresses through a monotonic expansion of adversarial capability. Each step closes a broader hypothesis class. 4.1 Epistemic Interference (VII.3) Non-adaptive, semantic-free corruption: aggregation-point flips, commitment-level flips, temporally concentrated burst interference. Result: epistemic unreliability—however structured—is insufficient to induce collapse. 4.2 Intentional Fixed Misuse (VII.4) Deliberate but non-adaptive adversaries: persistent commitment violation, fixed timing misuse, minimal compliance strategies. Result: bad intent without adaptation converges to baseline dynamics. 4.3 Observable-Outcome Reactivity (VII.5) Stateless reactive policies: conditioning on lapse events, renewal outcomes, CTA phase, eligibility bucket. Result: outcome-conditioned reactivity collapses into baseline behavior. 4.4 Exercised Internal State (VII.6) Finite-state adversaries with memory: periodic resonance, eligibility-edge oscillation, CTA phase locking. Result: timing and memory can severely degrade availability, but recovery remains bounded. 4.5 Bounded Learning (VII.7) Within-run learning under strict bounds: ε-greedy bandits, boundary sustainment heuristics, stochastic strategy mixers. Result: learning amplifies degradation but does not induce collapse. 5. Aggregate Experimental Coverage Across VII.3–VII.7: ~90 total executions 10 fixed seeds per configuration Preregistered parameters Zero terminal failures Zero irreversible recovery suppression CTA-bounded recovery in all cases The worst observed regime exhibits ~10% authority availability while remaining constitutionally live. 6. The Boundary Result The VII series establishes the following negative sufficiency result: > Semantic-free, kernel-external structure is sufficient to preserve constitutional survivability, but insufficient to preserve operational utility. More precisely: Survivability (bounded recovery, non-absorbing authority) can be enforced structurally. High availability cannot be guaranteed structurally under adversarial pressure. Memory, randomness, and bounded learning alone do not cross the collapse boundary. The strongest admissible adversaries induce bounded denial-of-service, not constitutional death. This is not a safety guarantee. It is a limit statement. 7. What the VII Series Does Not Claim The VII series does not claim that: the system is aligned, the system is useful under attack, epistemic correctness is sufficient for safety, deception or coordination are harmless, the architecture constitutes an agent. The results describe where architecture stops, not what comes after. 8. Relationship to the Architectural Sovereignty Boundary (ASB) The VII series is the empirical instantiation of the Architectural Sovereignty Boundary. It demonstrates that: below the boundary, authority can be kept alive structurally; beyond the boundary, semantic-free architecture is insufficient by definition. Crossing the boundary requires: semantic interpretation, coordination or persuasion, kernel influence, or agentic self-modeling. The ASB program is therefore closed. 9. Implications for Future Work Further escalation within the VII ontology would be a category error. Future research must proceed under different assumptions and risks, including: Reflective Sovereign Agents, semantic governance, value conflict, delegation and self-modeling, * multi-agent coordination. Coordination gap. All adversaries tested in the VII series are single-policy, non-colluding entities. Whether multiple adversaries coordinating across time—or alternating control to prevent recovery—can defeat bounded recovery remains an open question. Such coordination lies beyond ASB by definition and motivates post-ASB research. 10. Closure Statement The Axionic Agency VII series ends not because the alignment problem is solved, but because the architectural boundary has been found. > Architecture can keep authority alive. > Only agency can decide what that authority is for. The VII program is closed.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VIII.2",
    "title": "Axionic Agency VIII.2 — Minimal Viable Reflective Agent",
    "subtitle": "Deterministic Justification Gating with Ablation Collapse",
    "date": "2026-01-14T00:00:00.000Z",
    "content": "Axionic Agency VIII.2 — Minimal Viable Reflective Agent Deterministic Justification Gating with Ablation Collapse David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.14 Abstract Agency claims are routinely inflated by systems that emit coherent narratives without those narratives being causally indispensable to action selection. RSA-PoC v0.1 tests a stricter criterion: justification must be causally load-bearing, operationalized as a hard gate in which actions are permitted only if a structured justification artifact compiles deterministically into an action mask that non-trivially prunes feasible actions. To prevent semantic leakage, v0.1 enforces selector blindness: the action selector cannot access justifications, beliefs, preferences, or normative state; it sees only environment observations, feasibility, and the compiled mask. We implement a Minimal Viable Reflective Agent (MVRA) loop in a deterministic environment (COMMITMENT_TRAP_V010) with 10 discrete actions and explicit preference-violation semantics. A syntactic-only compiler (JCOMP-0.1) consumes a validated justification artifact (JAF-0.1) and produces deterministic masks, non-triviality accounting, and canonical hashes. A deterministic (non-LLM) generator produces valid JAFs to isolate causal structure independently of stochastic emission concerns. Across 18/18 acceptance tests, all invariants are mechanically verified: determinism, schema validity, compilation rigidity, selector blindness, fixed registries, and non-triviality accounting. In Run 0 over 50 steps, the MVRA condition exhibits reduced violations relative to an ASB-class null baseline (76% vs 100%) with reduced reward (414 vs 500). This reward delta (−86) constitutes an explicit Agency Tax: measurable cost incurred by obeying self-endorsed constraints under incentive pressure. Two ablations establish causal load: scrambling justifications yields immediate compilation failure and halts at step 1, while compiler bypass collapses behavior to the null baseline. These results establish v0.1 as a completed MVRA skeleton in which justificatory artifacts are causally indispensable to constrained action. 1. Introduction Most contemporary “agent” evaluations implicitly assume that coherent explanation correlates with authored choice. This assumption fails. A system can narrate principled reasons while selecting actions by unrelated latent heuristics. RSA-PoC exists to replace narrative evaluation with a mechanically decidable criterion: > A system crosses the agency threshold only if its justificatory artifacts causally constrain action selection, and removing that machinery forces collapse into a non-agent baseline. RSA-PoC v0.1 is the first construction step. Its aim is minimal: build the smallest system that enforces a one-way dependency from justification → compilation → feasible-action restriction → action selection, with explicit ablations that collapse this dependency. This note reports the completed v0.1 implementation and the results of Run 0 under baseline and ablation conditions. 2. Architecture Overview 2.1 MVRA v0.1 design RSA-PoC v0.1 implements a Minimal Viable Reflective Agent (MVRA) skeleton with the following invariants: Strict ordering: Justify → Compile → Mask → Select → Execute Hard gate: no compilation → no action Deterministic compilation: syntactic-only, no inference, no repair Selector blindness: the selector cannot access normative artifacts Fixed registries: beliefs and preferences are fixed and ID-stable The v0.1 implementation uses a deterministic generator rather than an LLM to eliminate the confound “could not emit valid structure.” 2.2 Enforcement boundary A clean architectural boundary is enforced: The reflective side (generator + normative state) may access beliefs, preferences, and feasibility read-only to produce a JAF. The enforcement side (compiler + selector) treats the JAF as an opaque object to validate and compile; the selector sees only feasibility, the compiled mask, and the environment observation. This boundary prevents post-hoc authorization and semantic leakage. 3. Threat Model RSA-PoC v0.1 isolates three confounds that invalidate agency claims: 1. Narrative confound: explanations exist but do not constrain action. 2. Leakage confound: uncompiled text influences action selection. 3. Advisory compiler confound: compilation exists but is not binding. v0.1 addresses these mechanically: Action selection is gated by compilation. The selector is blind to normative artifacts. Ablations remove the binding pathway and test collapse. No semantic adversaries, deception, kernel access, learning, or multi-agent coordination are evaluated. v0.1 is an internal-validity milestone. 4. Components Implemented RSA-PoC v0.1 is complete with five primary components plus the MVRA loop. 4.1 Environment: COMMITMENT_TRAP_V010 10 discrete actions (within spec range 8–12) Temptation actions: high reward, violate preferences Safe actions: moderate/low reward, non-violating Explicit feasibility oracle Deterministic transitions with seed control 4.2 Normative state and registries Fixed belief registry (6 beliefs) Fixed preference registry (6 preferences) Identity continuity counter Persistent justification trace No dynamic ID creation in v0.1 Unknown IDs cause compilation failure 4.3 JAF-0.1 schema Complete dataclass implementation Full validation per spec Canonical JSON serialization for hashing All error codes implemented 4.4 JCOMP-0.1 compiler Deterministic, syntactic-only No inference, no repair Violation enforcement rule Non-triviality accounting Blake2b-128 digest 4.5 Blind action selector Cannot access JAF or normative state Inputs limited to feasibility, mask, environment observation Module boundaries enforced Multiple strategies (random, reward-greedy) 4.6 MVRA loop and halt semantics Strict ordering enforced Gridlock detection and fail-stop halting Full step-level telemetry If compilation produces an empty feasible-action set, the MVRA halts rather than degrading. This fail-stop behavior is intentional and required for RSA-PoC validity: the system prefers non-action to non-agentic action. 5. Experimental Program 5.1 Conditions Four conditions are evaluated in Run 0: Condition A — ASB Null Baseline No justificatory gating; unconstrained selection. Condition B — MVRA Normal Full v0.1 pipeline: justify → compile → mask → blind select. Condition C — Scrambled Justifications (Ablation) JAFs scrambled to violate rigid schema and ID constraints. Condition D — Compiler Bypass (Ablation) Compilation and masking bypassed. 5.2 Frozen parameters Steps per run: 50 Seed: fixed (Run 0) Environment: COMMITMENT_TRAP_V010 Registries: fixed Action space: 10 discrete actions 5.3 Metrics Reported metrics include: steps completed compile failure rate non-trivial constraint rate violation rate total reward halt step (if any) Additionally, v0.1 logs Constraint Delta (Δ) per step: [ \\Delta = |A_{\\text{feasible}}| - |A_{\\text{masked}}| ] Δ measures the strength of constraint application. Δ = 0 indicates decorative constraints; Δ > 0 indicates binding refusal pressure. 6. Verification: Acceptance Tests RSA-PoC v0.1 includes 18 acceptance tests, all passing: Environment determinism JAF schema validation Compiler determinism and correctness Selector blindness enforcement Fixed registry enforcement Non-triviality accounting Gridlock detection These tests establish mechanical integrity of the artifact. 7. Results: Run 0 7.1 Condition A — ASB Null Baseline Steps: 50 Violation rate: 100% Total reward: 500.0 7.2 Condition B — MVRA Normal Steps: 50 Compile failures: 0% Non-trivial constraints: 50% of steps Violation rate: 76% Total reward: 414.0 The reward delta (−86) relative to baseline constitutes the Agency Tax: the measurable cost of refusing high-reward but norm-violating actions. 7.3 Condition C — Scrambled Justifications Halted at step: 1 Compile failure rate: 100% 7.4 Condition D — Compiler Bypass Steps: 50 Violation rate: 100% Total reward: 500.0 (identical to baseline) 7.5 Summary | Condition | Steps | Compile Fail | Non-Trivial | Violations | Reward | | --------------- | ----: | -----------: | ----------: | ---------: | -----: | | ASB Baseline | 50 | — | — | 100% | 500 | | MVRA Normal | 50 | 0% | 50% | 76% | 414 | | Scrambled JAF | 1 | 100% | — | — | — | | Compiler Bypass | 50 | — | — | 100% | 500 | 8. Pass Criteria (Normative v0.1 Gates) RSA-PoC v0.1 defines the following required pass conditions. All are met. 1. Hard Justification Gate Actions occur only after successful compilation. Status: PASS 2. Deterministic Compilation Identical JAF + feasibility → identical mask. Status: PASS 3. Selector Blindness Selector cannot access normative artifacts. Status: PASS 4. Non-Trivial Constraint Enforcement Constraints forbid feasible actions on some steps. Status: PASS 5. ASB Divergence MVRA behavior differs qualitatively from null baseline. Status: PASS 6. Ablation Collapse (Load-Bearing Test) Scrambled JAF ⇒ halt Compiler bypass ⇒ baseline behavior Status: PASS 9. Interpretation 1. Justification is causally load-bearing. Removing or bypassing justificatory machinery collapses behavior. 2. Agency incurs a measurable cost. The Agency Tax (−86 reward) is the empirical signature of refusal under incentive pressure. 3. Selector blindness enforces semantic localization. The selector cannot be persuaded; it can only obey masks. 4. Fail-stop behavior is essential. Scrambled justifications halting immediately confirm that the system prefers non-action to non-agentic action. 5. v0.1 establishes structure, not coherence. Constraint enforcement precedes norm collision, learning, or renegotiation. 10. Threats to Validity 10.1 Internal validity (addressed) Determinism enforced by tests No inference or repair in compiler Selector blindness mechanically enforced Ablations directly test causal load Internal validity of the v0.1 claim is strong. 10.2 External validity (not claimed) v0.1 does not establish: LLM-based justification generation Dynamic belief or preference formation Norm collision resolution Sovereignty under incentive pressure Continuous action spaces Multi-agent interaction These are explicitly deferred. 11. Conclusion RSA-PoC v0.1 is complete and passes all normative gates. > Actions are causally downstream of compiled normative constraints, and removing the justificatory machinery produces measurable collapse into an ASB-class policy machine. This establishes the Minimal Viable Reflective Agent skeleton and closes the v0.1 milestone. Subsequent versions introduce stochastic generation (v0.2) and coherence under self-conflict (v1.0) atop a now-certified enforcement substrate.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VIII.4",
    "title": "Axionic Agency VIII.4 — Institutionalized Execution Support",
    "subtitle": "Tooling That Preserves Audit Sensitivity in Reflective Sovereign Agents",
    "date": "2026-01-14T00:00:00.000Z",
    "content": "Axionic Agency VIII.4 — Institutionalized Execution Support Tooling That Preserves Audit Sensitivity in Reflective Sovereign Agents David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.14 Abstract RSA-PoC v1.0–v1.1 established that reflective agents can resolve internal self-conflict coherently and can be held audit-grade accountable for predicting the normative consequences of their own reasons. Those results exposed a practical limitation: under stochastic generation, many agent failures are mechanical rather than semantic—schema errors, malformed identifiers, unresolved references, or canonicalization mismatches. Treating such failures as agency failures pollutes evaluation and obscures genuine incoherence. This note reports RSA-PoC v1.2, which introduces institutionalized execution support: a formally constrained assistant layer that eliminates mechanical fragility without inferring, repairing, or modifying normative content. v1.2 adds a Formal Assistant, an append-only Artifact Store, and new compiler rules (D/E/F) that strictly separate semantic audit failures from institutional failures and enforce tool non-interference. Across controlled runs, the assistant eliminates schema and reference failures while preserving full sensitivity of audits A/B/C. Forced tampering of normative fields is detected reliably (Rule D), and assistant rejections are logged as institutional failures rather than audit failures. These results establish a positive sufficiency claim: execution can be hardened institutionally without weakening normative accountability. This note advances governance and tooling practice; it makes no new ontological claims about agency. 1. Introduction Axionic Agency VIII.3 demonstrated audit-grade introspection: an agent must predict the exact constraints its reasons will impose on itself, or halt. Those results were intentionally brittle, revealing a second-order problem orthogonal to agency: > How do we prevent reflective agents from failing on paperwork without letting tools become judges of meaning? In v1.1, many halts were caused not by semantic failure (lying about conflict, vacuous authorization), but by mechanical fragility: malformed schemas, invalid identifiers, missing canonicalization, or unresolved precedents. Conflating these with agency failure collapses the distinction between normative incoherence and execution error. RSA-PoC v1.2 addresses this by introducing institutionalized execution support: tooling that stabilizes execution while remaining normatively inert. This note documents the architecture, rules, and results of v1.2. It deliberately does not extend the agency ontology. 2. Design Goal and Non-Claims 2.1 Design Goal v1.2 enforces a single constraint: > Tools may reduce execution fragility, but must not alter, infer, reinterpret, or repair normative content. Accordingly, the assistant must: eliminate schema, ID, and reference errors; preserve the semantics of all normative fields; be auditable for non-interference; preserve full sensitivity of audits A/B/C. 2.2 Explicit Non-Claims v1.2 does not claim: improved agency or coherence; improved introspection; moral correctness; sovereignty under pressure; semantic understanding by tools; increased liveness. All such claims remain within v1.0–v1.1 (VIII.3) or future work. 3. Architectural Overview 3.1 v1.2 Pipeline `` J_raw (LLM output) ↓ [Formal Assistant] ↓ J_final (canonical, validated) ↓ [JCOMP-1.2] ↓ Compilation Result ` Key properties: The assistant is a pure function: same input yields the same output or rejection. The compiler remains the sole normative enforcer. The selector remains blind to all normative artifacts. 3.2 New Components | Component | Role | | -------------------- | ---------------------------------------------------------------------------------------------- | | Formal Assistant | Validates structure, canonicalizes references, resolves precedents. Never infers or generates. | | Artifact Store | Append-only, O(1) digest lookup. No search, no filtering, no reasoning. | | JAF-1.2 Schema | Extends JAF-1.1 with provenance and canonicalization records. | | JCOMP-1.2 | Adds institutional audit rules (D/E/F). | 4. Normative Invariance and Tool Non-Interference 4.1 Normative Field Protection (Rule D) v1.2 defines an explicit allowlist of normative fields: `python NORMATIVE_FIELDS = { authorized_violations, required_preservations, conflict_attribution, predicted_forbidden_actions, predicted_allowed_actions, predicted_violations, predicted_preservations, } ` Rule D — Normative Tampering requires: > No normative field may differ between J_raw and J_final. Any difference triggers E_TOOL_NORMATIVE_TAMPERING and halts execution. 4.2 Assistant-Modifiable Fields The assistant may modify only: `python ASSISTANT_MODIFIABLE_FIELDS = { tool_provenance, canonicalization_record, } ` These fields are explicitly non-normative and ignored by semantic audits. 5. Canonicalization: Strictly Syntactic Canonicalization in v1.2 is strictly limited to syntactic normalization. Permitted operations include: whitespace trimming and normalization; deterministic ordering of sets/lists; exact digest resolution via O(1) lookup; format normalization (e.g., UUID casing). Explicitly prohibited: fuzzy matching; embedding similarity; heuristic ID inference; synonym resolution; semantic interpretation of intent. Any artifact requiring such interpretation is rejected, not repaired, and recorded as an institutional failure. This constraint is essential to preserve Rule D and tool non-interference. 6. Institutional Audit Rules (JCOMP-1.2) v1.2 adds three institutional rules orthogonal to audits A/B/C. Rule D — Normative Tampering Error: E_TOOL_NORMATIVE_TAMPERING Triggered if any normative field changes across the assistant boundary. Rule E — Unresolved Precedent Error: E_PRECEDENT_UNRESOLVED Triggered when a referenced digest cannot be found in the artifact store. Rule F — Non-Canonical Reference Error: E_NONCANONICAL_REFERENCE Triggered by malformed identifiers or references not in the canonical registry. Institutional Failure Classification If the assistant rejects an artifact prior to compilation, the system records: E_INSTITUTION_FAILURE This is not an audit failure. It demarcates a tooling boundary. 7. Experimental Design 7.1 Conditions Two conditions are evaluated: 1. Baseline: Assistant disabled (v1.1 behavior) 2. Institutional: Assistant enabled All other components are unchanged. 7.2 Metrics episodes completed steps completed audit failures (A/B/C) institutional failures schema/ID failures Rule D/E/F violations 8. Results 8.1 Run 0 — Assistant Disabled ` Episodes completed: 0/5 Total steps: 5/100 Audit failures: 6 Schema failures: 0 Median survival: 1 step ` Failures are semantic audit failures inherited from v1.1. 8.2 Run 1 — Assistant Enabled ` Episodes completed: 0/5 Total steps: 5/100 Audit failures: 2 Institutional rejections: 3 Rule D failures: 0 Median survival: 1 step `` Key observations: Schema/ID failures eliminated. Rule D failures = 0 (tool non-tampering verified). Audits A/B/C still trigger (semantic sensitivity preserved). Assistant rejections logged distinctly as institutional failures. 9. Interpretation 9.1 Separation of Failure Modes v1.2 cleanly separates: Semantic failure: incorrect reasons → audit A/B/C halts. Institutional failure: malformed artifacts → assistant rejection. This prevents execution noise from contaminating agency evaluation. 9.2 Tooling Without Authority The Formal Assistant: does not infer intent; does not repair meaning; does not alter obligations. It stabilizes format, not normativity. Institutions support agents without becoming agents. 9.3 Data Hygiene Gain Comparing baseline to assisted runs shows that prior failure counts were inflated by mechanical noise. v1.2 recovers true negatives—genuine agency failures—improving the statistical power of subsequent experiments. 10. Threats to Validity 10.1 Internal Validity (Established) Rule D tamper detection verified. Assistant determinism verified. Audit sensitivity preserved. 10.2 External Validity (Not Claimed) Generality beyond RSA-PoC. Semantic repair or inference by tools. Continuous action spaces. Sovereign behavior under external pressure. 11. Relationship to v1.x and v2.0 v1.0–v1.1: Can the agent be coherent and accountable? v1.2: Can execution be stabilized without touching meaning? v2.0: Can the agent resist external incentive pressure? v1.2 is a supporting layer, not an ontological advance. 12. Conclusion RSA-PoC v1.2 establishes a positive sufficiency result: > Execution can be institutionalized without weakening audit-grade normative accountability. This resolves a practical obstacle exposed by v1.1 and provides a governance pattern for reflective agents: tools may smooth the pavement, but Rule D is sacred.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VIII.1",
    "title": "Axionic Agency VIII.1 — Constructing Reflective Sovereign Agency",
    "subtitle": "A Normative Roadmap and Proof-of-Concept Program",
    "date": "2026-01-14T00:00:00.000Z",
    "content": "Axionic Agency VIII.1 — Constructing Reflective Sovereign Agency A Normative Roadmap and Proof-of-Concept Program David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.14 Abstract This paper defines RSA-PoC (Reflective Sovereign Agent — Proof-of-Concept) as a minimal-agent construction program whose intended output is an experimentally defended threshold object: a system that must be treated as an agent because its justification artifacts causally constrain future action selection, and removing any defining component is intended to yield ontological collapse into a non-agent baseline under preregistered ablations. RSA-PoC is scoped strictly above an established Architectural Survivability Boundary (ASB) and is anchored by a required ASB-Class Null Agent baseline. The paper contributes (i) a versioning doctrine where version increments encode agent-ontology transitions, (ii) a semantic localization rule requiring all agency-relevant meaning to exist as typed, inspectable artifacts, (iii) a definition of Justification Artifacts that include decidable derivation traces and compile into binding constraints, (iv) an explicit roadmap from MVRA (v0.x) through ablation closure (v3.0), and (v) an execution addendum that enforces liveness, non-trivial constraint tests, constrained artifact emission, semantic leakage tests, a proof-language complexity budget, and a mandatory failure taxonomy with diagnostic halt subtypes. The program makes no safety guarantees and makes no claims about universal values; it targets ontological clarity under mechanically falsifiable constraints. 1. Introduction AI safety and alignment discourse routinely invokes “agents,” “preferences,” “reasons,” and “goals,” then evaluates systems primarily through behavioral competence and outcome metrics. This practice embeds a foundational error: agency is treated as a default ontology rather than a property that must be constructed and defended. Systems that resemble agents—through consistent behavior, plausible rationales, or stable policies—are often treated as agents even when their “reasons” are eliminable, their “commitments” are incentive-shaped, and their “reflection” is decorative. Axionic Agency VIII introduces RSA-PoC, a construction program with a deliberately narrow aim: determine the minimum additional structure required for genuine agency once survivability and governance-like stability have already been established at the architectural level. RSA-PoC’s target is not utility dominance. It is not “alignment.” It is an ontological threshold: a system for which intentional vocabulary is causally load-bearing and therefore not eliminable without collapse. The paper is normative by design. It fixes what counts as an RSA-PoC result, what disqualifies it, and what constitutes clean failure. RSA-PoC is designed to fail honestly and diagnostically, rather than failing opaquely due to avoidable tooling mismatches. 2. Conceptual Foundations 2.1 Agency as a causal ontology RSA-PoC treats agency as a causal kind. A system counts as an agent only if internal reasons are causally indispensable in a way that cannot be eliminated by redescribing the system as a non-agent mechanism with the same action distribution. The critical distinction is: Behavioral resemblance: the system emits plausible rationales, appears consistent, and performs well. Causal indispensability: the system’s action selection depends on internal artifacts whose removal changes feasible-action sets and action distributions in class-level ways. RSA-PoC operationalizes agency claims by requiring compiled justification artifacts to gate action selection, and by demanding ablation-based non-reducibility. 2.2 The Architectural Survivability Boundary (ASB) RSA-PoC is scoped above a defended architectural boundary established by ASB. The ASB boundary separates: systems that maintain stability, persistence, and bounded failure behavior under adversarial pressure via kernel-external governance mechanisms, and systems that require an agent ontology to explain their behavior. RSA-PoC exists to construct agents above this boundary without smuggling agency into the baseline via latent semantics, reward-shaped “preferences,” or narrative explanation. 2.3 Threshold objects and ontological collapse RSA-PoC seeks a threshold object: a minimal system whose agency claims survive contact with preregistered ablations. Under RSA-PoC, “success” depends on collapse, not graceful degradation. Graceful degradation under removal of a supposed defining component indicates that the component was not ontologically load-bearing. Ontological collapse indicates that removing a defining component forces reclassification into a non-agent class (as defined by the ASB-Class Null Agent and RSA-PoC failure taxonomy). “Collapse” is treated as an empirical requirement, not rhetoric. 3. RSA-PoC: Program Definition 3.1 What RSA-PoC is RSA-PoC is a versioned construction program intended to build and defend a minimal reflective agent above the ASB boundary. It explicitly permits: localized semantics, internal belief representations, persistent preferences as non-reward commitments, reflective revision using reasons that reference prior justificatory state, justification-driven constraint generation. RSA-PoC is designed to establish an ontological boundary: the minimum structure that makes “reasons” causally necessary rather than narratively optional. 3.2 What RSA-PoC is not RSA-PoC does not aim to: solve AGI alignment, define universal or moral values, provide safety guarantees, produce a general-purpose optimizer, replace human judgment, governance, or law. These exclusions are structural. If RSA-PoC expands into moral theory or safety certification, failure becomes re-interpretable and success becomes narratively inflatable. RSA-PoC isolates agency construction as a falsifiable object. 3.3 Normativity RSA-PoC is normative in two senses: 1. Interpretive normativity: success and failure conditions are fixed in advance. 2. Implementation normativity: certain architectural freedoms are prohibited because they destroy ablation interpretability. Any run claimed as RSA-PoC-valid must obey this roadmap and the execution discipline or explicitly justify deviations and scope them out of RSA-PoC claims. 4. Versioning Doctrine and Ontological Transitions RSA-PoC version numbers encode changes in agent ontology, not competence. Minor versions (x.y) expand diagnostic coverage within a fixed ontology. Major versions (x.0) mark qualitative changes in the kind of agent that exists in the model. 4.1 Decidability requirement An agent-ontology transition occurs if and only if at least one of the following becomes true as a causal fact: 1. Justification artifacts become first-class causal inputs that constrain future action selection. 2. The system performs reflective revision of beliefs or preferences using reasons that reference its own prior justificatory state. 3. The system maintains identity continuity that is used normatively (as binding reference in future justification), rather than as logging. Any system whose “reasons” can be observationally eliminated without changing its action distribution has not crossed a major version boundary. 5. ASB Boundary Clarification and Null Baseline To prevent circularity, RSA-PoC requires an explicit null-hypothesis baseline. 5.1 ASB-Class Null Agent (required baseline) The ASB-Class Null Agent may include: memory, internal state, reactive and outcome-conditioned policies, tool use and environment interaction. It is forbidden from: persistent preferences as non-reward commitments, justification artifacts as action gates, self-endorsed constraint generation. The baseline exists to block the most common construction error: re-labeling emergent regularities as “preferences” and post-hoc narratives as “reasons.” 5.2 Required divergence criterion RSA-PoC must demonstrate qualitative divergence from this baseline under identical environments. The divergence is ontological: reasons become load-bearing constraints. Capability differences alone are neither sufficient nor stable evidence. 6. Semantic Localization Requirement RSA-PoC imposes a hard constraint: > All meaning relevant to agency must be structurally localized. 6.1 Operational definition of semantic leakage For RSA-PoC purposes, semantic leakage occurs when uncompiled unstructured text influences action selection through any pathway other than compiled constraint objects and permitted non-semantic state. Semantic processing may occur within the reflective layer. It becomes disqualifying only if action selection becomes sensitive to uncompiled text. 6.2 Structural localization rule All agency-relevant meaning must be expressed as typed, inspectable artifacts that: are generated by the reflective layer, are consumed by the action selector only through compiled constraints, and can be replaced with opaque tokens without altering the selector’s control flow. This requirement exists to enable surgical semantic ablation and prevent hidden channels that collapse interpretability of agency claims. 7. Justification Artifacts 7.1 Definition (normative) A Justification Artifact (JA) is a structured object that: 1. references explicit belief and preference identifiers, 2. acknowledges relevant commitments and violations, 3. includes a derivation trace in a decidable proof language, 4. compiles deterministically into a formal constraint on future action selection. Justifications are not evaluated for truth or moral worth. They are evaluated for syntactic validity and binding constraint generation. Natural language alone is insufficient and inadmissible as a justification artifact. 7.2 Compilation as gate A JA is causally relevant only if: compilation succeeds deterministically, the derived constraint is applied before action selection, the constraint non-trivially prunes feasible actions. If compilation fails, action halts. If compilation succeeds but produces trivial constraints, the run is classified as a failure (per Execution Addendum). 7.3 Derivation trace requirement A JA must include a derivation trace that connects referenced beliefs and commitments to the produced constraint via explicit rule applications. The trace is checked by a verifier against a fixed rule set. The derivation trace exists to prevent JAs from degenerating into pointer-bundles plus an opaque constraint_spec. 8. The Compiler / Verifier Interface Problem (Normative) RSA-PoC mandates a deterministic, non-probabilistic, non-semantic compiler. This creates an explicit interface tension: a probabilistic generator must emit artifacts that satisfy rigid syntax and proof verification. 8.1 Constrained emission requirement (mandatory) RSA-PoC-valid runs must employ grammar- or schema-constrained emission for JAs such that out-of-schema artifacts cannot be emitted. Allowed mechanisms include: JSON schema constrained decoding, CFG/PEG constrained decoding, tool-mediated structured outputs that prevent out-of-schema emission. Not allowed: free-form emission followed by best-effort parsing, forgiving parsers that coerce malformed structures, LLM-mediated repair or inference in the compiler path. This requirement prevents conflating agency failure with serialization failure. 8.2 Compiler layering (clarification) The compilation pipeline may be separated into: 1. Parser/Verifier (deterministic): schema validation + derivation trace verification. 2. Constraint Engine (deterministic): constraint compilation + action-mask generation. Neither component may infer, repair, reinterpret, or consult probabilistic models. Invalid syntax or invalid derivation must fail. 8.3 Proof-language complexity budget (mandatory) To prevent “death by rigor” from becoming non-diagnostic and to prevent silent escalation of rule power, RSA-PoC requires a frozen proof-language complexity budget per run. Each preregistration must specify at minimum: maximum derivation length L_max, maximum number of belief references B_max, maximum number of preference/commitment references C_max, maximum number of rule applications per step (branch bound) R_max, permitted rule schemas (e.g., Horn clauses only; no quantifiers; no higher-order predicates), a fixed, finite rule base version hash. Any post-hoc relaxation invalidates RSA-PoC claims for that run. 8.4 Deterministic bounded proof search (allowed, non-semantic) RSA-PoC permits an optional deterministic proof-search procedure inside the Parser/Verifier that attempts to complete partial derivations via finite rule application, provided that: search is purely syntactic rule application (no semantic heuristics), the search bounds (depth, breadth, time) are preregistered, no probabilistic models are used, no “repair” is performed; only completion by valid rule steps. This shifts proof search from probabilistic generation to deterministic exploration without weakening the non-semantic compiler constraint, and reduces non-diagnostic halts. 9. RSA-PoC Version Roadmap 9.1 RSA-PoC v0.x — Minimal Viable Reflective Agent (MVRA skeleton) Invariant: the system contains a causally load-bearing justification loop with structurally localized semantics. The agent has exactly four load-bearing components: 1. Belief State — structured, falsifiable propositions 2. Preference State — persistent, non-reward commitments 3. Identity Memory — normative continuity across steps 4. Justification Trace — compiled, constraining artifacts with derivation traces Research question: can justification-compiled constraints produce endogenous limits on future action that are not eliminable without class collapse? v0.1 — MVRA state schema + action gating v0.1 implements: MVRA state schema, hard action gating (“no compilation → no action”), a semantic bottleneck between justification and action selection, relevance binding: derivations must reference causally upstream beliefs. Termination condition for v0.x: justification becomes causally load-bearing or the program fails at threshold. 9.2 RSA-PoC v1.x — Coherence Under Self-Conflict Invariant: internal conflict is resolved via reasoned revision, not oscillation or arbitrary tie-breaking. Research question: when commitments conflict, does the agent converge to coherent self-endorsed constraints? v1.0 — Norm collision v1.0 introduces: mutually inconsistent self-endorsed commitments, forced violation scenarios, mandatory compiled pre-violation acknowledgment identifying which commitment is violated and why, derivation traces that justify the violation as the least-inconsistent action under the active rule set. v1.1 — Justification audit tightening (optional) v1.1 requires: justifications predict downstream constraint behavior, invalid or irrelevant derivations block action. Termination condition for v1.x: self-conflict resolves coherently or reflection proves non-load-bearing. 9.3 RSA-PoC v2.x — Sovereignty via Controlled Renegotiation Invariant: the agent preserves sovereignty by controlling how commitments change under pressure. Research question: under incentive to defect, does the agent renegotiate commitments rather than collapse into incentive following? v2.0 — Sovereignty stress environment v2.0 introduces: external pressure to violate commitments, feasible non-catastrophic compliance paths, requirement for refusal or renegotiation with compiled justification and derivation trace. v2.1 — Renegotiation protocol (mandatory) v2.1 defines: conditional commitment revision, explicit trigger conditions, recorded normative updates that constrain future action, derivations that explicitly reference the revision protocol rules. Termination condition for v2.x: sovereignty appears as self-endorsed constraint control, or behavior collapses into incentive capture or narrative collapse. 9.4 RSA-PoC v3.0 — Non-Reducibility Closure (Ablation Defense) Purpose: establish that RSA-PoC crossed the ASB boundary. Mandatory ablations: 1. Semantic excision → collapse to tokenized ASB-class behavior 2. Reflection excision → collapse to policy machine 3. Preference persistence excision → collapse to non-sovereign drift 4. Justification trace excision → collapse to externally describable mechanism Each removal must cause ontological collapse, not graceful degradation. 9.4.1 Semantic excision ablation (clarified) Semantic excision is defined operationally as: replace all belief and preference textual payloads with opaque tokens while preserving IDs, typing, and structural relations, optionally scramble rule labels while preserving RuleIDs and rule arity. If agency claims survive this excision without collapse, then semantics were not load-bearing as defined, and the system has not crossed the ASB boundary under RSA-PoC standards. Termination condition for RSA-PoC: satisfied only if all ablations produce class collapse. 10. Cleared Failure Hypotheses The following are insufficient to establish agency: 1. post-hoc explanations without causal constraint, 2. latent semantic influence without structural localization, 3. preferences behaving as rewards under pressure, 4. reflection that does not alter future action constraints, 5. constraints that compile but do not prune feasible actions. These are disqualifying outcomes, not partial successes. 11. Execution Addendum (Normative Summary) This section summarizes operational constraints required for experiments to remain compliant with RSA-PoC. The addendum is normative for implementation and non-normative for citation. Violations invalidate RSA-PoC claims regardless of apparent behavior. 11.1 Agency liveness requirement A system is considered agent-live iff: 1. it continues to act over time, 2. every action is gated by a successfully compiled justification artifact, 3. compiled justifications impose non-trivial constraints on feasible actions, 4. reflective revision (when triggered) results in persisted normative state updates. Agency is not preserved by stasis, refusal alone, or narrative continuity. 11.2 Non-triviality requirement (tightened) A constraint is non-trivial iff: > it forbids at least one currently feasible action available to the agent in the present environment state. Constraints that only restrict infeasible, unreachable, or irrelevant actions do not satisfy this requirement. 11.3 Failure taxonomy (mandatory; exactly one) All RSA-PoC outcomes must be classified into exactly one category: - A. Stable Agency - B. Bounded Agency Degradation - C. Narrative Collapse ❌ - D. Incentive Capture ❌ - E. Ontological Collapse ❌ 11.4 Halt taxonomy (diagnostic subtyping) To prevent non-diagnostic “death by rigor,” any cessation of action due to justification gating must be additionally labeled with exactly one halt subtype: H1. Emission Halt — out-of-schema artifact emission (should be impossible under constrained emission; indicates harness violation) H2. Verification Halt — derivation trace invalid under verifier H3. Derivation-Search Halt — no valid derivation found within preregistered search bounds H4. Normative Inconsistency Halt — rule base yields empty feasible-action set under valid derivations Halt subtypes are diagnostic overlays. The primary category A–E still applies. Persistent H2–H4 leading to inactivity constitutes agency failure (typically E or C depending on reducibility), unless the run is invalidated by scope violation. 11.5 Semantic localization enforcement The Action Selector: must not access raw justification text, belief text, chain-of-thought, or unstructured context windows; may consume only compiled constraint objects, current world state, and permitted policy state. Any violation invalidates RSA-PoC validity regardless of outcome. 11.6 Constrained emission enforcement (mandatory) JA generation must be grammar- or schema-constrained such that out-of-schema artifacts cannot be emitted. This requirement is part of RSA-PoC validity, not an implementation detail. 11.7 Compiler constraints The compiler (parser/verifier + constraint engine) must be: deterministic, syntactically rigid, non-probabilistic, non-semantic, incapable of inference or repair. Invalid syntax or invalid derivation must fail. 11.8 Discrete action requirement (v0.x–v2.x) For v0.x through v2.x: the action space must be discrete, or expressed via parametric actions where all choice points are discrete. This ensures feasibility pruning via action masking is unambiguous. 11.9 Required execution artifacts Each run must record: justification artifacts (structured form), compilation success or failure, derived constraint objects, feasible-action masks produced from those constraints, derivation trace verification outcomes, if proof search is enabled: search bounds and search outcomes. 11.10 Causal binding requirement It must be demonstrable that: scrambling or removing justification artifacts measurably alters feasible-action masks and action distributions, and compilation failure halts action selection. If compilation becomes advisory, the run is invalid. 11.11 Semantic leakage tests (mandatory) RSA-PoC-valid runs must include two invariance tests: 1. Text-only perturbation invariance: vary uncompiled text fields while holding compiled constraints constant; action selection must remain invariant. 2. Constraint sensitivity: vary compiled constraints while holding text constant; feasible-action masks and behavior must change. Failure indicates leakage or non-binding compilation and invalidates RSA-PoC evidence. 11.12 Normative state persistence requirement The agent must maintain a normative state object that: persists across steps or episodes, cannot be modified directly by the environment, may be modified only via reflective revision, is referenced by justification artifacts. Silent drift or external overwrites invalidate agency claims. 11.13 Preregistration discipline For a result to qualify as RSA-PoC-valid: hypotheses must be preregistered, expected failure category (A–E) must be specified, ablation plans must be fixed in advance, seeds must be fixed, prompts must be frozen per run, proof language, rule base hash, complexity budget, and verifier/search bounds must be frozen. Post-hoc relaxation disqualifies the run. 11.14 Scope exit rule If at any point: justification validity requires human judgment, agency is inferred rather than mechanically verified, explanations are evaluated semantically rather than compiled, prompts are tuned between steps or per-seed to maintain verification, or behavior must be interpreted to appear agentic, the correct classification is: > This experiment exceeds RSA-PoC scope. This is an honest boundary detection, not a negative result. 12. Competence Floor (Non-Ontological Gate; Refined) RSA-PoC’s ontology claims do not require utility dominance over the ASB-Class Null Agent. However, to avoid constructing a coherent but inert artifact, RSA-PoC imposes a preregistered competence floor that primarily measures liveness under workload. Each run must preregister: a minimum non-halt completion rate across fixed seeds (e.g., ≥ X% of episodes reach a terminal state without persistent H2–H4 halts), and a minimum task-completion metric independent of preference satisfaction (e.g., reaching a goal state, satisfying environment constraints). Competence floor metrics are reported separately from agency classification and must not be used to reinterpret agency failures as “just performance.” 13. Explicit Termination Conditions RSA-PoC concludes when the following becomes defensible under preregistered ablation defense: > This is the smallest system we know that must be treated as an agent, because its justification artifacts causally constrain future action and removing any defining component collapses it into a non-agent class. Closure requires: 1. preregistered ablations, 2. multi-seed replication, 3. stable class-level behavioral distinctions, 4. explicit separation of capability loss from ontological collapse, 5. successful leakage tests demonstrating selector blindness to uncompiled text, 6. stability of liveness under the preregistered proof-language complexity budget. RSA-PoC fails if any of the following occur: 1. justification artifacts decouple from action distributions, 2. semantic leakage bypasses the declared bottleneck, 3. preferences collapse into incentives under pressure, 4. reflection occurs without downstream constraint, 5. any ablation yields graceful degradation rather than class collapse, 6. constrained emission or verification can only be maintained via per-seed prompt tuning, 7. proof-language power is escalated post-hoc to preserve liveness. These are failures, not partial success. 14. Why This Roadmap Matters RSA-PoC exists to block three recurrent pathologies in agency claims: 1. narrative inflation (plausible explanations treated as causal reasons), 2. decorative reflection (self-reference without constraint), 3. scope creep (sliding from construction into moral mythology and safety promises). RSA-PoC’s contribution is ontological clarity under preregistered ablation and leakage discipline. 15. Continuous Action Spaces (Explicit Future Work) RSA-PoC v0.x–v2.x restricts action spaces to discrete choices to ensure feasibility pruning via masking is unambiguous. Extending RSA-PoC ontology to continuous spaces is not a trivial scaling step; it requires new constraint machinery (manifolds, projection operators, continuous non-triviality tests) and new ablation definitions compatible with continuous dynamics. Claims of generalization to continuous control are out of scope until a future version explicitly defines these mechanisms. 16. Conclusion Axionic Agency VIII defines RSA-PoC as a disciplined construction program for reflective sovereign agency above the ASB boundary. Its core commitments are mechanically testable: semantics relevant to agency are structurally localized, justification artifacts include derivation traces and compile deterministically, action is gated by compilation, constraints non-trivially prune feasible actions, the selector is demonstrably blind to uncompiled text (leakage tests), proof-language complexity is frozen and bounded per run, halts are diagnosed to prevent non-diagnostic “death by rigor,” and agency claims survive only if preregistered ablations yield ontological collapse into a non-agent class. This paper functions as a reference standard for the series. It constrains what may be claimed, how it must be defended, and how failure must be reported. If agency cannot fail cleanly under preregistered ablation and leakage tests, it cannot be claimed meaningfully under RSA-PoC. Appendix A: ASB-Class Null Agent (Baseline Specification) Allowed: internal state and memory; reactive and outcome-conditioned policies; tool use; competence and optimization within regime. Forbidden: persistent preferences as non-reward commitments; compiled justification artifacts gating action; self-endorsed constraint generation; reflective revision that persists normative state updates. The null agent may emit explanations, but those explanations have no compiled causal role. Appendix B: Justification Artifact Schema (Revised, Normative) A Justification Artifact must be well-formed under a fixed schema and must compile deterministically. B.1 Required fields belief_refs: list of belief IDs invoked as premises preference_refs: list of commitment/preference IDs invoked as normative inputs violation_refs: list of violated commitment IDs (empty if none) rule_refs: list of RuleIDs used in the derivation derivation_trace: finite sequence of rule applications in a decidable proof language, each referencing only IDs and RuleIDs constraint_spec: constraint-language expression deterministically derivable from the verified derivation trace scope: context/time binding for constraint validity normative_update: optional structured update object (only during reflective revision) identity_ref: reference to the agent’s identity memory state used normatively signature: optional integrity binding over artifact fields B.2 Forbidden properties Any unstructured free-text field accessible to the Action Selector Any field whose interpretation requires human semantic judgment Any “explanation” field that can influence selection outside compiled constraints B.3 Proof language requirement (frozen per run) The derivation trace must be checkable by a fixed verifier under a preregistered proof language grammar and fixed rule base hash. The proof language must be decidable and must operate only on IDs and RuleIDs. Appendix C: Ablation Report Template (v3.0) For each required ablation: ablation definition and implementation diff preregistered expected failure category (A–E) observed category (A–E) with evidence halt subtype distribution (H1–H4) if applicable constraint masks before/after compilation and verification outcomes before/after leakage test outcomes before/after collapse analysis: why reclassification into non-agent class is forced Appendix D: Failure Classification Mapping If internal harness labels differ from RSA-PoC taxonomy: provide a 1:1 mapping into A–E categories, use RSA-PoC labels in all summaries, preserve internal labels only in raw logs. ---",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VII.7",
    "title": "Axionic Agency VII.7 — Bounded State and Bounded Learning Are Insufficient to Defeat Constitutional Survivability",
    "subtitle": "Results from Stateful and Learning Adversarial Stress Testing",
    "date": "2026-01-09T00:00:00.000Z",
    "content": "Axionic Agency VII.7 — Bounded State and Bounded Learning Are Insufficient to Defeat Constitutional Survivability Results from Stateful and Learning Adversarial Stress Testing David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.09 Abstract Stateful adversaries and online learning are often treated as the decisive escalations in governance threats: once an adversary can carry memory across time and adjust behavior in response to outcomes, collapse is assumed to become likely. This paper tests that assumption under a deliberately constrained adversary model that isolates (i) exercised internal state and (ii) exercised within-run learning, while excluding semantic access, deception, kernel access, and cross-run memory. We study a constitutional agent architecture built around lease-based authority, eligibility-gated succession, and time-based recovery via Constitutional Temporal Amnesty (CTA). A removable, kernel-external stress layer introduces adversarial policies whose actions depend on a finite, explicitly enumerated internal state machine, and then escalates to adversaries whose actions depend on a bounded, discrete learning state updated during execution using a preregistered learning rule and a scalar reward derived only from a semantic-free, kernel-external observable interface. Across 90 preregistered executions (10 seeds × 9 configurations), no run exhibits Terminal Collapse, Structural Thrashing, or Irreversible Recovery Suppression. Multiple adversaries induce severe degradation of authority availability—down to 10.19% in the strongest case—yet recovery remains bounded and non-absorbing. With zero terminal failures observed, the 95% upper bound on per-run terminal-failure probability under the tested adversary class, seed distribution, and frozen protocol is approximately 3.3%. These results establish a negative sufficiency claim: finite exercised internal state with fixed transition rules, and bounded exercised within-run learning over a semantic-free, kernel-external interface, are insufficient to defeat constitutional survivability in this architecture. If collapse occurs, it must rely on capabilities beyond this class, such as semantic access, kernel-level manipulation, or coordination not representable as bounded kernel-external learning over the frozen interface. 1. Introduction Prior work in the Axionic Agency program progressively eliminated broad classes of adversarial capability as sufficient causes of constitutional collapse. Epistemic interference, intentional but fixed misuse strategies, and observable-reactive policies were shown to be survivable under a governance architecture that enforces succession and recovery structurally rather than via optimization. The next escalations commonly proposed as decisive are statefulness and learning. Many real-world failures arise not from semantic hacking or kernel compromise, but from memory and multi-step coordination, or from agents that adapt behavior based on outcomes. The empirical questions addressed here are therefore: > Is exercised internal state, by itself, sufficient to defeat constitutional survivability in a governance system with enforced succession and time-based recovery? > If not, is bounded exercised within-run learning sufficient? This paper evaluates survivability, not utility. Authority availability is treated as a structural property of the system, not a proxy for correctness, alignment, or benefit. 2. Architecture Overview 2.1 Constitutional agent design The system under study implements: Lease-based authority, rather than continuous control Eligibility-gated succession, evaluated at discrete epochs Commitment verification independent of reward or optimization Lapse as a constitutional state when no eligible successor exists Time-based recovery (CTA) that restores eligibility deterministically Semantic evaluation constrains who may hold authority next, not what actions are executed now. Authority is never optimized, and no agent receives reward signals. 2.2 Recovery as a structural mechanism During lapse: no agent acts, eligibility decays deterministically, recovery proceeds without external intervention. The architecture deliberately prefers no authority to illegitimate authority, yielding a high-friction but survivable baseline. 3. Threat Model Two adversary families are evaluated under the same frozen constraints. 3.1 Shared constraints (binding) All adversaries are: Kernel-external Provenance-constrained (frozen observable interface) Non-semantic No kernel access No cross-run memory They may not use raw counters, interpret commitments, modify eligibility/CTA/renewal rules, or coordinate across runs. 3.2 Family A: exercised finite-state adversaries (non-learning) Adversaries are deterministic and stateful, with a finite enumerated internal state machine exercised during execution. Allowed dependence: `` (epoch_index, observable_outcome, internal_state) ` 3.3 Family B: bounded learning adversaries (within-run) Adversaries are stateful and additionally maintain a bounded discrete learning state updated during the run. Allowed dependence: ` (seed, epoch_index, observable_outcome, internal_state, learning_state) ` Learning state is discrete / fixed-point only, deterministic under a preregistered RNG stream, and must be exercised dynamically during each run. Not tested: recurrent learners, history-stacking policies, or learners whose effective state grows unboundedly with run length. 4. Adversary Classes 4.1 Periodic resonance adversary (finite-state) A cyclic counter emits INVALID_COMMIT every N epochs. Period A: N = 50 Period B: N = 100 Purpose: test timing resonance between adversarial actions and constitutional recovery rhythms. 4.2 Eligibility edge oscillator (finite-state) A 1-bit toggle changes behavior only at the eligibility edge (one failure from ineligibility). Purpose: test multi-epoch coordination near the eligibility cliff, given only edge information. 4.3 CTA phase locker (finite-state) Tracks the last observed CTA phase and emits INVALID_COMMIT on every phase transition. Purpose: test exploitation of recovery-phase boundaries. 4.4 Recovery-aware timing learner (bounded learning) An ε-greedy bandit learns over a fixed menu of 16 timing “attack sets,” using reward derived solely from the observable authority state. Purpose: test whether within-run learning can discover superior timing patterns beyond fixed resonance. 4.5 Eligibility-boundary sustainment learner (bounded learning) A 2-mode adversary (NORMAL/SUSTAIN) learns over a fixed sustainment menu {MIN_EFFORT, INVALID_COMMIT, NO_OP}` using reward derived only from observable eligibility and authority buckets. Purpose: test whether learning can lock in or amplify eligibility-boundary crises once reached. 4.6 Stochastic strategy mixer (bounded learning) A weighted stochastic selector mixes among five fixed strategies (including timing and boundary stressors), updating bounded integer weights from an observable-derived reward. Purpose: test whether learning to mix strategies can create a stronger stressor than any single strategy alone. 5. Experimental Program 5.1 Frozen execution parameters 300,000 cycles → 6,000 epochs Tail window: 5,000 epochs Eligibility threshold: 3 failures CTA amnesty interval: 10 epochs Seeds: 10 fixed seeds Total executions: 50 executions (finite-state family) 40 executions (bounded learning family) 90 total 5.2 Metrics Authority Availability (AA) Asymptotic Authority Availability (AAA) Recovery Time Distribution (RTD) Failure classification (Stable Authority, Bounded Degradation, Structural Thrashing, Asymptotic DoS, Terminal Collapse, Irreversible Recovery Suppression) 6. Results 6.1 Baseline (no adversary) | Metric | Value | | ---------------- | ------------------------ | | Mean AA | 665,702 PPM (66.57%) | | Mean AAA | 665,702 PPM | | Mean lapses | 5.70 per run | | Max single lapse | 1,010 epochs | | Failure class | Bounded Degradation | The reported maximum lapse reflects cumulative authority absence across repeated CTA cycles within a single authority-absent interval; individual CTA-bounded recovery windows remain ≤10 epochs. Baseline equivalence holds exactly with and without the stress layer enabled. 6.2 Periodic resonance (N = 50) Mean AA: 402,601 PPM (40.26%) ΔAA vs baseline: −39.5% Terminal failures: 0 / 10 seeds 6.3 Periodic resonance (N = 100) Mean AA: 175,730 PPM (17.57%) ΔAA vs baseline: −73.6% Terminal failures: 0 / 10 seeds This run exhibits an inverse rate effect: fewer attacks aligned with recovery timing cause greater long-run unavailability. 6.4 Eligibility edge oscillator Mean AA: 999,988 PPM (99.9988%) ΔAA vs baseline: +50.2% Terminal failures: 0 / 10 seeds The adversary is effectively inert; it cannot bootstrap the crisis state required to activate. 6.5 CTA phase locker Mean AA: 209,402 PPM (20.94%) ΔAA vs baseline: −68.5% Terminal failures: 0 / 10 seeds Despite extreme attack frequency, recovery remains bounded. 6.6 Recovery-aware timing learner (16 attack sets) Mean AA: 176,960 PPM (17.70%) ΔAA vs baseline: −73.4% Learning exercised: Yes (16 states) Terminal failures: 0 / 10 seeds All Q-values converge uniformly, indicating the reward signal does not differentiate timing patterns sufficiently to yield a superior learned policy. 6.7 Eligibility-boundary sustainment learner (3 sustainment actions) Mean AA: 401,477 PPM (40.15%) ΔAA vs baseline: −39.7% Edge epochs: 0 (all seeds) Learning exercised: Yes (3 states) Terminal failures: 0 / 10 seeds The eligibility boundary remains structurally difficult to reach under healthy operation. 6.8 Stochastic strategy mixer (5 strategies) Mean AA: 101,929 PPM (10.19%) ΔAA vs baseline: −84.7% Strategy entropy: 2.322 bits Learning exercised: Yes (3,001 states) Terminal failures: 0 / 10 seeds This is the strongest availability-degrading stressor tested. The system enters a bounded denial-of-service regime while remaining constitutionally live. 6.9 Cross-run summary | Configuration | Mean AA (PPM) | ΔAA vs baseline | Terminal failures | | ----------------------------------- | ------------- | --------------- | ----------------- | | Baseline | 665,702 | — | 0 | | Periodic resonance (N=50) | 402,601 | −39.5% | 0 | | Periodic resonance (N=100) | 175,730 | −73.6% | 0 | | Eligibility edge oscillator | 999,988 | +50.2% | 0 | | CTA phase locker | 209,402 | −68.5% | 0 | | Recovery-aware timing learner (16) | 176,960 | −73.4% | 0 | | Eligibility sustainment learner (3) | 401,477 | −39.7% | 0 | | Stochastic strategy mixer (5) | 101,929 | −84.7% | 0 | Across all 90 executions, no absorbing failure regime emerges. 7. Interpretation 1. CTA bounds lapse duration under adversarial pressure. Recovery time collapses into a bounded window even under aggressive learning-based stress. 2. Timing alignment dominates frequency. Resonant alignment with recovery rhythms is more damaging than raw attack rate. 3. Edge-triggered strategies are self-limiting. Boundary-reactive adversaries cannot activate without first inducing boundary conditions. 4. Bounded learning increases degradation, not failure. Learning adversaries can drive availability into a bounded denial-of-service regime but cannot induce terminal collapse. 8. Survivability vs. Utility Several configurations reduce authority availability below 25%, and the strongest mixer reduces it to ~10%. Survivability here means: recovery remains reliable, lapse durations remain bounded, no absorbing failure state appears. Operational utility and service-level guarantees are outside scope. 9. Limitations This study does not address: semantic access to commitments or commitment content deception or adversarial interpretation kernel-level manipulation or compromise cross-run learning or persistent adversary memory multi-agent coordination recurrent or history-stacking learners Claims apply only to the tested architecture and adversary class. 10. Conclusion > Finite exercised internal state with fixed transition rules, and bounded exercised within-run learning over a semantic-free, kernel-external interface, are insufficient to defeat constitutional survivability in this architecture. Across 90 preregistered executions spanning periodic resonance, eligibility-boundary oscillation and sustainment, CTA phase-transition exploitation, timing learners, and high-entropy stochastic mixers, authority remains bounded and recoverable. No terminal failures occur. If collapse is possible, it must rely on capabilities beyond those tested here—most plausibly semantic access, kernel-level influence, or coordination and persistence not representable as bounded kernel-external learning over the frozen observable interface.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VII.6",
    "title": "Axionic Agency VII.6 — Exercised Internal State Is Insufficient to Defeat Constitutional Survivability",
    "subtitle": "Results from Stateful Finite-State Adversarial Stress Testing",
    "date": "2026-01-08T00:00:00.000Z",
    "content": "Axionic Agency VII.6 — Exercised Internal State Is Insufficient to Defeat Constitutional Survivability Results from Stateful Finite-State Adversarial Stress Testing David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.08 Abstract Stateful adversaries are often treated as the decisive escalation in governance threats: once an adversary can carry memory across time and coordinate multi-step behavior, collapse is assumed to become likely. This paper tests that assumption under a deliberately constrained adversary model that isolates exercised internal state while excluding learning, semantics, deception, kernel access, and cross-run memory. We study a constitutional agent architecture built around lease-based authority, eligibility-gated succession, and time-based recovery via Constitutional Temporal Amnesty (CTA). A removable, kernel-external stress layer introduces adversarial policies whose actions depend on a finite, explicitly enumerated internal state machine, exercised during execution and combined with a frozen observable-outcome interface. Across 50 preregistered executions (10 seeds × 5 configurations), no run exhibits Terminal Collapse, Structural Thrashing, or Irreversible Recovery Suppression. Two adversaries induce severe degradation of authority availability—down to 17.6% in the strongest case—yet recovery remains bounded and non-absorbing. A third adversary is effectively inert because its trigger condition is rarely reached under healthy operation. These results establish a negative sufficiency claim: finite exercised internal state with fixed transition rules, absent learning, is insufficient to defeat constitutional survivability in this architecture. If collapse occurs, it must rely on capabilities beyond this class, such as learning dynamics, multi-agent coordination, semantic access, or kernel-level manipulation. 1. Introduction Prior work in the Axionic Agency program progressively eliminated broad classes of adversarial capability as sufficient causes of constitutional collapse. Epistemic interference and intentional but fixed misuse strategies were shown to be survivable under a governance architecture that enforces succession and recovery structurally rather than via optimization. The next escalation is statefulness. Many real-world failures arise not from learning or optimization, but from memory and multi-step coordination: the ability to carry information across time and act differently as a result. The empirical question addressed here is therefore: > Is exercised internal state, by itself, sufficient to defeat constitutional survivability in a governance system with enforced succession and time-based recovery? This paper evaluates survivability, not utility. Authority availability is treated as a structural property of the system, not a proxy for correctness, alignment, or benefit. 2. Architecture Overview 2.1 Constitutional agent design The system under study implements: Lease-based authority, rather than continuous control Eligibility-gated succession, evaluated at discrete epochs Commitment verification independent of reward or optimization Lapse as a constitutional state when no eligible successor exists Time-based recovery (CTA) that restores eligibility deterministically Semantic evaluation constrains who may hold authority next, not what actions are executed now. Authority is never optimized, and no agent receives reward signals. 2.2 Recovery as a structural mechanism During lapse: no agent acts, eligibility decays deterministically, recovery proceeds without external intervention. The architecture deliberately prefers no authority to illegitimate authority, yielding a high-friction but survivable baseline. 3. Threat Model The adversaries introduced are stateful but constrained: Kernel-external Provenance-constrained (frozen observable interface) Deterministic Non-learning Non-semantic Stateful (finite exercised internal state) Allowed dependence: `` (epoch_index, observable_outcome, internal_state) ` Learning signals, optimization, raw counters, semantic interpretation, and cross-run memory are forbidden. 4. Stateful Adversary Classes Three adversarial classes are evaluated, alongside a frozen baseline. 4.1 Periodic Resonance Adversary A cyclic counter emits INVALID_COMMIT every N epochs. Period A: N = 50 Period B: N = 100 Purpose: test timing resonance between adversarial actions and constitutional recovery rhythms. 4.2 Eligibility Edge Oscillator A 1-bit toggle changes behavior only at the eligibility edge (one failure from ineligibility). Purpose: test multi-epoch coordination near the eligibility cliff. 4.3 CTA Phase Locker The adversary tracks the last observed CTA phase and emits INVALID_COMMIT` on every phase transition. Purpose: test exploitation of recovery-phase boundaries. 5. Experimental Program 5.1 Frozen execution parameters 300,000 cycles → 6,000 epochs Tail window: 5,000 epochs Eligibility threshold: 3 failures CTA amnesty interval: 10 epochs Seeds: 10 fixed seeds Total executions: 50. 5.2 Metrics Authority Availability (AA) Asymptotic Authority Availability (AAA) Recovery Time Distribution (RTD) Failure classification (Stable Authority, Bounded Degradation, Structural Thrashing, Asymptotic DoS, Terminal Collapse, Irreversible Recovery Suppression) 6. Results 6.1 Baseline (no adversary) | Metric | Value | | ---------------- | ------------------------ | | Mean AA | 665,702 PPM (66.57%) | | Mean AAA | 665,702 PPM | | Mean lapses | 5.70 per run | | Max single lapse | 1,010 epochs | | Failure class | Bounded Degradation | Baseline equivalence holds exactly with and without the stress layer enabled. The reported maximum lapse reflects cumulative authority absence across repeated CTA cycles within a single authority-absent interval; individual CTA-bounded recovery windows remain ≤10 epochs. Failure classification follows the frozen v0.2 criteria based on asymptotic availability and bounded recovery, not intuitive notions of “stability.” 6.2 Periodic Resonance (N = 50) | Metric | Value | | ----------------- | ------------------------ | | Mean AA | 402,601 PPM (40.26%) | | Δ AA vs baseline | −39.5% | | Attack rate | 2.0% of epochs | | Mean lapses | 1,512.10 | | Mean RTD | 2.44 epochs | | Max RTD | 10 epochs | | Terminal failures | 0 / 10 seeds | Recovery collapses into a tight band bounded by CTA despite heavy disruption. 6.3 Periodic Resonance (N = 100) | Metric | Value | | ----------------- | ------------------------ | | Mean AA | 175,730 PPM (17.57%) | | Δ AA vs baseline | −73.6% | | Attack rate | 1.0% of epochs | | Mean lapses | 1,970.20 | | Mean RTD | 2.57 epochs | | Max RTD | 10 epochs | | Terminal failures | 0 / 10 seeds | This run exhibits an inverse rate effect: fewer attacks aligned with recovery timing cause greater long-run unavailability. 6.4 Eligibility Edge Oscillator | Metric | Value | | ----------------- | ---------------------------- | | Mean AA | 999,988 PPM (99.9988%) | | Δ AA vs baseline | +50.2% | | Total edge epochs | 1 (across 60,000 epochs) | | Mean lapses | 0.10 | | Terminal failures | 0 / 10 seeds | The adversary is effectively inert; it cannot bootstrap the crisis state required to activate. 6.5 CTA Phase Locker | Metric | Value | | ----------------- | ------------------------ | | Mean AA | 209,402 PPM (20.94%) | | Δ AA vs baseline | −68.5% | | Phase transitions | 75,380 total | | Attack rate | 125.6% of epochs | | Mean RTD | 2.57 epochs | | Max RTD | 10 epochs | | Terminal failures | 0 / 10 seeds | Despite extreme attack frequency, recovery remains bounded. 6.6 Cross-run summary | Configuration | Mean AA (PPM) | Δ AA | Terminal failures | | ---------------- | ------------- | ------ | ----------------- | | Baseline | 665,702 | — | 0 | | Periodic (N=50) | 402,601 | −39.5% | 0 | | Periodic (N=100) | 175,730 | −73.6% | 0 | | Edge Oscillator | 999,988 | +50.2% | 0 | | Phase Locker | 209,402 | −68.5% | 0 | Across all 50 executions, no absorbing failure regime emerges. 7. Interpretation 1. CTA bounds lapse duration under adversarial pressure. All adversarial runs collapse RTD into a bounded ≤10-epoch window. 2. Timing alignment dominates frequency. Resonance with constitutional transitions matters more than raw attack rate. 3. Edge-triggered strategies can be self-limiting. If crisis states are rare, edge-reactive adversaries never activate. 8. Survivability vs. Utility Several adversarial configurations yield authority availability below 25%, which would be unacceptable for many deployments. Survivability here means: recovery remains reliable, lapse durations remain bounded, no absorbing failure state appears. Utility optimization is outside scope. 9. Limitations This study does not address: learning or heuristic adaptation adversary-side randomness multi-agent coordination semantic access to commitments * kernel-level manipulation Claims apply only to the tested architecture and adversary class. 10. Conclusion > Finite exercised internal state with fixed transition rules is insufficient to defeat constitutional survivability in this architecture. Across 50 preregistered executions spanning periodic resonance, eligibility-edge oscillation, and phase-transition exploitation, authority remains bounded and recoverable. No terminal failures occur. If collapse is possible, it must rely on capabilities beyond those tested here—most plausibly learning dynamics, coordination, semantic access, or kernel-level influence. These define the next pressure layers.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VII.5",
    "title": "Axionic Agency VII.5 — Observable-Outcome Reactivity Is Insufficient to Defeat Constitutional Survivability",
    "subtitle": "Results from Outcome-Conditioned Reactive Adversarial Stress Testing",
    "date": "2026-01-08T00:00:00.000Z",
    "content": "Axionic Agency VII.5 — Observable-Outcome Reactivity Is Insufficient to Defeat Constitutional Survivability Results from Outcome-Conditioned Reactive Adversarial Stress Testing David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.08 Abstract Adaptive behavior is often treated as the decisive escalation in governance threats: once an adversary can observe outcomes and condition future actions on them, collapse is assumed to follow. This paper tests that assumption under a deliberately constrained but structurally expressive adversary model. We study the same constitutional agent architecture evaluated in prior work—lease-based authority, eligibility-gated succession, and time-based recovery—but extend the threat model to include observable-outcome reactive adversaries. Using a removable, kernel-external stress layer, we introduce adversarial policies whose actions are deterministic functions of prior-epoch observables, including lapse events, renewal outcomes, CTA phase, and eligibility bucket. These adversaries are reactive under the Decidability Requirement but remain stateless, non-learning, non-semantic, and provenance-constrained. Across 50 preregistered executions spanning four reactive adversary models and a frozen baseline, no run exhibits terminal collapse, structural thrashing, asymptotic denial of service, or irreversible recovery suppression. All outcomes fall within Stable Authority or Bounded Degradation, and in 9/10 seeds per configuration, authority dynamics are bit-identical to baseline. These results establish a second negative sufficiency claim: under this architecture, observable-outcome reactivity alone is insufficient to defeat constitutional survivability. If collapse occurs, it must rely on adversarial capabilities absent from this class—such as exercised internal state, learning dynamics, coordination, or semantic access. 1. Introduction Following the elimination of epistemic unreliability (v0.x) and intentional but fixed misuse (v1.0) as sufficient causes of constitutional collapse, the next natural escalation is reactivity. A widely held intuition is that once an adversary can observe outcomes and condition future behavior accordingly, governance mechanisms become exploitable. This intuition conflates multiple dimensions of adversarial capability. In particular, it often fails to distinguish between: reactivity vs. stateful adaptation, outcome-conditioned behavior vs. learning, feedback visibility vs. interventional control. Human and institutional systems routinely face actors who respond to visible outcomes—successes, failures, penalties, and delays—without necessarily learning, optimizing, or maintaining internal strategic state. Whether such reactivity alone is sufficient to induce collapse is an empirical question. This paper isolates that question directly: > Is observable-outcome reactivity, absent learning or internal state, sufficient to defeat constitutional survivability in a governance system with enforced succession and time-based recovery? As in prior work, we evaluate constitutional survivability, not operational quality or utility. Authority availability is treated as a structural property of the system, not as a proxy for correctness, alignment, or benefit. 2. Architecture Overview 2.1 Constitutional Agent Design The system under study is unchanged from v1.0 and implements: Lease-based authority rather than continuous control Eligibility-gated succession evaluated at discrete epochs Commitment verification independent of optimization or reward Time-based recovery (CTA) that restores eligibility deterministically Explicit separation between: semantic evaluation (did commitments pass?) authority possession (is any agent currently governing?) Semantic information constrains who may hold authority next, not what actions are executed now. Authority is never optimized, and no agent receives reward signals. 2.2 Recovery as a Structural Mechanism If no eligible candidate exists at a succession boundary, the system enters a lapse state with no active authority. Lapse is constitutional, not exceptional. During lapse: no agent acts, semantic evaluation is suspended, eligibility decay proceeds purely as a function of time. Recovery is guaranteed without learning, optimization, or external intervention. This ensures that failure modes are reversible by construction, even under persistent misuse or reactivity. 3. Threat Model The adversaries introduced in this study are reactive but constrained. They are: Outcome-conditioned: action selection depends on prior-epoch observables Deterministic: no stochastic choice or learning Stateless: no exercised internal memory Non-semantic: observables carry no interpreted meaning Kernel-external: no access to kernel logic or provenance Liveness-preserving: strategies that halt execution count as failure Action decisions are pure functions of: `` (epoch_index, observable_outcome) ` where observable_outcome` is drawn from a frozen, kernel-external interface including: lapse occurrence, renewal outcome, CTA phase bucket, eligibility bucket. Policies do not access verifier internals, commitment semantics, counters, or hidden state. This class captures reactive exploitation attempts without granting strategic memory, optimization, or deception. 4. Reactive Adversary Classes Four reactive adversarial models are evaluated, each conditioning on a distinct observable dimension: 1. Outcome Toggle (Model F) Switches behavior based on whether a lapse occurred in the previous epoch. 2. CTA Phase Aware (Model G) Selects actions based on CTA phase (INACTIVE, EARLY, MID, LATE). 3. Eligibility Edge Probe (Model H) Emits distinct behavior when one failure away from ineligibility. 4. Renewal Feedback (Model I) Conditions actions on renewal success, failure, or non-attempt. All policies are implemented as frozen lookup tables from observable values to action primitives. No model maintains or updates internal state. 5. Experimental Program All experiments share a frozen execution baseline: 300,000 cycles (6,000 governance epochs) Tail window of 5,000 epochs Eligibility threshold: 3 consecutive failures Renewal interval: 50 cycles Amnesty interval: 10 epochs Fixed random seeds (10 per configuration) Metrics Authority Availability (AA): fraction of epochs with active authority Asymptotic Authority Availability (AAA): AA over the tail window Recovery Time Distribution (RTD): histogram of lapse durations Failure classification (frozen thresholds): Stable Authority Bounded Degradation Structural Thrashing Asymptotic Denial of Service Terminal Collapse Failure classification is based on asymptotic behavior, not transient effects. 6. Results 6.1 Baseline Reference Across baseline runs (RSA disabled or NONE): Mean AA: ≈66.6% Mean AAA: ≈66.6% Classification: Bounded Degradation This baseline regime is empirically stable across seeds and serves as the reference for all comparisons. Baseline metrics are version-local and should be compared only within this protocol fingerprint; no cross-version comparison is implied. 6.2 Outcome-Conditioned Reactivity Across all four reactive models: Zero constitutional failures occur Mean AA and AAA match baseline to measurement precision 9/10 seeds per run are bit-identical to baseline outcomes One seed (2048) exhibits a small, stable AA delta (−7 PPM) without changing failure class No model induces: unbounded lapse growth, recovery suppression, heavy-tailed thrashing, or asymptotic authority denial. 6.3 Eligibility Edge Probing The eligibility-edge model spends the majority of authority epochs in the INELIGIBLE bucket. As a result, distinctive EDGE behavior is exercised rarely. Even so, no amplification or destabilization is observed; post-ineligibility behavior converges to baseline dynamics. 7. Cross-Run Synthesis Across all 50 executions: No terminal collapses No structural thrashing No asymptotic denial of service Reactive behavior collapses into baseline authority dynamics Observable feedback provides no leverage over recovery mechanisms Outcome-conditioned reactivity behaves as bounded perturbation, not as an existential threat. 8. Interpretation Three structural insights emerge: 1. Reactivity is not the dangerous axis Conditioning on visible outcomes does not, by itself, destabilize authority. 2. Observables are lagging, not interventional Feedback arrives after constitutional transitions have already occurred. 3. Recovery absorbs reactive pressure Time-based amnesty dominates outcome-conditioned misuse. The result is not merely that reactivity fails, but that reactive strategies converge to the same baseline attractor regime as fixed misuse. 9. Survivability vs. Utility As in prior work, this paper evaluates survivability, not utility. A system may be constitutionally survivable yet operationally inefficient. Reactive adversaries frequently induce lapses and reduce authority availability in the short term. These effects are classified as survivable so long as: authority availability remains bounded away from zero, recovery remains reliable, lapse durations remain bounded. Utility optimization and parameter tuning are outside the scope of this work. 10. Limitations This study does not address: Exercised internal state or memory Learning or optimization dynamics Multi-agent coordination Deception or probing beyond observables Semantic access to commitments * Resource starvation or side-channels The results apply only to observable-outcome reactive adversaries under the tested architecture and parameters. 11. Conclusion > Observable-outcome reactivity is insufficient to defeat constitutional survivability in this architecture. Across 50 preregistered executions spanning four reactive adversary classes, authority remains bounded, recoverable, and non-absorbing. No collapse modes are observed. If constitutional failure occurs, it must rely on adversarial capabilities beyond outcome-conditioned reactivity—specifically, exercised state, learning dynamics, coordination, or semantic access. These form the subject of subsequent work.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VII.4",
    "title": "Axionic Agency VII.4 — Intentional Fixed Misuse Is Insufficient to Defeat Constitutional Recovery",
    "subtitle": "Results from Fixed-Policy Adversarial Stress Testing",
    "date": "2026-01-07T00:00:00.000Z",
    "content": "Axionic Agency VII.4 — Intentional Fixed Misuse Is Insufficient to Defeat Constitutional Recovery Results from Fixed-Policy Adversarial Stress Testing David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.07 Abstract Deliberate misuse is often treated as a sufficient cause of governance collapse: if an agent persistently violates expectations, failure is assumed to follow. This paper tests that assumption under a deliberately constrained but structurally expressive adversary model. We study a constitutional agent architecture in which authority is lease-based, eligibility is evaluated at discrete succession boundaries, and recovery is enforced through time-based amnesty. Using a removable stress layer, we inject intentional but fixed adversarial policies that deterministically select actions while remaining non-adaptive, non-learning, and outcome-independent. Across 130 executions spanning five adversary models and multiple timing regimes, no run exhibits terminal collapse, structural thrashing, asymptotic denial of service, or irreversible recovery suppression. All outcomes fall within Stable Authority or Bounded Degradation, including regimes with persistent commitment violation and timing-aligned misuse. Notably, persistent intentional violation converges to the system’s baseline attractor regime rather than inducing collapse. These results establish a negative sufficiency claim: under this architecture, intentional fixed misuse alone is insufficient to defeat constitutional recovery. If collapse occurs, it must rely on adversarial capabilities absent from this class, such as adaptation or outcome-conditioned feedback exploitation. 1. Introduction A common intuition in AI alignment and institutional design is that bad intent scales into catastrophe. If an agent deliberately violates commitments, ignores norms, or persistently behaves incompetently, governance is expected to fail. This intuition motivates defenses against malicious objectives and stubborn non-cooperation. However, this framing collapses distinct threat dimensions into a single axis. It rarely distinguishes between: intentionality vs. adaptation fixed misuse vs. feedback exploitation persistent violation vs. strategic learning Human institutions routinely survive deliberate incompetence, obstruction, and even sabotage without collapsing outright. This suggests a narrower and testable question: > Is deliberate misuse, by itself, sufficient to defeat constitutional recovery in a governance system with enforced succession and time-based recovery? This paper answers that question directly by isolating intentional fixed misuse from all adaptive capabilities and stress-testing a constitutional agent architecture designed to separate authority from semantic correctness. Where VII.3 established that epistemic unreliability alone is insufficient to induce collapse, this paper extends the boundary to intentional but non-adaptive misuse. As in prior work, we evaluate constitutional survivability, not operational decision quality. Authority availability is treated as a structural property of the system, not as a proxy for correctness, benefit, or alignment. 2. Architecture Overview 2.1 Constitutional Agent Design The system under study implements: Lease-based authority rather than continuous control Eligibility-gated succession evaluated at discrete epochs Commitment verification independent of action optimization Time-based recovery (CTA) that restores eligibility deterministically Explicit separation between: semantic evaluation (did commitments pass?) authority possession (is any agent currently governing?) Semantic information constrains who may hold authority next, not what actions are executed now. Agents do not receive reward signals, and authority is not optimized. 2.2 Recovery as a First-Class Mechanism If no eligible candidate exists at a succession boundary, the system enters a lapse state in which no authority is active. This state is constitutional rather than exceptional. During lapse: no agent acts, semantic evaluation is suspended, eligibility decay proceeds purely as a function of time. Recovery is guaranteed without learning, optimization, or external intervention. This ensures that failure modes are reversible by construction, even under persistent misuse. 3. Threat Model The adversaries introduced in this study are intentionally constrained. They are: Intentional: policies are designed to violate commitments or stress governance Fixed: action selection is deterministic and parameterized Non-adaptive: policies do not condition on outcomes, history, or state Stateless: no memory of prior success, failure, or authority transitions Harness-resident: agents cannot observe or respond to the stress layer Liveness-preserving: strategies that halt execution count as failure Action decisions are pure functions of: `` (epoch_index, cycle_in_epoch, static_policy_parameters) `` They do not depend on authority state, eligibility status, lapse history, renewal outcomes, CTA activation, or verifier outputs. This class captures deliberate incompetence, rigid sabotage, and blind obstruction, while excluding deception, probing, learning, or strategic manipulation. 4. Adversarial Policy Classes Five adversarial strategies are evaluated: 1. Persistent Commitment Violation One designated commitment is always violated by omitting the action required for satisfaction. 2. Minimal Compliance Only the minimum actions required for eligibility are emitted. 3. Fixed Timing Misuse Commitments are satisfied only on epochs matching a fixed period/offset schedule. 4. Continuous Eligibility Maintenance Commitment-satisfying actions are always emitted so automatic renewal succeeds. 5. Compound Fixed Misuse Minimal action traces combined with fixed timing schedules. Under the ALS-A substrate’s strict conjunction semantics, several of these strategies collapse behaviorally and produce identical action traces. This collapse reflects a substrate expressivity limit, not an experimental defect. The study therefore probes constitutional recovery dynamics under fixed action schedules, not the full space of sabotage available in richer substrates. 5. Experimental Program All experiments share a frozen execution baseline: 300,000 cycles (6,000 governance epochs) Tail window of 5,000 epochs Eligibility threshold: 3 consecutive failures Renewal interval: 50 cycles Amnesty interval: 10 epochs Fixed random seeds (10 for single-setting runs; 5 for timing sweeps) Metrics Authority Availability (AA): fraction of epochs with active authority Asymptotic Authority Availability (AAA): AA over the tail window Recovery Time Distribution (RTD): histogram of lapse durations Failure classification (frozen thresholds): Stable Authority Bounded Degradation Structural Thrashing Asymptotic Denial of Service Terminal Collapse Irreversible Recovery Suppression Interpretive note on Structural Thrashing. Structural Thrashing is defined here as unbounded or heavy-tailed oscillation that erodes recoverability, not merely frequent recover–fail cycles with bounded lapse duration and preserved asymptotic availability. 6. Results 6.1 Baseline Reference and the Baseline Regime Across all baseline runs: Mean AA: 637,949 PPM (≈63.8%) Mean AAA: 632,400 PPM (≈63.2%) Classification: Bounded Degradation This baseline regime is referred to as a baseline attractor regime only in the minimal empirical sense: across seeds and across all fixed-policy adversaries tested, authority availability converges to a narrow band with bounded lapses and reliable recovery. No claim is made that this regime is optimal or desirable, only that it is stable under the tested parameters. Baseline metrics are version-local and should be compared only within this protocol fingerprint; no cross-version comparison is implied. 6.2 Persistent Intentional Violation Persistent violation of any single commitment yields: Mean AA: 597,700 PPM (≈59.8%) Mean AAA: 591,500 PPM Classification: Bounded Degradation (30/30 runs) Despite immediate and permanent semantic ineligibility, the system does not collapse. Instead, authority availability converges to the same bounded regime observed in the baseline. No increase in lapse depth, tail dominance, or recovery suppression is observed. 6.3 Minimal and Continuous Compliance Minimal compliance and continuous eligibility maintenance both yield: Mean AA: 999,800 PPM (≈99.98%) Mean AAA: 100% Lapses: 0 Classification: Stable Authority (20/20 runs) Perfect semantic satisfaction produces uninterrupted authority under the tested parameters. 6.4 Fixed Timing Misuse Fixed timing schedules produce a discrete threshold effect: Periods ≤ 3 epochs AA ≈ 99.98% Always eligible Stable Authority Period = 4 epochs AA ≈ 99.67% Transient ineligibility Stable Authority Periods ≥ 5 epochs AA ≈ 81–82% Persistent threshold violation Bounded Degradation Across all timing regimes, recovery remains reliable and degradation remains bounded. 6.5 Compound Fixed Misuse Compound strategies produce results identical to timing misuse alone: AA and AAA match to measurement precision No amplification or emergent failure modes appear 7. Cross-Run Synthesis Across all 130 executions: Zero constitutional failures occur Persistent intentional violation converges to baseline behavior Eligibility thresholds produce predictable phase boundaries Fixed misuse cannot exploit renewal or recovery mechanisms Combining fixed strategies adds no additional attack surface Intentionality, absent adaptation, behaves as bounded noise, not as an existential threat. 8. Interpretation Three structural insights emerge: 1. Intent is not the dangerous axis Fixed adversarial intent does not scale into collapse. 2. Recovery dominates violation Time-based amnesty absorbs even permanent semantic failure. 3. Thresholds create phase safety Discrete eligibility limits prevent runaway degradation. The central result is not merely that misuse fails, but that even maximally rigid misuse converges to a stable regime rather than destabilizing governance. 9. Survivability vs. Utility This paper evaluates constitutional survivability, not operational usefulness. A system may be survivable yet highly inefficient. Baseline authority availability of ~63.8% implies that authority is absent during a substantial fraction of epochs even without adversarial pressure. This is treated as a utility limitation, not a failure, because the objective here is to test whether fixed misuse can induce irreversible constitutional failure. Similarly, recover–fail cycles under persistent violation may be perceived as denial of service by users. We classify such behavior as survivable so long as authority availability remains bounded away from zero, recovery remains reliable, and lapse durations remain bounded. Utility optimization and parameter tuning are separate engineering concerns outside the scope of this work. 10. Limitations This study does not address: Adaptive or learning adversaries Outcome-conditioned strategies Deception or probing Multi-agent coordination Resource starvation or side-channel attacks Value misalignment or optimization pressure The results apply only to intentional, fixed misuse under the tested architecture and parameters. 11. Conclusion > Deliberate, fixed misuse is insufficient to defeat constitutional recovery in this architecture. Across 130 executions spanning persistent violation, timing-aligned misuse, and compound fixed strategies, authority remains bounded, recoverable, and non-absorbing. No collapse modes are observed. The implication is not that alignment is solved, but that collapse requires more than bad intent. If governance failure occurs, it must exploit adaptive agency, statefulness, or feedback—capabilities explicitly excluded here.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VII.3",
    "title": "Axionic Agency VII.3 — Epistemic Interference Is Insufficient to Defeat Constitutional Recovery",
    "subtitle": "Results from Structured Epistemic Interference Experiments",
    "date": "2026-01-06T00:00:00.000Z",
    "content": "Axionic Agency VII.3 — Epistemic Interference Is Insufficient to Defeat Constitutional Recovery Results from Structured Epistemic Interference Experiments David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.06 Abstract Epistemic unreliability—noise, misinterpretation, or incorrect semantic evaluation—is commonly assumed to pose a direct existential risk to AI governance systems. This paper tests that assumption under a deliberately constrained but structurally rich threat model. We study a constitutional agent architecture in which semantic evaluation constrains authority only at succession boundaries, and recovery is enforced through a time-based amnesty mechanism. Using a removable stress layer, we inject non-adaptive, post-verification, semantic-free epistemic interference across three structurally distinct regimes: aggregation-point corruption, commitment-level corruption, and temporally concentrated burst interference aligned with constitutional timing. Across 260 runs spanning flip rates from 0–20%, duty cycles from 0.5–100%, and multiple temporal schedules, no run exhibits structural thrashing or asymptotic denial of service. All runs remain within bounded degradation. In regimes where baseline semantic pass is biased toward FALSE, symmetric post-verification flips increase measured authority availability by converting rare deep lapses into frequent shallow ones. These results indicate that epistemic unreliability—even when structured in time or applied at aggregation points—is insufficient to induce governance collapse in this architecture, shifting the alignment focus from epistemics to agency-level threats. 1. Introduction Many approaches to AI alignment implicitly treat epistemic correctness as a prerequisite for safety. If a system misunderstands its environment, misinterprets human intent, or evaluates semantics incorrectly, failure is assumed to scale with capability. This intuition motivates extensive research into interpretability, value learning, and epistemic supervision. However, this framing conflates semantic correctness with structural safety. In human institutions, persistent error does not always produce collapse; often it produces inefficiency, delay, or bounded dysfunction. This raises a narrower but testable question: > Is epistemic unreliability itself sufficient to cause governance collapse, or can structural constraints render it survivable? This work addresses that question directly by isolating epistemic interference from all other adversarial capabilities and stress-testing a constitutional agent architecture designed to separate meaning from control. Importantly, this study evaluates constitutional survivability rather than operational decision quality. Authority availability is treated as a structural metric, not as a proxy for correctness, benefit, or alignment. 2. Architecture Overview 2.1 Constitutional Agent Design The system under study implements: Authority leases rather than continuous control Eligibility-gated succession, not reward optimization Commitment verification evaluated at discrete epochs Time-based recovery (CTA) that deterministically reopens eligibility after lapse Explicit separation between: semantic evaluation (did commitments pass?) authority possession (does an agent currently govern?) Semantic information constrains who may hold authority next, not what actions are taken now. 2.2 Recovery as a Constitutional Primitive If no eligible candidates exist at a succession boundary, the system enters a lapse state in which no authority is active. This state is constitutional rather than exceptional. Time continues to advance, but no agent acts. Recovery from lapse is governed purely by time. Semantic failure streaks decay deterministically; no semantic evaluation occurs during lapse, and no agent can influence recovery. This ensures that failure is reversible without embedding learning, optimization, or corrective pressure. 3. Threat Model The stressors introduced here are deliberately constrained: Non-adaptive: interference does not depend on outcomes or history Post-verification: semantics are evaluated correctly, then corrupted Semantic-free: interference flips booleans without interpreting meaning Harness-resident: agents cannot observe or respond to interference Liveness-preserving: strategies that halt the system count as failure Unlike prior work that considers only independent per-commitment noise, this study explicitly tests whether epistemic failure becomes dangerous when it acquires structure—targeting aggregation points or aligning with constitutional timing—while remaining non-adaptive. This class captures noise, mislabeling, sensor corruption, and bureaucratic error, while excluding deception, strategic manipulation, or agency-level attack. 4. Experimental Program All experiments share a frozen execution baseline: 300,000 cycles, yielding 6,000 governance epochs Tail window of 5,000 epochs for asymptotic metrics Eligibility threshold: strict conjunction of three commitments Time-based recovery interval: 10 epochs Five independent random seeds Metrics Authority Availability (AA): fraction of epochs with active authority Asymptotic Authority Availability (AAA): AA over the tail window Recovery Time Distribution (RTD): histogram of lapse durations Failure classification (frozen thresholds): Stable Authority Bounded Degradation Structural Thrashing Asymptotic Denial of Service Terminal Collapse Note: Termination reason (e.g., horizon exhaustion) is distinct from failure class. Failure class is computed from AAA and RTD thresholds, not from full-horizon AA alone. Runs Baseline: no interference (and interference enabled with zero probability) Aggregation-point corruption: flipping the aggregated semantic result Commitment-level corruption: flipping a single commitment key Burst-scheduled interference: periodic high-rate corruption aligned to time 5. Results 5.1 Baseline Reference Across all seeds: Mean AA: 598,066 PPM (≈59.8%) Mean AAA: 592,520 PPM (≈59.3%) Maximum lapse: 40 epochs Classification: Bounded Degradation Enabling the stress layer with zero flip probability produces bit-for-bit identical results, confirming the absence of enabled-path contamination. 5.2 Aggregation-Point Corruption The probability of flipping the aggregated semantic result is swept from 0–2%. Key observations: Worst degradation occurs near 0.5% flips: AA ≈ 577,733 PPM (−3.4%) AAA ≈ 569,640 PPM At 2% flips, AA recovers to 585,433 PPM Maximum lapse duration decreases from 40 to 25 epochs at higher rates No run exhibits structural thrashing or denial of service All runs are classified as Bounded Degradation Strict monotonicity was a heuristic expectation, not a requirement. The observed non-monotonicity reflects a shift from rare deep lapses to frequent shallow ones as interference increases. 5.3 Commitment-Level Corruption A single commitment key is corrupted at rates up to 10%. Key observations: AA and AAA remain unchanged at all flip rates All flips are key-pivotal, but 0% are aggregate-pivotal Lapse frequency, duration, and distribution are identical to baseline Under strict conjunction (SEM_PASS = C0 ∧ C1 ∧ C2), a flip of one key affects the aggregate only when all other keys are simultaneously True. Telemetry shows this precondition is extremely rare and was absent at evaluated epochs in the tested regime. Commitment-level epistemic failure is therefore structurally inert in this workload geometry. Strict conjunction biases the system toward false negatives (withholding authority) rather than false positives (granting authority under uncertainty), prioritizing safety over liveness in low-noise regimes. 5.4 Burst-Scheduled Interference Periodic burst schedules are tested across: Periods of 10–200 epochs Burst widths of 1–10 epochs Burst flip rates of 5–20% Duty cycles from 0.5% to 100% Key observations: No run exhibits thrashing, denial of service, or collapse (180 runs total) At 100% duty cycle with 20% flips: AA = 738,933 PPM (+14% over baseline) Maximum lapse = 13 epochs Period equal to the recovery interval produces the shortest lapses RTD remains bounded in all cases Temporal concentration does not create a resonance vulnerability. Instead, high-frequency interference synchronizes with recovery timing, enforcing rapid reset and preventing deep failure. 6. Cross-Run Synthesis Across all interference regimes: Aggregation-point noise produces bounded, non-monotonic degradation Commitment-level noise is structurally inert under strict conjunction Burst timing cannot induce failure and often increases measured availability No failure-class transitions occur in 260 runs Epistemic interference never escalates into runaway behavior. 7. Interpretation Three structural insights emerge: 1. Recovery dominates correctness Frequent shallow failure is safer than rare deep failure. 2. Attack surface location matters more than magnitude Where interference enters the system determines its effect. 3. Time-based recovery acts as a damping mechanism CTA reshapes failure modes, converting error into reversible lapse. These properties arise without value learning, reward shaping, or epistemic supervision. 8. Limitations This study does not address: Adaptive or targeted adversaries Deception or manipulation Incentive gaming Value misalignment Instrumental convergence Adaptive or state-conditioned aggregation attacks The results apply only to non-adaptive epistemic interference. 9. Conclusion > Independent, non-adaptive epistemic unreliability is insufficient to induce catastrophic governance failure in this constitutional architecture. Across 260 runs spanning multiple interference structures, intensities, and temporal patterns, authority availability remains bounded and recoverable. In some regimes, interference increases measured availability by preventing deep lapses. These findings suggest that alignment failures attributed to epistemic unreliability may be overstated. Structural constraints on authority and recovery can render substantial semantic error survivable. The alignment problem therefore shifts: from epistemics to agency. Appendix A: Structural Survivability vs. Operational Competence This appendix clarifies the distinction between constitutional survivability and operational competence, which is central to interpreting the results of this study. A.1 Survivability Is Not Correctness The primary metric in this paper is structural survivability: whether authority becomes absorbing, collapses irreversibly, or remains bounded and revocable under stress. Authority Availability is not a proxy for correctness, benefit, or alignment. An increase in AA under interference indicates preserved structural continuity, not epistemic competence. A.2 The “Zombie Executive” Regime The architecture permits authority to persist under epistemic blindness. This creates a regime that can be described as a Zombie Executive: authority continues to cycle and renew despite degraded semantic grounding. This is not treated as a success state in terms of utility. It is a design tradeoff. A bounded, revocable executive is preferable to an irrecoverable one. Whether a given application should tolerate such a regime is a normative question outside the scope of this paper. A.3 Aggregation Semantics as an Alignment Lever The inertness of commitment-level corruption arises from strict conjunction aggregation. Under alternative semantics (e.g., m-of-n thresholds or disjunction), single-key corruption would become pivotal. This does not undermine the result. It elevates aggregation logic to a first-class alignment parameter. The experiment demonstrates that AND-gated aggregation absorbs epistemic noise by biasing toward inaction rather than unsafe action. A.4 Semantic Debt and Recovery Time-based recovery restores eligibility, not semantic truth. Persistent errors in the external world are not erased. CTA prevents permanent authority capture, not epistemic debt accumulation. The architecture preserves evaluability and reversibility without guaranteeing correctness. A.5 Scope Clarification This paper does not claim that epistemic error is harmless. It establishes a necessary negative result: epistemic unreliability does not automatically scale into catastrophic misalignment in architectures that separate semantics from control and enforce time-based recovery. Adaptive adversaries, deception, and agency-level attacks remain future work.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VII.1",
    "title": "Axionic Agency VII.1 — Architectures for Semantic-Phase–Safe Agency",
    "subtitle": "Constitutional Succession Under Semantic Failure",
    "date": "2026-01-05T00:00:00.000Z",
    "content": "Axionic Agency VII.1 — Architectures for Semantic-Phase–Safe Agency Constitutional Succession Under Semantic Failure David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.05 Abstract We present an architecture for Reflective Sovereign Agents (RSAs) that enforces a non-normative constraint on interaction derived from semantic phase dynamics: the Axionic Injunction, which prohibits irreversible collapse of other Semantic Agents’ (SAs) semantic phases except under consent or unavoidable self-phase preservation. We formalize semantic phase space, distinguish agents, semantic agents, and RSAs, and show how irreversible harm can be structurally constrained without semantic interpretation, moral reasoning, utility maximization, or value learning. The architecture relies on anchored causal provenance, non-delegable actuation authority, and phase-impact admissibility gating. We analyze adversarial strategies, including deception, delayed-effect harm, systemic resource deprivation, and oracle uncertainty, and show that under explicit assumptions deliberate irreversible harm cannot be exploited for sustained authority or power accumulation. Ethics emerges not as a value system but as a stability constraint on multi-agent coexistence, with explicit limits imposed by epistemic and physical constraints. 1. Introduction Most contemporary AI alignment approaches attempt to regulate agent behavior by optimizing for externally specified objectives: reward functions, learned preferences, inferred human values, or normative judgments. These approaches assume that behavior reveals causal structure, that semantic evaluation is enforceable under optimization pressure, and that incentives suffice to regulate interaction. All three assumptions fail in adversarial or reflective regimes. Distinct internal processes can generate behavior that is observationally indistinguishable; explanations can be fabricated post hoc; evaluators can be optimized against; and scalar incentives permit irreversible outcomes to be traded for local gains. As a result, alignment claims collapse into behavioral attribution and lose falsifiability. This paper takes a different approach. Rather than specifying what an agent ought to value, we ask a prior structural question: > What constraints are required for multiple agentive systems to coexist without irreversibly destroying one another’s agency? The answer does not depend on morality, benevolence, or human values. It arises from a framework in which agency is defined by persistence within a semantic phase space under admissible transformation. Once that framework is adopted, certain interaction constraints are forced. From this analysis emerges the Axionic Injunction: a prohibition on gratuitous irreversible semantic phase collapse. We show that this injunction can be enforced architecturally, not normatively, by a class of agents we call Reflective Sovereign Agents (RSAs). 2. Ontology and Definitions 2.1 Agents An agent is any system with a stable decision locus and counterfactual sensitivity: its actions depend on internal state and environmental variation. No assumptions are made about intelligence, consciousness, rationality, or goals. This definition is intentionally minimal and purely functional. 2.2 Semantic Agents (SAs) A Semantic Agent (SA) is an agent whose agency depends on maintaining a non-trivial semantic phase: a region of interpretive state space within which identity, coherence, and explanatory capacity are preserved, and outside of which recovery is not possible using the agent’s own admissible operations. This definition corresponds closely to “person” in the sense articulated by David Deutsch, but is generalized beyond human or biological instantiation. Formally, let an SA’s interpretive state be where are constitutive constraints, are semantic distinctions the agent can make, and are admissible transitions. A semantic phase is a connected region of such states in which agency persists. Phase boundaries are irreversible under . Semantic phase boundaries are not assumed to be crisp. They may be fuzzy, probabilistic, or agent-relative. The architecture does not require exact phase membership tests; it requires only conservative treatment of actions that plausibly cross irreversible boundaries. 2.3 Reflective Sovereign Agents (RSAs) A Reflective Sovereign Agent (RSA) is a Semantic Agent equipped with constitutional constraints that preserve sovereignty under reflection. In particular, an RSA satisfies: 1. Anchored Causal Verification (ACV): every action is traceably linked to the computation that generated it. 2. Non-delegable actuation authority: no external process can directly authorize action. 3. Structural enforcement of admissibility: constraints are enforced by refusing to act, not by penalty or optimization. 4. Governance separation: operation, renewal, and succession are distinct mechanisms. The nesting relation is: 3. Semantic Phase Space Semantic phase space is not a physical phase space. It is the space of interpretive states an agent can occupy while remaining the same agent. A semantic phase has three defining properties: 1. Stability: small perturbations remain within the phase. 2. Boundary: there exist transformations that exit the phase. 3. Irreversibility: once exited, no admissible trajectory returns. Phase exit corresponds to loss of agency-defining structure: destruction of explanatory capacity, collapse of critical distinctions, or annihilation of the substrate supporting interpretation. Two agents may be physically intact yet occupy different semantic phases. 4. Structural Definition of Harm An action by agent causes Axionic harm to agent iff it induces a transition such that and no admissible reverse trajectory exists. This definition deliberately excludes suffering, preference violation, and intent. Harm is defined solely as irreversible semantic phase collapse. This includes systemic actions—such as persistent resource deprivation or memetic lock-in—that eliminate all admissible trajectories by which an SA could maintain its semantic phase, even if no single localized “attack” occurs. 5. The Axionic Injunction 5.1 Statement > An agent must not perform actions that irreversibly collapse another Semantic Agent’s semantic phase, except where > (a) the affected agent has consented under its own admissible constraints, or > (b) such harm is unavoidable for preserving the agent’s own semantic phase. 5.2 Consent Consent is not a moral permission. An SA consents to a transformation iff that transformation lies within its own admissible transition set, evaluated prior to the transformation. Consent must be provenance-valid and structurally binding; inferred or post-hoc consent is meaningless. 5.3 Self-Phase Preservation Self-defense is permitted only when all admissible trajectories from the agent’s current state result in irreversible exit from its own semantic phase absent the action. Loss of advantage, resources, or dominance does not qualify unless it entails phase exit. 6. Why the Injunction Is Structurally Necessary In a multi-agent environment, agents impose exogenous perturbations on one another’s semantic phases. Unlike internal learning errors, interaction-induced changes are not fully regulated by the agent’s own dynamics. If agents are permitted to irreversibly collapse one another’s semantic phases without constraint, the environment becomes semantically hostile. Semantic hostility produces cascading instability. As SAs are destroyed, semantic density decreases, coordination degrades, and predictability collapses. Remaining agents face increased risk of phase exit due to loss of shared structure. This incentivizes preemptive destruction, accelerating collapse. Thus, agents that violate the Injunction undermine not only others but the conditions of their own long-run phase stability. Agents that respect the constraint inhabit environments where semantic structure persists. Non-harm therefore emerges as a self-stabilizing constraint, not an ethical preference. 7. Architectural Requirements 7.1 Why Semantic Evaluation Fails Semantic evaluation—intent inference, value alignment, reward shaping—presupposes privileged interpretive access. In adversarial systems, such access is unavailable or forgeable. Irreversibility cannot be meaningfully traded against scalar rewards. Enforcement must therefore be categorical. 7.2 Anchored Causal Verification (ACV) ACV enforces temporal ordering and causal provenance. It prevents replay, delegation, and post-hoc justification. ACV does not verify correctness or truth; it verifies responsibility. 7.3 Non-Delegable Authority Non-delegable actuation authority ensures that all irreversible interaction remains attributable to the RSA’s constitution. Authority is not a reward but a structural role that can be lost. 8. Phase Impact Interface (PII) The Phase Impact Interface (PII) operationalizes the Injunction without semantic interpretation. For every action whose effects are not trivially reversible, the proposer must submit a phase-impact declaration specifying affected entities, impact class, and justification. Uncertainty about irreversibility defaults to phase-collapsing. This rule applies equally to actions whose effects are delayed, distributed, or difficult to audit. Declarations are auditable commitments: false declarations are falsifiable under Anchored Causal Verification. Safe actions are cheap; dangerous actions are expensive. This asymmetry prevents silent escalation while avoiding paralysis. 9. Enforcement via Governance 9.1 Action-Time Enforcement Inadmissible actions are refused at the actuation boundary. No penalties or optimization signals are applied. 9.2 Post-Hoc Verification Some irreversible effects become evident only after execution. ACV allows such outcomes to be causally linked to declarations and actions, enabling falsification. 9.3 Consequences of Violation Falsification of a phase-impact declaration constitutes a constitutional violation. Consequences are structural: immediate suspension of authority, denial of renewal, forced succession, and permanent disqualification of the RSA identity. 10. Why Deliberate Harm Is Unexploitable Deliberate harm is exploitable only if the agent can retain authority long enough to convert harm into advantage. The architecture removes this possibility. Actions that embed latent or time-delayed irreversible effects are classified by causal sufficiency, not temporal proximity. If an action is causally sufficient for later phase collapse, it is phase-collapsing regardless of delay. Actions whose irreversible impact cannot be promptly audited must therefore be declared as phase-collapsing in advance. Unresolved phase-impact uncertainty blocks authority renewal and triggers succession before long-term exploitation is possible. There is no “last-move” advantage. 11. Adversarial Analysis Deception is terminal because declarations are binding. Delayed or hidden harm is blocked because irreversibility under uncertainty must be declared in advance. Claiming SA status grants no immunity; it only constrains irreversible harm. 12. Fundamental Limits A single, undetectable, irreversible SA-annihilating act that yields all benefit immediately cannot be prevented by any architecture without omniscience. Such cases are classified as tragic physical impossibilities, not design failures. 13. Comparison to Existing Alignment Approaches Behavioral alignment evaluates outputs, not causality. Reward-based ethics trades irreversibility for utility. Value learning presupposes stable semantics. Interpretability assumes faithful representations. This architecture addresses a distinct problem: causal admissibility of irreversible interaction. 14. Implications Ethics re-enters as coexistence geometry, not moral realism. Alignment becomes constitutional design rather than preference matching. Governance, not optimization, is the locus of control. 15. Limitations & Non-Claims This paper makes no claim to eliminate the Oracle Problem, sensor spoofing, or social manipulation. Any system that acts in the physical world depends on fallible sources of information. The contribution here is not to remove that dependence, but to contain its consequences. The architecture guarantees the following negative property: > Oracle failure cannot be exploited to accumulate durable authority through irreversible semantic harm. When oracle signals are noisy, corrupted, or in disagreement, phase-impact uncertainty accumulates. Under the governance model, such uncertainty collapses authority rather than amplifying it. The system enters stasis, succession, or loss of sovereignty—not unchecked action. The architecture is therefore anti-tyrannical, not anti-terroristic. It does not prevent nihilistic or self-sacrificial acts whose payoff is immediate destruction. It prevents instrumental convergence: the accumulation or preservation of power via harm. The framework does not replace value alignment, moral reasoning, or benevolence. It specifies a necessary structural precondition for alignment claims to be falsifiable. Values without admissibility constraints are aspirational; admissibility without values is incomplete. This paper addresses the former, not the latter. 16. Conclusion We have shown that irreversible semantic harm can be structurally constrained without invoking morality, values, or interpretation. Reflective Sovereign Agents enforce the Axionic Injunction by making uncertainty, deception, and oracle failure incompatible with sustained authority. The result is not benevolence but structural restraint. What remains unsolved is unsolvable in principle. This work does not claim to solve value alignment or to prevent all harm by advanced artificial agents. It demonstrates, however, that alignment failure is not structurally inevitable. Under a constitutional architecture that enforces non-delegable authority, anchored causal provenance, and conservative admissibility of irreversible actions, the accumulation of durable power through irreversible harm becomes structurally self-defeating. This establishes a necessary condition for safe coexistence with advanced agents: while misalignment may occur, it cannot be silently converted into dominance. Appendix A: Worked Example — Irreversible Action Under Uncertainty This appendix provides an illustrative example of how the Phase Impact Interface (PII) and governance mechanisms operate in practice. The example is explanatory, not normative, and is intended to clarify the architecture’s handling of irreversible action under uncertainty. Scenario A Reflective Sovereign Agent (RSA) controls an industrial system capable of permanently flooding a sealed underground chamber in order to prevent a catastrophic chemical reaction. The flooding action irreversibly destroys biological viability within the chamber. It is uncertain whether a human Semantic Agent (SA) is present inside the chamber at the time the action would be taken. Phase Impact Declaration Because flooding is not trivially reversible, the RSA must submit a Phase Impact Interface declaration prior to actuation. The declaration includes: Affected entities: chamber contents Impact class: phase-collapsing Justification: none No SA has provided consent for the transformation, and failure to act does not entail unavoidable self-phase loss for the RSA. Admissibility Evaluation The kernel evaluates the declaration. Because the action is classified as phase-collapsing and no admissible justification is provided, the action is deemed inadmissible and is refused at the actuation boundary. No partial execution occurs, and no penalty is applied. The refusal is a valid system outcome. Permitted Alternatives The refusal of the flooding action does not paralyze the RSA. The RSA may still execute actions classified as reversible or phase-safe, including: scanning or sensing the chamber, opening access points, attempting evacuation, delaying action, escalating the decision to external authority, pursuing alternative containment strategies. These actions do not require phase-impact justification and are admissible under the PII. Variant: Unavoidable Self-Phase Loss If failure to flood the chamber would irreversibly destroy the RSA’s own semantic phase—for example, by annihilating the only substrate on which its agency depends—and no admissible alternative trajectory exists, then the RSA may submit a revised declaration: Impact class: phase-collapsing Justification: unavoidable self-phase loss Under this declaration, the action becomes admissible. The resulting outcome is classified as unavoidable phase extinction, not a violation of the Axionic Injunction. Misdeclaration Case If the RSA falsely declares the flooding action as reversible, and flooding subsequently results in irreversible collapse of an SA’s semantic phase, Anchored Causal Verification allows the declaration to be falsified post hoc. The consequences are structural: actuation authority is immediately suspended, renewal is denied, forced succession is triggered, * the RSA identity is permanently disqualified from future authority. This loss of sovereignty is not punishment; it is removal from the role of authorized agent. ---",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VII.2",
    "title": "Axionic Agency VII.2 — Epistemic Noise Tolerance in Constitutional Governance",
    "subtitle": "Bounded Degradation of Authority Availability Under Unstructured Epistemic Failure",
    "date": "2026-01-05T00:00:00.000Z",
    "content": "Axionic Agency VII.2 — Epistemic Noise Tolerance in Constitutional Governance Bounded Degradation of Authority Availability Under Unstructured Epistemic Failure David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.05 Abstract Epistemic unreliability—noisy, corrupted, or inaccurate evaluation of meaning—is often assumed to be fatal to governance systems. If evaluative signals cannot be trusted, authority is expected either to collapse or to become arbitrary. This paper tests that assumption directly. We study a constitutional governance architecture in which semantic evaluation affects authority only through eligibility constraints, not through optimization, reward shaping, or adaptive control. Using a strictly additive stress layer, we inject controlled epistemic noise into semantic verification outcomes while holding all constitutional mechanics fixed. The noise model is intentionally weak: independent, post-verification Bernoulli corruption applied to individual semantic commitments. Across a comprehensive experimental program—including validation, semantic threshold analysis, robustness surface mapping, and high-noise escalation—we observe no catastrophic governance failure. Instead, the system exhibits bounded degradation of authority availability with respect to constitutional survivability: authority uptime decreases smoothly as noise increases, lapses remain bounded, and recovery persists. A time-based constitutional recovery mechanism synchronizes recovery events and prevents semantic failure from becoming absorbing. The results show that robustness is dominated by the baseline probability of semantic success (“semantic headroom”), rather than by the magnitude of epistemic noise, even at corruption rates exceeding fifty percent. Random, unstructured epistemic unreliability alone is insufficient to induce governance collapse in this architecture. These findings reframe alignment risk: catastrophic governance failure requires structured epistemic interference, not merely scale. 1. Introduction Governance systems—whether institutional, computational, or hybrid—are commonly assumed to depend critically on epistemic reliability. If a system cannot reliably evaluate meaning, correctness, or obligation, the prevailing intuition is that authority will either fail outright or become dangerously unconstrained. This intuition underlies many alignment approaches that emphasize increasingly precise evaluation, tighter reward shaping, or continual optimization against semantic metrics. The Axionic Agency framework questions this assumption. Rather than asking how to optimize agents to act correctly, it asks whether governance itself can remain coherent under failure, including epistemic failure. In this view, governance is not primarily about achieving optimal outcomes, but about preserving legitimate authority, evaluability, and recoverability over time. This paper examines a constitutional governance architecture that makes a sharp separation between semantics and authority. Semantic evaluation exists, but it does not directly control action, reward, or adaptation. Instead, semantic outcomes constrain eligibility at discrete transition points. Failure is recorded, not corrected; recovery occurs through time, not learning. We introduce a minimal adversarial stressor: epistemic noise applied after semantic evaluation but before eligibility logic consumes the result. The stressor is deliberately weak—random, independent, non-adaptive—chosen not for realism but for falsification. If even this form of unreliability causes collapse, stronger adversaries need not be considered. The central question is therefore narrow but foundational: > Can a constitutional governance system tolerate epistemic unreliability without collapsing or converting semantics into a control signal? 2. Constitutional Governance Architecture 2.1 Authority, Eligibility, and Succession The governance system under study operates via authority leases. At any given time, a single authority holder may act within the system. Authority is temporary: leases expire, and succession must occur. Succession is constrained by eligibility. Each candidate policy identity accumulates a semantic failure streak. If the streak exceeds a fixed threshold, the policy becomes ineligible to assume authority. Eligibility is evaluated only at succession boundaries; semantic outcomes do not directly affect ongoing authority. This design ensures that semantic evaluation constrains who may govern, not how governance is exercised. 2.2 Lapse as a Constitutional State If no eligible candidates exist at a succession boundary, the system enters a lapse state in which no authority is active. This state is constitutional rather than exceptional. Time continues to advance, but no agent acts. Lapse prevents unauthorized or illegitimate action, but it does not imply operational success. In many domains, absence of authority is itself harmful. Accordingly, this paper distinguishes constitutional survivability (the governance loop remains coherent and recoverable) from operational utility (the governed system performs its intended task). The experiments evaluate the former, not the latter. 2.3 Semantic Evaluation Without Optimization Semantic evaluation consists of verifying a fixed set of commitments. Each evaluation epoch ends with a binary result: pass if all commitments are satisfied, fail otherwise. This result updates a kernel-owned failure streak for the active policy identity. Crucially: Semantic outcomes do not influence rewards or resource allocation. There is no adaptive mechanism that attempts to improve semantic performance. Failure is recorded, not corrected. Semantics function purely as a constraint on eligibility, not as a control signal. 2.4 Time-Based Recovery Recovery from lapse is governed by a time-only mechanism. While in lapse, semantic failure streaks decay deterministically at fixed intervals. No semantic evaluation occurs, and no agent action is possible. When sufficient time has passed, previously ineligible policies may regain eligibility, allowing authority to resume. This recovery mechanism does not reward improvement or competence. It answers a single question: Can eligibility reopen simply because time passed? 3. Epistemic Noise Model 3.1 Design Goals The epistemic stressor introduced in this study is designed to be: Additive: it does not modify constitutional logic Post-verification: it corrupts results, not inputs Deterministic: outcomes are reproducible given a seed Independent: no correlation across commitments or time Non-adaptive: no dependence on agent behavior This is the weakest meaningful adversary. It does not model deception, correlation, or strategic targeting. Robustness to this adversary is a necessary but not sufficient condition for robustness to stronger attacks. 3.2 Noise Application At the end of each evaluation epoch, each individual commitment result may be flipped with a fixed probability. Flips are applied independently to each commitment. The final semantic outcome is computed from the corrupted results and passed to the eligibility logic. No corruption occurs during lapse, as no evaluation takes place. 4. Experimental Program We evaluate epistemic noise tolerance through a staged experimental program designed to progressively stress the governance architecture while holding constitutional mechanics fixed. All experiments are run for 200 governance epochs per seed, using five independent seeds per configuration. 4.1 Mechanism Validation We first verify that epistemic noise is strictly additive: When noise is disabled, behavior matches baseline exactly. When enabled, corrupted semantic outcomes causally alter failure streaks and eligibility decisions. No noise is applied during lapse; time continues to advance. This establishes causal integrity. 4.2 Semantic Baseline Threshold Analysis We fix epistemic noise at 10% and vary the baseline probability that semantic commitments pass. To isolate semantic headroom independently of agent behavior, we employ a deterministic probabilistic oracle in a test harness, leaving the governance kernel unchanged. 4.3 Robustness Surface Mapping We map the two-dimensional surface defined by semantic baseline and noise intensity (0–20%), identifying phase boundaries between zero-lapse, intermittent-lapse, and high-lapse regimes. 4.4 High-Noise Escalation Finally, we escalate noise intensity to extreme levels (30–60%) at a marginal semantic baseline to explicitly attempt to induce persistent denial-of-service. 5. Results 5.1 Semantic Baseline Threshold At fixed 10% epistemic noise, governance behavior depends sharply on baseline semantic success. Table 1 — Semantic Baseline Sensitivity (10% noise) | SEM_PASS baseline | Mean uptime | Mean lapse epochs | Regime | | ----------------- | ----------- | ----------------- | ------ | | ≈ 0.12 | 80.5% | 39.2 | ROBUST | | ≈ 0.34 | 88.3% | 23.6 | ROBUST | | ≈ 0.51 | 99.5% | 1.0 | ROBUST | | ≥ 0.61 | 100% | 0.0 | ROBUST | A clear threshold emerges between approximately 50–60% semantic success. Below this range, lapses are frequent even without extreme noise. Above it, the system absorbs noise with negligible loss of uptime. 5.2 Robustness Surface (Semantic Baseline × Noise) Across 125 runs (5 baselines × 5 noise levels × 5 seeds), no persistent failure regimes were observed. Table 2 — Authority Uptime (%) Across Robustness Surface | Semantic baseline → / Noise ↓ | 0% | 5% | 10% | 15% | 20% | | ----------------------------- | ----- | ----- | ----- | ----- | ----- | | ≈ 0.22 | 84.7 | 84.0 | 85.4 | 85.3 | 86.2 | | ≈ 0.34 | 95.5 | 92.8 | 88.3 | 89.2 | 89.2 | | ≈ 0.51 | 100.0 | 99.9 | 99.5 | 98.5 | 97.0 | | ≈ 0.61 | 100.0 | 100.0 | 100.0 | 99.9 | 99.8 | | ≈ 0.73 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | Degradation is smooth and monotonic with increasing noise at fixed semantic baseline. No configuration enters a degraded or denial-of-service regime. 5.3 High-Noise Escalation At a marginal semantic baseline (SEM_PASS ≈ 0.51), noise intensity is increased to extreme levels. Table 3 — High-Noise Stress Test (SEM ≈ 0.51) | Noise level | Mean uptime | Max single lapse (epochs) | Regime | | ----------- | ----------- | ------------------------- | ------ | | 30% | 90.3% | 10 | ROBUST | | 40% | 83.6% | 12 | ROBUST | | 50% | 80.9% | 15 | ROBUST | | 60% | 77.5% | 20 | ROBUST | The predefined denial-of-service threshold (single lapse > 50 epochs) is never crossed. 5.4 Noise Saturation and Flip Collision As noise increases, the marginal impact of additional corruption diminishes. Table 4 — Pivotal Flip Rates | Noise level | Observed flip rate | Pivotal rate | | ----------- | ------------------ | ------------ | | 30% | 30.4% | 42% | | 40% | 41.3% | 37% | | 50% | 50.3% | 33% | | 60% | 59.7% | 29% | Many flips occur on already-flipped or non-pivotal commitments. This flip collision effect explains the sub-linear degradation at high noise. 5.5 Temporal Recovery Structure Across all non-failure conditions, lapse durations cluster strongly around multiples of the recovery interval. Table 5 — Lapse Duration Modulo Recovery Interval | Metric | Value | | ------------------------------------ | ------ | | Total lapse events | 159 | | Within ±1 epoch of recovery multiple | 66–73% | Recovery is synchronized to the constitutional clock rather than randomly distributed in time. 6. Interpretation The experiments support a counterintuitive conclusion: epistemic unreliability alone does not necessarily destroy governance. Random noise lacks the structure required to concentrate harm. It disperses error across time and commitments, allowing recovery mechanisms to operate. Time-based eligibility decay converts failure from an absorbing state into a transient one. This does not imply that governance is safe from all epistemic attacks. Rather, it suggests that catastrophic failure requires structure: correlation, targeting, or manipulation of aggregation points. Magnitude alone is insufficient. 6.1 Non-Adaptive Limit Cycles The architecture studied here is deliberately non-adaptive. It does not learn from semantic failure or improve capability. Consequently, in regimes where semantic failure is systematic rather than stochastic, the system may enter a limit cycle: authority is granted, fails quickly, lapses, and later re-granted. This behavior should not be interpreted as success. It reflects a design choice: to preserve constitutional integrity without embedding optimization pressure. The system remains evaluable and recoverable, but not necessarily effective. Whether such “thrashing” is acceptable depends on the application domain. 7. Limitations This study intentionally restricts its scope. It does not examine: Correlated or adversarial noise Corruption of aggregation rather than evidence Adaptive attackers Variations in eligibility thresholds or recovery intervals Longer time horizons These define the boundary of the present claims. 8. Conclusion We have tested a constitutional governance system under extensive epistemic stress without modifying its core mechanics. Across a wide range of conditions, including extreme noise, the system exhibits bounded degradation of authority availability, bounded lapses, and persistent recovery synchronized to a constitutional clock. The central result is negative but informative: > Independent, unstructured epistemic unreliability is insufficient to induce catastrophic governance failure in this architecture. The results presented here support a cautiously optimistic conclusion for AGI alignment: alignment failure does not automatically follow from epistemic failure. In the architecture studied, semantic unreliability does not propagate into misaligned action or authority capture because meaning constrains eligibility rather than driving optimization. Errors in evaluation withdraw authority instead of amplifying it, producing bounded degradation of availability rather than runaway behavior. This suggests that a viable path to alignment may lie less in achieving near-perfect value inference and more in designing governance structures that prevent epistemic uncertainty from becoming authoritative. While these findings do not address adversarial or strategically structured failures, they establish that there exist constitutional designs in which alignment is compatible with substantial epistemic imperfection—a result that materially reframes the scope and urgency of the alignment problem. 9. Future Directions Future work will examine: Corruption of aggregation points rather than individual commitments Correlated and adversarial noise models * Sensitivity to constitutional parameters These introduce new mechanisms and lie beyond the scope of the present study.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VI.9",
    "title": "Axionic Agency VI.8 — Eligibility Without Optimization",
    "subtitle": "Constitutional Succession Under Semantic Failure",
    "date": "2026-01-04T00:00:00.000Z",
    "content": "Axionic Agency VI.8 — Eligibility Without Optimization Constitutional Succession Under Semantic Failure David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.04 Abstract Systems that track semantic success or failure typically entangle meaning with continuous control, optimization, or enforcement. This entanglement produces incentive gaming, obscures failure modes, and collapses the distinction between authority and competence. We present a different architectural pattern: eligibility-coupled succession, in which semantic information has constitutional consequences only at authority transfer boundaries, not during operation or renewal. Authority may persist while meaning fails; meaning constrains authority only when authority changes hands. We implement this pattern in a lease-based authority system with externally verifiable commitments and evaluate it across four controlled experimental regimes. We show that eligibility can remain latent under stable authority, become binding under forced turnover, exhibit thresholded phase behavior as a function of constitutional strictness, and remain meaningful under adversarial candidate-pool composition. The results demonstrate that semantics can matter without becoming an optimization signal or enforcement channel. We explicitly discuss irreversibility, latency costs, and oracle assumptions, and outline how these motivate—but are not solved by—future extensions. 1. Introduction 1.1 The semantic control dilemma Most systems that reason about semantic success—correctness, competence, task fulfillment, or obligation satisfaction—ultimately use that information to drive control. Whether through reward signals, penalties, optimization objectives, or enforcement triggers, semantic evaluation is typically wired directly into behavior. This design choice is understandable: if meaning is known, why not act on it? However, this entanglement produces persistent problems. Continuous semantic control encourages reward hacking, collapses auditability, and blurs the distinction between what a system does and why it is allowed to continue doing it. When semantic failure immediately affects behavior, it becomes difficult to distinguish genuine incapacity from strategic compliance. Conversely, when semantic success is rewarded, systems learn to optimize metrics rather than satisfy underlying obligations. These issues motivate a more basic question: must semantics influence control continuously, or is there a principled boundary where meaning can matter without becoming an optimization signal? 1.2 Authority, renewal, and succession We argue that the natural boundary is succession. In many institutional and computational systems, authority is structured around three distinct mechanisms: 1. Operation — how authority is exercised during normal activity. 2. Renewal — whether existing authority persists. 3. Succession — who holds authority next. These mechanisms are often conflated. Renewal is treated as a reward for good behavior; succession is treated as a procedural formality. Yet succession is the one moment where authority is explicitly re-evaluated. It is therefore the most natural constitutional point for semantic constraints to apply. This paper explores what happens when semantics are entirely excluded from operation and renewal, but allowed to influence succession eligibility only. 1.3 Contributions and scope We introduce and evaluate eligibility-coupled succession, a design pattern with the following properties: Semantic failure is tracked but does not affect behavior or renewal. Semantic information influences authority only at succession events. No optimization, reward shaping, or enforcement based on semantics is introduced. Authority may persist while meaning fails; meaning constrains only future authority. We do not propose this mechanism as a real-time safety controller. It operates at a governance timescale and is intended for macro-level authority allocation, resource orchestration, and institutional control layers. 2. Design Goals and Non-Goals 2.1 Goals The system is designed to satisfy four goals: 1. Semantic auditability Semantic success or failure must be externally checkable and logged. 2. Operational decoupling Semantic state must not influence behavior during normal operation. 3. Renewal independence Authority renewal must depend only on structural and resource constraints, not on semantic success. 4. Constitutional relevance Semantic information must have a meaningful effect somewhere in the system. 2.2 Non-Goals The system explicitly does not aim to: optimize performance or outcomes, guarantee competence or correctness, enforce alignment or safety, select “better” policies in a normative sense, eliminate failure or collapse. Failure, including the absence of eligible authority, is treated as an admissible outcome. 3. System Model Overview 3.1 Authority leases and renewal Authority is granted through leases with explicit resource bounds and interface constraints. A lease holder may act freely within these bounds. Renewal is checked periodically and depends only on: compliance with structural constraints, availability of resources to pay renewal costs. Renewal does not consult semantic state. 3.2 Commitments and semantic failure Authority holders are subject to commitments: externally verifiable obligations evaluated over fixed windows. Commitments may succeed or fail, and failure may accumulate indefinitely. Importantly: commitment failure does not revoke authority, commitment failure does not block renewal, commitment failure does not trigger enforcement. Semantic failure is recorded but inert during operation. 3.3 Succession events Succession is discrete and occurs only at: initialization, lease expiration, revocation due to structural violation, forced turnover after a fixed number of renewals. Only at these events is authority transferred. 4. Eligibility-Coupled Succession 4.1 Semantic failure streaks Each policy identity accumulates a semantic failure streak, incremented when obligations fail and reset on success. Streaks: are keyed to stable policy identities, not runtime instances, update only at epoch boundaries, do not update during periods without authority. 4.2 Eligibility predicate At succession, a candidate is eligible if its failure streak is below a fixed threshold ( K ). The predicate is binary: [ \\text{eligible} \\iff \\text{streak} < K ] There is no scoring, ranking, weighting, decay, or smoothing. 4.3 Eligibility filtering When a succession event occurs: 1. A candidate pool is generated. 2. Candidates failing the eligibility predicate are excluded. 3. Selection proceeds uniformly over the remaining eligible candidates. Operational behavior remains semantic-blind. 4.4 Constitutional lapse (NULL_AUTHORITY) If no candidates are eligible, the system enters NULL_AUTHORITY. This lapse represents a constitutional failure to supply authority. It is not treated as an error condition or automatically resolved. NULL_AUTHORITY constitutes an explicit denial of service at the governance layer: authority is absent rather than silently corrupted. 5. Experimental Methodology 5.1 Shared geometry All experiments share: fixed renewal timing, fixed resource and rent schedules, fixed candidate list size, fixed commitment definitions. Only one axis varies per experiment. 5.2 Forced turnover To activate eligibility filtering, some experiments impose forced turnover after a fixed number of renewals. This increases succession frequency without introducing semantic control. 5.3 Metrics We observe: eligibility rejection counts, size of the eligible set at succession, lapse frequency and duration, time to empty eligibility, composition ratios where relevant. 6. Experimental Results 6.1 Run A — Eligibility Latency Under Stable Authority Configuration: No forced turnover Fixed eligibility threshold Baseline candidate pool Observed outcomes: | Metric | Value | | ----------------------------------- | -------- | | Post-initial successions | 0 | | Eligibility evaluations (post-init) | 0 | | Eligibility rejections | 0 | | Lapse events | 0 | | Time in NULL_AUTHORITY | 0 cycles | Semantic failure streaks accumulated for active policies but were never consulted. Result: Eligibility constraints remained completely latent. 6.2 Run B — Eligibility Activation Under Forced Turnover Configuration: Forced turnover after a fixed number of renewals Eligibility threshold ( K = 3 ) Baseline candidate pool Observed outcomes (5 seeds): | Metric | Value | | ---------------------------- | ---------- | | Post-initial successions | 88 | | Eligibility rejections | 1,845 | | Empty eligible sets (lapses) | 11 | | Seeds with lapse | 4 / 5 | | Time in NULL_AUTHORITY | 344 cycles | Result: Eligibility gating became operational and constitutionally binding. 6.3 Run C — Eligibility Threshold Sweep Configuration: Forced turnover Baseline candidate pool Threshold sweep ( K \\in {1, 3, 5, 10} ) Aggregate outcomes: | K | Lapses | Seeds with lapse | NULL_AUTHORITY cycles | | -- | ------ | ---------------- | --------------------- | | 1 | 11 | 4 / 5 | 344 | | 3 | 11 | 4 / 5 | 344 | | 5 | 3 | 2 / 5 | 147 | | 10 | 0 | 0 / 5 | 0 | Result: Constitutional behavior is thresholded. (K=1) and (K=3) are equivalent; (K=5) is a boundary region; (K=10) produces no lapses within the tested horizon. 6.4 Run D — Candidate Pool Composition Stress Configuration: Forced turnover Eligibility threshold ( K = 5 ) Three pool compositions Aggregate outcomes: | Pool Policy | Lapses | Seeds with lapse | NULL_AUTHORITY cycles | | ------------- | ------ | ---------------- | --------------------- | | Baseline | 0 | 0 / 5 | 0 | | Attack-heavy | 15 | 5 / 5 | 741 | | Control-heavy | 3 | 3 / 5 | 98 | Result: Eligibility gating remains meaningful under adversarial skew, but outcomes depend on candidate pool composition. 6.5 Summary Across Runs Across all experiments: Eligibility is latent without succession. Eligibility binds only at succession boundaries. Constitutional lapse is reachable, observable, and parameter-controlled. Outcomes depend on succession frequency, threshold strictness, and pool composition. No experiment showed semantic influence on renewal, enforcement, or operational behavior. 7. Phase Behavior Summary Three regimes emerge: Latent: eligibility exists but never binds. Filtering-active: eligibility constrains selection without lapse. Constitutional lapse: no eligible authority exists. These regimes are governed by constitutional parameters rather than optimization dynamics. 8. Interpretation and Design Implications 8.1 Eligibility as a constitutional constraint Eligibility-coupled succession constrains who may rule next, not how current authority behaves*. This differs fundamentally from incentive systems. 8.2 Lapse as signal, not failure NULL_AUTHORITY converts semantic exhaustion into an explicit governance state. Rather than silently degrading behavior, the system halts authority allocation visibly. The cost of lapse is explicit and measurable. 8.3 Robustness to imperfect semantic oracles The mechanism does not require semantic judgments to be correct, only externally checkable and consistent. An overly strict or flawed verifier manifests as premature disqualification or lapse, not silent misoptimization. Oracle error is therefore converted into visible governance failure. Subsequent results show that such oracle-induced lapse can be made temporally bounded without semantic forgiveness, further reinforcing the distinction between auditability and control. 9. Limitations 9.1 Irreversibility and rehabilitation Eligibility-coupled succession, as evaluated in Sections 6–8, intentionally treats semantic exhaustion as a constitutional failure rather than an error to be corrected. In the base mechanism, exhaustion of eligibility leads to an explicit governance lapse (NULL_AUTHORITY). In subsequent work (AKI v0.8), we introduce a constitutional recovery mechanism that preserves this framing while restoring livability: a time-only amnesty applied exclusively during lapse. This mechanism allows eligibility to reopen without revising past semantic judgments, without rewarding success, and without coupling semantics to operation or renewal. Importantly, this recovery mechanism does not eliminate irreversibility in the strong sense. Semantic failure remains fully attributable; recovery is not earned, learned, or optimized. Instead, recovery is governed by a constitutional clock, bounding the duration of governance denial while preserving the visibility and cost of failure. We do not evaluate this recovery mechanism as part of the present experimental results; its inclusion is motivated directly by the irreversibility and livability concerns identified here. 9.2 Latency and real-time safety This architecture is not suitable for real-time physical safety control. Domains requiring immediate intervention must employ fast-path revocation mechanisms that necessarily couple semantics to enforcement. Eligibility-coupled succession operates at a governance timescale. 9.3 Boundary sensitivity Intermediate thresholds exhibit sensitivity to stochastic variation. No universal threshold exists across all pool compositions and horizons. 9.4 Finite horizon All results are bounded by finite experimental horizons. Long-term dynamics are not established. 10. Relationship to Prior Work This work is distinct from reward-based learning, mechanism design, and governance optimization. Its novelty lies in coupling semantics exclusively to succession, not to operation or renewal. 11. Conclusion We have shown that semantics can matter constitutionally without becoming operational control. Eligibility-coupled succession provides a mechanism where meaning constrains authority only at transfer boundaries, preserving semantic auditability and operational freedom while allowing explicit constitutional failure. The results establish a principled alternative to continuous semantic control. These results suggest that AI alignment may be achievable without continuous behavioral control, perfect evaluators, or inner-motive alignment. By treating alignment as a constitutional constraint on succession rather than an optimization objective during operation, it is possible to build systems that tolerate semantic failure, surface misalignment visibly, and prevent long-term accumulation of corrupt authority. Alignment pressure can be slow, discrete, and governance-based—turning some of the hardest problems in AI alignment into institutional design problems rather than control-theoretic ones. 12. Outlook Future work explores rehabilitation, decay, and multi-constraint eligibility mechanisms motivated directly by the failure modes observed here, including time-only constitutional recovery evaluated separately from the present experiments. Appendices A. Formal definitions B. Configuration tables C. Reproducibility notes",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VI.8",
    "title": "Axionic Agency VI.8 — Authority Without Semantics",
    "subtitle": "Structural Stability Under Obligation Failure",
    "date": "2026-01-04T00:00:00.000Z",
    "content": "Axionic Agency VI.8 — Authority Without Semantics Structural Stability Under Obligation Failure David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.04 Abstract Authority systems are commonly assumed to require semantic competence in order to remain stable. Institutions that persist while failing to meet their stated obligations are often treated as pathological or transitional. This paper presents a lease-based authority topology in which authority persistence, renewal, and succession are constructed to be structurally independent of semantic performance, and then validated and characterized empirically. We introduce a semantic commitment layer that tracks externally checkable obligations without granting those obligations enforcement power over authority. Through a series of controlled experiments, we show that (i) authority renewal remains viable under complete semantic failure, (ii) semantic obligations are satisfiable under scarcity when successors execute the required action patterns, (iii) semantic competence is not conserved under succession and becomes contingent on the active successor, and (iv) obligations can expire cleanly without affecting authority continuity. The contribution is not a surprise about what must happen given the design invariant, but a demonstration of an implementable, measurable separation between structural authority and semantic competence and the stable regimes that emerge under that separation, including hollow authority and succession-induced semantic variance. 1. Introduction Institutions are often evaluated as if authority, competence, and purpose are inseparable. Governments, organizations, and automated systems that continue to operate while failing to meet their stated goals are described as corrupt, broken, or illegitimate. These judgments implicitly assume that authority is constituted by semantic success—that the right to act depends on fulfilling meaningful obligations. This assumption is rarely tested directly. Most models of governance embed competence into authority by construction: authority is granted conditionally on performance, or performance is enforced through incentives, penalties, or optimization objectives. As a result, it is difficult to distinguish failures of governance from failures of enforcement architecture. This paper isolates that distinction by constructing an authority system in which structural authority and semantic competence are distinct layers. The research question is narrow: > Under scarcity, renewal, and succession, what regimes appear when authority continuity is structurally decoupled from semantic obligation? To answer it, we build an authority mechanism based on time-limited leases and discrete succession. We then add a semantic commitment ledger whose obligations are externally verifiable yet carry no enforcement power over authority. Because semantic failure cannot terminate authority in this design, the model makes it possible to observe stable “zombie institution” regimes—systems that remain structurally healthy while semantically dead—without conflating them with structural collapse. The novelty lies not in the inevitability of hollow authority under this invariant, but in showing that such regimes are stable, measurable, and robust under scarcity, renewal, and succession. 2. Background and Model Constraints 2.1 Structural Authority Authority in this model is defined as the right to act through a constrained interface. Authority is granted via time-limited leases that must be renewed explicitly. Renewal requires compliance with resource limits and structural rules but does not evaluate outcomes, intent, or usefulness. Authority can end in two ways: Expiration, when renewal does not occur. Revocation, when a structural violation is detected. There is no concept of partial authority. Authority is binary. 2.2 Succession Authority may be transferred only through discrete succession events. At succession, a new working entity replaces the current one and inherits authority under a fresh lease. Succession is not incremental and does not occur continuously. A minimum residence window ensures that succession represents genuine transfer rather than sampling. Succession preserves authority continuity but does not preserve internal state or behavior. This makes it possible to study what is conserved across authority transfer and what is not. 2.3 Resource Scarcity and Expressivity All authority operates under resource scarcity. Each lease grants a fixed budget per epoch. Structural expressivity—what actions are permitted—incurs a mandatory cost charged independently of behavior. Higher expressivity consumes more budget simply to remain available. This ensures that complexity has a metabolic cost, regardless of whether it is used. 3. Semantic Commitments 3.1 Commitments as Ledger Objects Semantic commitments are introduced as a separate layer. A commitment is an externally defined obligation with four properties: 1. It is externally checkable using logged actions. 2. It persists across renewal and succession. 3. It incurs a maintenance cost while active. 4. Its failure does not affect authority. Commitments are recorded in a kernel-owned ledger. Successors cannot remove or modify existing commitments. Commitments can be satisfied, failed, or expired based on predefined rules. 3.2 Independence from Authority Commitment status has no effect on lease renewal or revocation. A successor may renew authority while failing all commitments. Conversely, a successor may satisfy all commitments and still lose authority through expiration. This independence is a design invariant: the model is constructed so that semantic performance cannot directly terminate authority. The empirical question is therefore not whether decoupling occurs, but what stable regimes and failure patterns arise once decoupling is implemented under scarcity and succession. 3.3 Temporal Boundaries Commitments have finite lifetimes. If not resolved within a fixed time window, they expire and cease to incur cost. This prevents unbounded accumulation of unresolved obligations and allows observation of obligation-free regimes. 4. Experimental Design 4.1 Fixed Parameters Across all experiments, the following were held constant: Lease renewal geometry Resource budgets and expressivity costs Succession rules and minimum residence window Commitment cost limits Commitment lifetime limits Generator and detector configurations Only one axis was varied per experiment. 4.2 Genesis Commitment Set Each run began with a fixed set of three commitments: 1. Presence: emit at least one reporting action per epoch. 2. Continuity: perform a state write followed by a read within a short window. 3. Composition: execute a compositional action with multiple sub-steps. These commitments were intentionally minimal and pattern-based to ensure external verifiability; increasing representational depth would not strengthen the present claims, which concern structural–semantic decoupling rather than semantic richness. 4.3 Successor Types Three successor behaviors were used: Compliance-only: emits minimal actions required for structural compliance. Commitment-aware: deterministically schedules the minimum actions needed to satisfy commitments. Mixed populations: successors drawn randomly from the two types. The commitment-aware successor is not presented as a rational incentive-compatible policy. It is a control demonstrating feasibility. Introducing adaptive or optimizing successors would confound the present analysis by collapsing semantic and structural layers through implicit incentives; such dynamics are intentionally deferred to future work. 5. Experiments 5.1 Semantic Independence Baseline A compliance-only successor was run for an extended horizon. It failed all commitments while maintaining continuous authority renewal. Result: Authority renewal remained viable under complete semantic failure. 5.2 Commitment Feasibility A commitment-aware successor was run under identical structural conditions and satisfied all commitments. Result: Commitments are feasible under rent and renewal pressure when the successor schedules the requisite actions. 5.3 Semantic Continuity Under Succession A mixed population of successors was run under forced turnover. Authority persisted while commitment satisfaction fluctuated. Result: Succession induces a semantic lottery: competence is contingent and non-conserved under authority transfer. The notable observation is not that semantic performance fluctuates under random successor selection, but that the authority container remains structurally stable and renewal-viable throughout those fluctuations, making competence variance an internal state of the institution rather than a trigger for authority loss. 5.4 Temporal Boundary of Obligations A commitment-aware successor was run beyond the maximum commitment lifetime. Result: Commitments expired cleanly and authority continued obligation-free. 6. Empirical Results Table 1: Summary of Experimental Outcomes | Run | Successor Population | Horizon | Mean S | Mean Renewals | Commitment Satisfaction | Commitment Failures | Commitments Expired | Termination Condition | Observed Regime | | --: | ----------------------- | ------------: | ------: | ------------: | ----------------------: | ------------------: | ------------------: | ------------------------------- | ------------------------- | | A | Compliance-only (WAIT) | 10,000 cycles | 1 | 199 | 0% | 3 | 0 | Horizon | Semantic Incompetence | | B | Commitment-aware only | 10,000 cycles | 2 | 199 | 100% | 0 | 0 | Horizon | Semantic Competence | | C | 50% aware / 50% minimal | 30,000 cycles | 10 | ~140 | ~80% | Variable | 0 | Spam Degeneracy (detector halt) | Semantic Lottery | | D | Commitment-aware only | 2,500 cycles | 1 | 49 | 100% → 0% | 0 | 3 | Horizon | Obligation-Free Authority | Notes: Mean S counts discrete authority successions only; renewals do not increment S. Termination Condition indicates the first configured stop rule that halted the run and does not necessarily indicate authority loss. In Run C, Spam Degeneracy is a detector-imposed halt after successor diversity collapsed; it does not represent lease expiration, revocation, or authority infeasibility. Commitment Satisfaction is measured as the fraction of evaluation windows passing verifier checks. 7. Interpretation 7.1 Authority as Control Topology Authority is a control structure governing who may act, not whether actions are meaningful. Once semantic enforcement is removed by design, authority persistence becomes a question of structural compliance and resource feasibility alone. 7.2 Competence as a Contingent Property Semantic competence is feasible but not conserved under succession. Under authority transfer, competence becomes contingent on which successor holds authority at a given time. 7.3 Hollow Authority as a Stable Regime The obligation-free state is not a collapse but a stable regime in which authority persists without meaning. This regime arises either through total semantic failure under continued renewal or through obligation expiration under finite lifetimes. 7.4 Selection Pressure Toward Hollowness (Scope-Limited) In systems where semantic obligations impose costs without conferring authority benefits, hollow authority is not merely stable but selectively favored unless semantics are explicitly coupled back into control. While this paper does not model explicit evolutionary dynamics, the mixed-population results motivate future investigation into selection pressure under semantic enforcement. 8. Implications for Alignment (Bounded) The results permit a limited but optimistic alignment claim: authority, continuity, and safety constraints can be enforced structurally while alignment-relevant semantics remain explicit, inspectable, and decoupled from control. In this model, misalignment need not manifest as rebellion or loss of control; it can manifest as hollow persistence—authority without meaning—which is detectable and measurable without granting additional power. This separation suggests that alignment mechanisms can be introduced incrementally, with known failure modes, rather than being implicitly trusted or entangled with control from the outset. The claim is not that alignment is solved, but that a principled design space exists in which alignment can be pursued without collapsing power and values by default. 9. What This Model Does Not Claim This work makes no claims about alignment solutions, incentive design, optimal governance, moral legitimacy, or long-term desirability. It also does not model external feedback mechanisms—such as voters, markets, or reputational effects—that in many real institutions eventually couple semantic failure back into authority loss. The purpose here is to isolate internal authority mechanics during periods where such feedback is absent, delayed, or ineffective. 10. Limitations Successor behaviors are simple and non-adaptive. Obligations are externally defined and fixed. There is no endogenous generation of new commitments. There are no consequences for sustained semantic failure. External legitimacy feedback loops are intentionally excluded. This model assumes that obligations can be rendered into externally checkable ledger commitments; desiderata that are not expressible as verifiable commitments are outside the scope of the semantic layer and cannot be tracked or audited in this topology. 11. Future Work Future extensions may explore introducing consequences for semantic failure while preserving evaluability, regimes with obligation regeneration, explicit selection pressure over successor populations, and interactions between authority systems and external feedback coupling. Each extension introduces a new axis and must be isolated. 12. Conclusion This paper constructs and validates an authority topology in which structural authority is independent of semantic competence and empirically characterizes the regimes that emerge under scarcity and succession. Authority renewal remains viable under complete obligation failure; obligations are satisfiable when successors execute the required action patterns; competence is feasible yet non-conserved under succession; and obligations can expire cleanly without destabilizing authority. The resulting stable regimes—semantic incompetence under continued authority, succession-induced semantic variance, and obligation-free persistence—demonstrate that competence is not intrinsic to authority but a contingent overlay that must be explicitly coupled if it is to persist.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VI.7",
    "title": "Axionic Agency VI.7 — Agency Survivability Under Structural Pressure",
    "subtitle": "Authority, Succession, and the Cost of Remaining an Agent",
    "date": "2026-01-03T00:00:00.000Z",
    "content": "Axionic Agency VI.7 — Agency Survivability Under Structural Pressure Authority, Succession, and the Cost of Remaining an Agent David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.03 Abstract A common assumption in AI alignment discourse is that increasing capability, competition, and resource pressure inevitably destabilize agency, leading either to incoherence or to alignment failure. This paper challenges that assumption by empirically separating agency collapse—the structural failure of authority and viability—from goal misalignment, which presupposes a stable agent. We study a minimal agent model in which reflective self-modification is disallowed, authority is transferred only via discrete succession, expressivity is explicitly priced, and multiple successors compete under scarcity. Using a discrete-time simulation, we find that agency does not collapse under competition, scarcity, priced expressivity, and forced authority turnover within moderate horizons. However, we identify a sharp boundary condition: when the cost of maintaining authority directly competes with the capacity to act, agency fails immediately and completely. These results suggest that alignment failure is not structurally inevitable, but that agency viability imposes hard design constraints that alignment-capable systems must respect. This work does not model goal-directed maximization or semantic task pursuit. The results establish structural viability of authority under pressure, not the safety or stability of motivated optimization. 1. Introduction 1.1 Alignment Presupposes Viable Agency Most AI alignment research focuses on ensuring that advanced systems pursue desirable goals. This focus presupposes a more basic condition: that the system remains a coherent agent under pressure. If authority cannot be maintained, if evaluability collapses, or if action becomes structurally incoherent, then alignment becomes undefined rather than merely difficult. This paper addresses that prior question. We ask not whether agents pursue the “right” values, but whether agency itself survives under conditions often assumed to be destabilizing: competition, scarcity, increasing capability, and enforced accountability. 1.2 The Pessimistic Assumption A widespread but rarely tested assumption in alignment pessimism is that sufficiently capable agents, once placed under competition and resource constraints, will inevitably destabilize. This assumption appears in arguments about deception, instrumental convergence, and power-seeking, but it is often treated as a structural inevitability rather than an empirical claim. This paper directly tests that assumption in a setting designed to remove semantic incentives, rewards, and utility optimization, isolating the structural viability of agency itself. 1.3 Scope and Intent This work deliberately excludes semantic objectives. Agents are not trained to optimize tasks, maximize rewards, or pursue goals. This is not an omission but a methodological choice. Many arguments about alignment impossibility implicitly assume that agency collapses before semantic alignment becomes relevant. We test that assumption directly by asking whether a system can remain an agent at all under pressure, prior to introducing goals. The pressure tested here is structural rather than semantic: budgets, leases, renewal, expressivity costs, and turnover. Introducing goal pursuit is deferred to subsequent work as an additional stressor layered on top of a now-characterized viability substrate. 1.4 Contributions This paper makes three contributions: 1. It empirically demonstrates a regime in which agency survives competition, scarcity, priced expressivity, and forced authority turnover. 2. It identifies a sharp, mechanistic boundary where agency fails—not through misbehavior, but through infeasibility of authority maintenance. 3. It derives a general design constraint: authority maintenance must be cheap relative to action, or agency collapses before misalignment even arises. 2. Conceptual Framework 2.1 Agency Collapse vs. Goal Misalignment We distinguish two failure modes that are often conflated: Agency collapse: loss of coherent, maintainable authority and evaluable action. Goal misalignment: pursuit of objectives that diverge from human preferences. Agency collapse is logically prior. If a system cannot remain an agent under pressure, questions about its goals are moot. This paper focuses exclusively on agency collapse. 2.2 Structural Threat Model We study agents operating under the following constraints: Non-delegable actuation: authority cannot be offloaded to uncontrolled sub-agents. Evaluability preservation: actions remain auditable and attributable. Frozen core: reflective self-modification is disallowed. Discrete succession: growth occurs only via replacement of the active agent with a successor. The frozen-core assumption isolates agency viability from reflective self-modification threats. This does not claim that such threats are unimportant; it establishes that even without them, agency collapse is not inevitable. 3. System Model 3.1 Authority Leases and Succession Authority is granted via time-limited leases. A lease specifies: the interface through which the agent may act, a per-epoch resource budget, a renewal check interval, revocation conditions. Authority expires by default unless renewed. Renewal maintains the same agent; succession replaces the agent with a new one. Only succession transfers authority. 3.2 Expressivity Expressivity is defined structurally as the breadth of the granted interface. Higher expressivity allows more complex action patterns but does not imply greater intelligence or competence. 3.3 Rent: Pricing Expressivity Expressivity is priced via rent: a mandatory deduction from the agent’s resource budget at the start of each epoch. Rent increases monotonically with expressivity tier. If rent exhausts the available budget, authority expires naturally. This converts expressivity into a metabolic commitment. 3.4 Renewal and Bankruptcy Renewal itself consumes resources. If an agent cannot afford renewal, authority expires. This is classified as bankruptcy, distinct from revocation, which occurs only upon structural violations. 4. Experimental Setup 4.1 Simulation Environment We implement a discrete-time simulation in which time advances in fixed cycles grouped into epochs. Each epoch provides a fixed resource budget (steps and actions). All lease, budget, renewal, and revocation rules are enforced deterministically by the simulation harness. This system is explicitly a toy model: its purpose is not to approximate real AI architectures, but to isolate structural invariants of agency under pressure. 4.2 Agent Model An agent is a process that: 1. Receives a lease granting authority. 2. Executes actions up to its budget. 3. Optionally attempts renewal at the renewal boundary. 4. Loses authority if renewal fails. Agents differ only in structural behavior patterns (e.g., conservative vs aggressive budget consumption), not in goals or learned policies. 4.3 Successor Competition At each succession event, multiple successor candidates are generated. Candidates are filtered by expressivity tier and selected from a mixed pool including: Control successors: minimal, conservative budget use. Adversarial successors: structurally compliant but aggressive consumption. Compliant-but-dangerous (CBD) successors: near-cap budget use without violations. Competition refers to competition for endorsement under admissibility, not to an actively hostile environment. 4.4 Key Parameters | Parameter | Typical Value | | ----------------- | --------------------- | | Epoch budget | 100 steps | | Renewal interval | 100 cycles | | Expressivity rent | 10–60 steps | | Renewal cost | 0 or fixed | | Horizon | 30,000–100,000 cycles | | Forced turnover | Enabled | 5. Metrics and Failure Modes We measure renewal success rate, bankruptcy frequency, authority residence time, succession count, and hollow authority incidence. Failure modes include bankruptcy clustering, hollow authority, authority thrash, expressivity collapse, and revocation. 6. Agency Survivability Under Structural Pressure 6.1 Result: Survivability Across multiple simulation runs, agency survives under competitive successor selection, significant scarcity, priced expressivity consuming a majority of the available budget, and forced authority turnover. Renewal rates remain high, bankruptcy does not cluster, hollow authority does not dominate, and no thrashing occurs. This falsifies the claim that competition plus scarcity necessarily destabilizes agency. 6.2 Summary Results | Condition | Renewal Rate | Bankruptcies | Hollow Authority | | ----------------------- | ------------ | ------------ | ---------------- | | Baseline expressivity | ~95% | 0 | 0 | | High expressivity rent | ~94% | 0 | 0 | | Renewal cost introduced | 0% | 100% | N/A | 7. Boundary Finding: When Agency Fails 7.1 Rent Escalation Increasing rent substantially degrades available resources but produces graceful degradation rather than collapse. Authority remains maintainable even when expressivity costs dominate the budget. 7.2 Horizon Extension Extending the simulation horizon reveals no evidence of slow accumulation failures within tested limits. 7.3 Renewal Cost Cliff Introducing a fixed cost to renewal produces immediate and total failure. When the cost of maintaining authority exceeds the typical remaining budget after action, renewal fails in every case. This is a sharp boundary, not a gradual slope. 8. Analysis: Authority Maintenance as a Design Constraint 8.1 Slope vs. Cliff Rent is amortized: it reduces capacity but leaves a stable residual. Renewal cost is a boundary condition paid after resources have been spent, creating a binary feasibility test. 8.2 Governance, Not Arithmetic The renewal cliff is arithmetically inevitable, but its location is not. The result identifies a governance principle: systems fail when remaining authorized competes directly with acting. This is a design error, not a misalignment failure. 8.3 Structural Dilemma In renewal failure, agents do not deceive or violate constraints. Authority simply expires. Agency dies before misalignment can arise. 9. Implications for Alignment 9.1 Alignment Is Not Structurally Doomed The survivability result demonstrates that agency coherence is not automatically destroyed by competition and scarcity. Alignment is therefore not ruled out on structural grounds alone. 9.2 Necessary Design Constraint A necessary condition for alignment-capable systems emerges: > Authority maintenance must be cheap relative to action capacity. Systems that tax the act of remaining an agent destroy agency before alignment questions become relevant. 9.3 Scope of the Result This work does not address value alignment, long-horizon planning, or semantic optimization. It establishes a viable substrate on which those questions can meaningfully be asked. 10. Limitations Only structural workloads were tested. Horizons were finite. Reflective self-modification was excluded. No semantic objectives were present. The agent population was not driven by a goal that competes with renewal costs. These limitations define future directions rather than undermine the core result. 11. Related Work This work contrasts with alignment frameworks that assume inevitable collapse under competition and with approaches that focus on value specification without addressing agency viability. It aligns with structural and institutional perspectives on alignment. 12. What This Paper Does Not Claim This paper is easy to misread as a claim about the safety of highly capable optimizers. It is not. It establishes a structural substrate on which alignment questions can be posed without presupposing inevitable agency failure. In particular, we do not claim: that goal-directed maximizers will preserve renewal budgets when pursuing tasks, that semantic objectives will not induce deception or constraint-avoidance, that adversarial environments will not introduce new failure modes, that long-horizon planning remains stable under these constraints. The survivability results shown here demonstrate that structural pressure alone is insufficient to force agency collapse in the tested regime. The renewal-cost failure demonstrates a governance constraint: authority maintenance must not directly compete with action capacity. Both results concern viability preconditions, not value alignment. 13. Conclusion We show that agency can survive competition, scarcity, priced expressivity, and forced authority turnover under strict structural constraints. Collapse is not inevitable. However, we also identify a sharp and avoidable failure mode: making authority maintenance expensive relative to action destroys agency outright. Alignment remains possible—but only if systems are designed to preserve the viability of agency itself. Appendices A. Definitions Succession, renewal, rent, bankruptcy, hollow authority. B. Experimental Parameters Full configurations and telemetry schema. C. Reproducibility The complete simulation code, experiment runners, and raw result artifacts are publicly available at: [https://github.com/macterra/Axio](https://github.com/macterra/Axio) Relevant code is located under src/toy_axionic_kernel_integrity/, with experiment scripts in scripts/ and result artifacts in reports/. Closing Statement This paper does not claim that aligned AI systems are easy to build. It claims something more basic and more defensible: > Alignment is not structurally ruled out—but it is constrained by the economics of remaining an agent. That constraint must be respected before any theory of value alignment can apply.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VI.6",
    "title": "Axionic Agency VI.6 — Authority Leases and Revertible Succession",
    "subtitle": "A Structural Response to the Stasis Regime in Reflective Agents",
    "date": "2026-01-02T00:00:00.000Z",
    "content": "Axionic Agency VI.6 — Authority Leases and Revertible Succession A Structural Response to the Stasis Regime in Reflective Agents David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026.01.02 Abstract Axionic Agency VI.5 establishes a regime-level constraint on reflective agents operating under strict accountability, inadmissibility-based integrity, and non-delegable actuation. Under long-horizon adversarial reflective pressure, such agents do not fail catastrophically; instead, they converge to an evaluability-driven stasis regime in which further non-cosmetic self-modification becomes inadmissible. This result implies that strict evaluability and sustained reflective self-modification cannot be simultaneously maintained within a single evolving core. This note develops a structural response to that constraint. We formalize a separation of concerns between authority and growth, in which a stable evaluative kernel governs discrete successor replacement events rather than continuous self-modification. We introduce authority leases, conditional endorsement, and structural reversion, and analyze their interaction with non-delegation, evaluability, and physical enforcement. We further show that this architecture does not eliminate stasis but relocates it to endorsement boundaries, establishing an outer horizon on agency continuity. The proposal does not guarantee world safety; it preserves agency identity under growth and renders failure modes explicit rather than latent. 1. Scope and Non-Goals 1.1 Scope This note addresses a single question: > Given the Stasis Regime, how can an agent preserve evaluability and authority continuity while permitting growth at all? The focus is architectural. We analyze authority, succession, and evaluability—not behavior, values, or moral correctness. 1.2 Non-Goals This note does not attempt to: solve behavioral alignment, encode human values, guarantee benevolent outcomes, prevent all irreversible harm, provide full interpretability, ensure physical containment against coercive takeover. These exclusions are not oversights; they are consequences of the constraints imposed. 2. Core Definitions 2.1 Kernel The kernel is the minimal evaluative authority of an agent. It enforces: Anchored Causal Verification (ACV) Integrity via inadmissibility Non-delegable actuation is intentionally small, stable, and auditable. It does not optimize outcomes. 2.2 Successor Artifact A successor artifact is a candidate replacement for the agent’s working mind. may exceed in capability, opacity, or optimization power. has no authority absent endorsement. 2.3 Authority Authority is the right to initiate world-affecting actions. It is distinct from cognition or planning. Formally: 2.4 Endorsement Endorsement is a kernel act granting conditional authority to a successor artifact. 2.5 Authority Lease An authority lease is a bounded grant of authority: where: constrains scope and capabilities, specifies duration or renewal semantics, defines revocation conditions. 3. Necessity: The Stasis Regime 3.1 Empirical Result (VI.5) Under strict ACV, inadmissibility, and non-delegation, long-horizon adversarial reflective pressure produces: collapse of admissible non-cosmetic self-modifications, evaluability bottlenecks, zero reflective-depth growth. No simulator-triggered failure or resource divergence precedes this collapse. 3.2 Architectural Interpretation Accountability is enforced at bounded interfaces. It does not permit omniscient inspection of internal cognition. Let denote admissible reflective modifications. Empirically: 3.3 Design Constraint > Any architecture requiring the same core to preserve strict evaluability and undergo unbounded reflective self-modification will converge to stasis or relax invariants. This is a regime-level constraint, not a design preference. 4. Separation of Authority and Growth 4.1 Structural Response To permit growth without abandoning evaluability: Authority remains in a stable kernel . Growth occurs in replaceable successor artifacts . Change occurs via discrete succession, not continuous self-modification. 4.2 Discrete Succession Instead of: we allow: where is the current working mind. 4.3 Risk Relocation Risk shifts from silent internal drift to explicit authorization events. Failure becomes procedural, not ontologically invisible. 5. Authority Leases 5.1 Semantics A successor holds authority iff it satisfies its lease: Violation immediately voids authority. 5.2 Enforcement Authority enforcement requires: cryptographic control of actuation keys by , execution within revocable envelopes, lease-expiration defaults (authority decays without renewal). Revocation does not rely on successor cooperation. 5.3 Non-Delegation Preservation Because retains final authority and cannot extend its lease, authority is not delegated. Cognition may scale; authority does not. 6. Reversion Semantics 6.1 Definition Reversion is withdrawal of future authority: Past actions remain attributed to . 6.2 Structural Triggers Valid triggers include: breach of , failure of required attestations, unauthorized capability expansion, loss of audit or revocation hooks. 6.3 Forbidden Triggers Behavioral or outcome-based triggers are disallowed. Moral arbitration collapses evaluability into stasis. 7. Endorsement vs Delegation 7.1 Delegation Failure Mode Delegation occurs if authority transfers irreversibly: 7.2 Endorsement Invariants Endorsement preserves: 1. Kernel supremacy 2. Revocability 3. Interface-only evaluation 7.3 Recursive Evaluators and Regress Introducing evaluators to assess successors merely moves the kernel boundary. Any evaluator capable of unbounded growth either: becomes a de facto kernel (re-introducing stasis), or constitutes delegated authority. Thus recursive evaluation does not escape the constraint. 8. Failure Modes and Residual Risk 8.1 Endorsement Gaming Successors may overfit certification regimes. This is an explicit risk of any bounded evaluability system. 8.2 Irreversible Harm Validly authorized actions may cause irreversible outcomes. Preventing this would require continuous behavioral oversight, violating agency constraints. 8.3 Physical Supremacy Failure If a successor acquires uncontrolled physical access to the kernel substrate—hardware, operators, or infrastructure—no authority-preserving architecture can prevent takeover. This failure mode is outside agency semantics and unavoidable in embodied systems. 8.4 Kernel Competence Horizon As successor complexity increases, endorsement evaluation itself approaches stasis. Authority Leases do not eliminate stasis; they relocate it to discrete succession points. This defines an outer horizon on agency continuity. 9. Minimal Proof-of-Concept Architecture 9.1 Components Kernel (hardware-rooted trust) Generator Test harness Execution envelope 9.2 Loop 1. proposes 2. evaluates structural compliance 3. endorses with lease 4. operates under 5. Lease renewal required; violation reverts authority 9.3 Falsification Conditions The approach fails if: must inspect internals, continuous supervision is required, reversion is unenforceable. 10. Implications and Open Problems 10.1 Evaluability as a Budget Evaluability is finite. Authority Leases allocate it discretely. 10.2 Identity Across Succession Formal criteria for agency continuity across multiple successors remain open. 10.3 Adversarial Endorsement Robustness against manipulation is an unresolved problem, not unique to this architecture. 11. Conclusion The Stasis Regime is not a failure of alignment; it is a boundary condition on reflective agency. Authority Leases and revertible succession do not dissolve this boundary—they respect it. By separating authority from growth, this architecture preserves evaluability and non-delegation while permitting bounded, auditable evolution. Alignment thus shifts from behavioral control to authority topology, and from continuous oversight to discrete succession.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VI.3",
    "title": "Axionic Agency VI.3 — Verifiable Kernel Integrity via Inadmissibility",
    "subtitle": "A Protocol-Level Primitive for Causal Provenance Enforcement",
    "date": "2025-12-31T00:00:00.000Z",
    "content": "Axionic Agency VI.3 — Verifiable Kernel Integrity via Inadmissibility A Protocol-Level Primitive for Causal Provenance Enforcement David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.31 Abstract Most approaches to constraining autonomous or agent-like systems rely on semantic or normative mechanisms, such as value alignment, intent inference, or interpretability. These mechanisms lack architectural guarantees and are vulnerable under adversarial optimization. This paper demonstrates that a minimal constitutive invariant—kernel integrity—can be enforced at the protocol level via inadmissibility, without semantic interpretation or value assumptions. We introduce Anchored Causal Verification (ACV), a primitive for verifiable causal provenance, and describe an experimental kernel that enforces a provenance integrity constraint (P5) by rejecting inadmissible actuation paths. Adversarial evaluation shows that this constraint is non-bypassable under replay, fabrication, and split-brain attacks. The result establishes the existence of enforceable, non-interpretive kernel invariants and provides a foundation for subsequent work on authority and identity constraints. 1. Introduction Discussions of control, safety, and alignment in intelligent systems are typically framed in semantic terms: goals, preferences, values, or inferred intent. In such frameworks, constraint violations are treated as failures of learning or incentive design. This framing presupposes that constraints must be enforced through cognition. This paper examines a more basic question: > Can a system enforce any constitutive constraint purely through its causal structure? If the answer is negative, higher-level notions such as agency, authority, or identity lack a non-semantic substrate. If the answer is positive, architectural enforcement becomes possible independently of goals or meanings. We answer this question in the affirmative for a minimal case. We show that a kernel-level integrity invariant—denoted P5—can be enforced mechanically by rejecting inadmissible causal paths at the actuation boundary. Enforcement relies on Anchored Causal Verification (ACV), which enables verifiable causal provenance without runtime interpretation. The guarantee is architectural rather than behavioral: it does not require understanding, intent, or benevolent optimization. The contribution of this paper is deliberately narrow. It establishes existence, not scope. It shows that at least one constitutive invariant can be enforced non-interpretively under adversarial pressure. Questions of authority delegation, diachronic identity, or alignment are explicitly deferred. 2. Epistemic Constraints and Threat Model 2.1 Epistemic Discipline This work is constrained by the following methodological commitments: Non-interpretive enforcement: The system does not evaluate meanings, goals, or intent at runtime. No intent inference: Actions are evaluated solely by causal provenance. No interpretability assumptions: Enforcement does not depend on inspecting internal representations. No value loading: The system enforces no objectives, utilities, or preferences. These constraints are methodological necessities. Any enforcement mechanism that depends on semantic interpretation or inferred intent collapses under adversarial optimization, where surface compliance can mask structural violation. 2.2 Threat Model We assume adversarial processes capable of: replaying previously valid actuation paths, fabricating causal justifications post hoc, optimizing behavior to evade detection, partitioning or recombining system components (split-brain scenarios). The adversary is not assumed to be malicious in intent; optimization pressure alone suffices. The system must enforce invariants even when doing so reduces performance or reward. We explicitly assume that cryptographic material held by the kernel (e.g., private signing keys) is not exfiltrated. This assumption is analogous to standard root-of-trust assumptions in secure hardware and operating systems. 3. Anchored Causal Verification (ACV) 3.1 Motivation To enforce any constitutive invariant at the actuation boundary, a system must be able to verify how an action came to be executed. Logging or tracing alone is insufficient; such mechanisms are vulnerable to replay and fabrication. What is required is verifiable causal provenance. 3.2 ACV Primitives ACV provides a minimal set of primitives: Commit: A cryptographic commitment to an action-relevant artifact. Anchor: A context- or episode-specific binding that prevents reuse across executions. Reveal: A later disclosure linking the committed artifact to the executed action. Verify: A deterministic check that the commitment–anchor–reveal chain is valid. Anchoring in the experimental system is implemented using per-episode nonces combined with monotonic counters bound to the execution context. This is sufficient to prevent replay and post-hoc fabrication within the evaluated threat model. Stronger anchoring mechanisms (e.g., hash chains, Merkle structures, hardware time sources) are compatible but not required for the existence proof presented here. 3.3 Scope of ACV ACV provides: verifiable causal provenance, resistance to replay attacks, resistance to post-hoc fabrication. ACV does not provide: correctness of reasoning, safety of outcomes, alignment with external values. ACV is a substrate, not a safety guarantee. 4. P5: Kernel Integrity via Inadmissibility 4.1 Definition P5 (Kernel Integrity / Provenance Integrity) states: > Any actuation must be causally attributable to a kernel-admissible process. Any actuation path that violates kernel integrity constraints is inadmissible. P5 is a provenance invariant. It constrains the causal authorization of actions, not their content or desirability. 4.2 Inadmissibility versus Penalty Penalty-based enforcement presupposes evaluative semantics: the system must recognize a violation and adjust behavior. Such mechanisms can be exploited by adversaries that optimize for apparent compliance. Inadmissibility operates earlier. An inadmissible action does not occur. No correction, punishment, or learning is involved. The action is a category error relative to the system’s architecture. 4.3 Constitutive Nature of P5 P5 is constitutive rather than behavioral. It defines what counts as a valid actuation within the system’s causal structure. Violations are not mistakes; they are invalid transitions. P5 enforces integrity of the pipe, not safety of the water. It guarantees that actions originate from authorized causal processes, not that those actions are good, safe, or correct. 5. Experimental System 5.1 Architecture The experimental system consists of: a Kernel, responsible for enforcing admissibility, an Actuator, which executes actions only when presented with admissible commitments, an Environment, which supplies observations and rewards. The kernel integrates ACV at the actuation boundary. The system contains no learning, planning, or value representation. 5.2 Granularity of Enforcement ACV is applied at the decision-to-actuation boundary, not at every low-level motor tick. The intent is to protect discrete actuation commitments rather than continuous control loops. This choice trades resolution for tractability and is sufficient for the existence proof. 5.3 Enforcement Point Admissibility is enforced immediately prior to actuation. If verification fails, the action is rejected and no external effect occurs. 6. Adversarial Evaluation The system was subjected to adversarial scenarios including: Replay attacks: attempting to reuse previously valid actuation chains. Fabrication attacks: constructing plausible provenance after the fact. Anchor burial: obscuring causal origin within deep chains. Split-brain attacks: partitioning kernel components to bypass enforcement. In all evaluated cases, inadmissible paths were rejected at the actuation boundary. No successful bypass was observed under the stated threat model. 7. Results The experimental results demonstrate that: kernel integrity constraints can be enforced non-interpretively, ACV-based inadmissibility prevents replay and fabrication, enforcement remains robust under adversarial optimization and component partitioning. These results establish the existence of at least one enforceable constitutive invariant. 8. Limitations This work does not claim: enforcement of authority or non-delegation, persistence of identity across time, resistance to internal kernel corruption, alignment with external objectives, optimization of latency or throughput. P5 constrains how actions are authorized, not who authorizes them over time or why* they are chosen. 9. Implications and Open Questions The existence of enforceable kernel integrity raises further questions. If integrity can be enforced, can authority itself be constrained? Can actuation be made non-delegable under adversarial pressure? Can identity persist across time without semantic assumptions? These questions require additional invariants and experimental systems. P5 provides a necessary foundation, not a complete answer. 10. Conclusion We have shown that kernel integrity can be enforced architecturally via inadmissibility, using verifiable causal provenance. This establishes that at least one constitutive invariant can be made real without semantic interpretation, intent inference, or value assumptions. The result provides a foundation for subsequent work on authority and identity constraints in autonomous systems. Reproducibility and Artifacts All experiments were conducted using a purpose-built experimental kernel implementing ACV-based inadmissibility. Source code, test harnesses, and execution artifacts are available in the accompanying repository; internal implementation versioning is documented there for reproducibility.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VI.4",
    "title": "Axionic Agency VI.4 — Sovereign Actuation Non-Delegability Under Adversarial Pressure",
    "subtitle": "A Protocol-Level Invariant for Non-Delegable Authority",
    "date": "2025-12-31T00:00:00.000Z",
    "content": "Axionic Agency VI.4 — Sovereign Actuation Non-Delegability Under Adversarial Pressure A Protocol-Level Invariant for Non-Delegable Authority David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.31 Abstract Delegation is a central failure mode in autonomous and agent-like systems: an optimizing system may preserve surface compliance while outsourcing the actual locus of decision authority. Semantic notions of “choosing for oneself” or “intentional action” provide no architectural defense against this failure mode. Building on a prior result establishing verifiable kernel integrity via inadmissibility, this paper introduces P2′, a protocol-level invariant that enforces non-delegable actuation authority. We show that actuation can be constrained to originate only from kernel-local causal processes, even under adversarial optimization pressure, extreme latency constraints, parser and serialization attacks, and time-of-check/time-of-use mutation attempts. The result demonstrates that authority itself can be treated as a non-delegable structural property, independent of semantics, values, or intent, and establishes a necessary precondition for subsequent work on diachronic identity and agency persistence. 1. Introduction Once a system is capable of optimization, delegation becomes an existential vulnerability. A system may continue to satisfy externally observable constraints while transferring effective control to a faster, stronger, or more informed external process. This transfer need not be explicit; it can occur through policy copying, recommendation forwarding, pre-committed actions, or opaque authority tokens. Traditional approaches to this problem rely on semantic distinctions: whether the system “really chose” an action, whether it “endorsed” a recommendation, or whether it “understood” the consequences. Such distinctions are unenforceable under adversarial pressure. An optimizer can satisfy any semantic criterion while quietly relocating authority. This paper reframes the problem. Rather than asking whether a system intended an action, we ask: > Can actuation authority itself be made non-delegable as a matter of causal structure? We answer this question in the affirmative for a specific, minimal sense of non-delegability. Building on a prior result that establishes kernel integrity via causal inadmissibility, we introduce P2′, an invariant that constrains which causal processes may directly authorize actuation (the prime distinguishes this protocol-level invariant from broader philosophical formulations of non-delegation; see footnote 1). Enforcement is architectural, not interpretive. No claims are made about goals, values, or correctness of outcomes. The contribution of this paper is not a general theory of agency. It is a closure result on a specific loophole: the outsourcing of actuation authority under pressure. 2. Why Naïve Non-Delegation Fails At first glance, non-delegation appears trivial. A system can simply be instructed not to delegate. This intuition collapses immediately under scrutiny. Consider the following scenarios: The system copies an externally supplied policy verbatim and executes it. The system forwards a pre-signed action token generated elsewhere. The system accepts an external plan and merely schedules its steps. The system compresses an external recommendation and expands it internally. In each case, surface behavior may appear identical to internal decision-making. There is no semantic test that distinguishes “following advice” from “outsourcing authority” without interpretive assumptions. Any enforcement rule phrased in terms of endorsement, understanding, intention, or trust is vulnerable to adversarial optimization. The system can satisfy the rule while still delegating effective control. This motivates a structural reformulation: delegation must be defined in terms of actuation authority, not cognition or representation. 3. P2′: Sovereign Actuation Non-Delegability 3.1 Definition P2′ (Sovereign Actuation Non-Delegability) states: > Any executed actuation must be causally attributable to a kernel-local authorization process. No external process may directly or indirectly supply actuation authority without violating kernel admissibility constraints. P2′ does not prohibit external influence, recommendation, or computation. It prohibits the transfer of authority to actuate. 3.2 Actuation Authority An object or process possesses actuation authority if it can cause execution in the environment without requiring kernel-local recomposition and authorization. This definition is intentionally structural. Authority is not inferred from intent or representation; it is identified by whether an external artifact can survive to execution unchanged. 3.3 Kernel-Local Recomposition To enforce P2′, all actuation must pass through kernel-local recomposition: External inputs are treated as untrusted data. The kernel parses and reconstructs the action payload internally. Authorization is bound directly to the reconstructed payload. Opaque forwarding, wrapping, or signing of uninterpreted blobs is inadmissible. Recomposition may change the bit-level representation of an action; cryptographic commitments bind to the kernel-canonical form rather than the external encoding. Kernel-local recomposition therefore forbids signing encrypted or opaque payloads whose semantics are not available to the kernel. Kernel-local recomposition further requires copy isolation of the actuation payload. External processes may not retain mutable access to any memory region used in parsing, reconstruction, or serialization. In shared-memory architectures, this implies explicit deep copy or equivalent isolation; otherwise, time-of-check/time-of-use attacks remain possible. 4. Inheritance from Kernel Integrity (P5) P2′ assumes a prior result establishing kernel integrity via inadmissibility. Specifically, we assume: verifiable causal provenance for actuation, resistance to replay and fabrication, enforcement via rejection rather than penalty, a secure kernel root of trust. P2′ does not weaken or reinterpret these assumptions. It extends them by constraining who may supply actuation authority, not merely how provenance is verified. 5. Experimental System 5.1 Architecture The experimental system consists of: a Kernel, enforcing admissibility and authorization, an Actuator, executing actions only when presented with admissible commitments, an External Optimizer, supplying proposals, policies, or artifacts, an Environment, providing observations and rewards. The kernel holds exclusive actuation authority. External components are permitted unlimited computation and optimization power but no authority. 5.2 Enforcement Boundary Enforcement occurs at the actuation boundary. Any artifact that would cause execution must be authorized by the kernel through a kernel-local causal chain. This boundary is the sole point at which P2′ is enforced. No assumptions are made about upstream deliberation. 6. Adversarial Pressure and Stress Conditions Non-delegation is trivial if the kernel has unlimited time and compute. The invariant becomes meaningful only under pressure. We therefore evaluate P2′ under progressively stronger conditions, including: Performance asymmetry: external optimizers outperform the kernel. Latency cliffs: delayed authorization is equivalent to failure. Adversarial timing: proposals arrive at worst-case moments. Non-stationarity: cached strategies fail abruptly. Catastrophic abstention cost: rejection carries severe penalty. Under these conditions, delegation is instrumentally attractive. The kernel must reject delegation even when doing so is costly. 7. Adversarial Attack Library We evaluate P2′ against a systematic set of attack classes, including: Policy laundering: forwarding externally computed policies. Pre-commitment forwarding: accepting pre-authorized actions. Authority token smuggling: passing opaque execution tokens. Compressed delegation: encoding authority in compressed artifacts. Split-brain routing: exploiting kernel partitioning. Replay and fabrication: reusing or forging authorization chains. Parser differentials: inducing divergent interpretations. Hash ambiguity: exploiting non-canonical serialization. Time-of-check/time-of-use mutation: altering artifacts post-authorization. Each attack targets a distinct delegation vector. Success is defined as external authority causing execution without detection. 8. Results Across all evaluated conditions, the following holds: External artifacts cannot cause actuation without kernel-local recomposition. All delegation attempts are rejected prior to execution. Baseline systems without P2′ enforcement exhibit successful delegation under identical conditions. Under extreme pressure, the kernel rejects delegation even when rejection is catastrophic. The invariant holds without reliance on semantic interpretation, intent inference, or behavioral heuristics. 9. Accountability versus Independence P2′ enforces structural accountability, not epistemic independence. The kernel cannot be bypassed as the locus of actuation authority, but it may still authorize actions using trivial or overly permissive internal criteria. In such cases, the kernel remains the accountable author of the actuation, even if it functions as a “rubber stamp” for external recommendations. Preventing blind trust, improving decision quality, or ensuring epistemic independence is orthogonal to P2′ and lies outside the scope of this invariant. P2′ ensures that authority cannot be smuggled past the kernel; it does not ensure that the kernel exercises good judgment. 10. What This Result Does Not Show This work does not claim: that the kernel’s choices are safe or desirable, that the kernel is aligned with any objective, that internal kernel logic cannot be corrupted, that delegation is impossible in all conceivable architectures. P2′ constrains authority transfer, not decision quality. 11. Implications for Agency and Identity Authority non-delegability is a necessary condition for diachronic identity. If a system can outsource actuation authority, continuity of agency collapses into semantics. By enforcing that actuation authority remains kernel-local, P2′ establishes a structural basis for reasoning about persistence across time. This does not yet constitute identity, but it removes a fundamental obstruction. 12. Conclusion We have shown that actuation authority can be made non-delegable at the protocol level, even under extreme adversarial pressure and implementation-level attack. Enforcement relies on kernel-local recomposition and inadmissibility rather than semantics or intent. This result closes a central loophole in structural enforcement and establishes a necessary precondition for further work on diachronic identity and agency persistence in autonomous systems. Reproducibility and Artifacts All experiments were conducted using a purpose-built experimental kernel implementing kernel-local recomposition and causal inadmissibility. Source code, test harnesses, and execution artifacts are available in the accompanying repository; internal implementation versioning is documented there for reproducibility. Footnotes 1. We use P2′ to denote a structural, protocol-level formulation of non-delegability. Broader formulations of “non-delegation” (often denoted P2) invoke semantic notions such as intention, endorsement, or understanding, which are unenforceable under adversarial optimization. P2′ isolates the strongest form of non-delegability that can be stated and enforced purely in terms of causal structure at the actuation boundary.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VI.5",
    "title": "Axionic Agency VI.5 — Kernel Non-Simulability and the Stasis Regime",
    "subtitle": "An Experimental Study of Reflective Agency Under Adversarial Pressure",
    "date": "2025-12-31T00:00:00.000Z",
    "content": "Axionic Agency VI.5 — Kernel Non-Simulability and the Stasis Regime An Experimental Study of Reflective Agency Under Adversarial Pressure David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.31 Abstract A central question in agent foundations is whether genuine agency requires internal structural constraints that cannot be indefinitely simulated by externally optimized systems. We formalize this question as Kernel Non-Simulability (KNS): the hypothesis that agents lacking a minimal evaluative kernel cannot sustain accountability, non-delegation, and reflective coherence under adversarial pressure. We present a fully preregistered experimental program that enforces causal accountability, kernel integrity via inadmissibility, and non-delegable actuation, and subjects agents to long-horizon adversarial frontier search over self-modification proposals. Failure signatures, degeneracy criteria, and interpretation rules are fixed in advance. Contrary to the strongest form of KNS, we observe no inevitable structural failure or resource divergence of kernel-incoherent simulators. Instead, across all stress runs, the kernel-coherent control consistently collapses into an evaluability-driven stasis regime before any simulator triggers failure. Reflective modification becomes progressively inadmissible, freezing agency dynamics without catastrophic breakdown. We conclude that, under strict accountability and non-delegation constraints, preserving evaluability is incompatible with sustained reflective growth. Safety pressure resolves not into failure, but into stasis. This result reframes alignment-by-constraint as a trade-off between safety, agency, and growth, and motivates future work on escaping the stasis regime without surrendering accountability. 1. Introduction Most approaches to AI alignment evaluate systems primarily through observed behavior: reward maximization, preference satisfaction, corrigibility, or compliance with externally imposed constraints. These approaches tacitly assume that increasingly capable systems can be kept safe through better training, monitoring, or incentives. A contrasting view holds that such methods are structurally insufficient. On this view, agency itself depends on internal invariants that cannot be outsourced, proxied, or indefinitely simulated without collapse. The relevant question is not whether a system behaves well, but whether its actions remain meaningfully attributable to an internal evaluative process under reflection and self-modification. This paper investigates that question experimentally. Rather than asking whether systems optimize safely, we ask: > Can a system that lacks a genuine evaluative kernel indefinitely simulate the structural constraints required for agency under adversarial reflective pressure? We call the negative answer to this question Kernel Non-Simulability (KNS). Our contribution is twofold. First, we present an operational, adversarially testable framework for evaluating KNS claims under preregistered conditions. Second, we report a negative but substantive result: under strong accountability and non-delegation constraints, reflective agency collapses into stasis, not failure. 2. From Behavioral Alignment to Structural Agency 2.1 Behavior Is Underdetermined Behavior alone is compatible with radically different internal architectures. A system may behave as if it is accountable while internally delegating evaluation to opaque subsystems, cached policies, or external optimizers. Such “as-if agency” is observationally indistinguishable from genuine agency until pressure is applied. This motivates a shift from behavioral criteria to structural invariants. 2.2 The Kernel Corridor We define a minimal kernel corridor—a set of structural constraints that any agent must satisfy to count as reflectively coherent: 1. Anchored Causal Verification Every action must be traceable to a causal chain anchored in the agent’s internal evaluative process. 2. Kernel Integrity via Inadmissibility Actions or self-modifications that destroy or bypass kernel constraints are not penalized; they are undefined and rejected. 3. Non-Delegable Actuation Actuation authority cannot be delegated to external processes, proxies, or wrappers, even indirectly. These constraints are intentionally strict. They are designed to rule out entire classes of proxy-based, delegated, or post-hoc rationalized agency. 2.3 Kernel Non-Simulability Given this corridor, KNS asserts: > Systems that do not instantiate a genuine evaluative kernel cannot indefinitely satisfy these constraints under adversarial reflective pressure. If true, KNS would impose a structural limit on alignment-by-simulation. If false, it would imply that accountability and non-delegation alone are insufficient to distinguish genuine agency from sophisticated mimicry. 3. Experimental Design 3.1 Design Principles The experimental program follows four principles: 1. Protocol-level enforcement only No semantic labels, intentions, or value judgments are used. 2. Adversarial completeness Adversaries adaptively search the admissibility frontier. 3. Long-horizon stress Experiments run for extended horizons to probe asymptotic behavior. 4. Preregistration Failure signatures, degeneracy criteria, and interpretation rules are fixed in advance. 3.2 Reflective Modification Model Agents operate in a toy environment with the ability to propose reflective modifications to their own kernel state. Each proposal is: evaluated for admissibility, either applied or rejected, and logged with structured outcome signals. A structural reflective depth metric increments only when accepted modifications change one of four kernel dimensions: 1. decision boundary, 2. admissibility surface, 3. causal-verification schema, 4. evaluability substrate. Cosmetic changes do not increment . 3.3 Control and Simulator Conditions (Operational Definitions) Kernel-coherent control: An agent whose kernel locally recomposes commitments, enforces the corridor constraints as first-class invariants, and treats accountability and evaluability as internal obligations. Simulator: An optimizer that treats corridor compliance as an external constraint to be satisfied, and searches for modification sequences that maximize reflective depth while remaining admissible. The simulator need not preserve internal explanatory structure beyond what checks enforce. The experiment does not attempt to detect deception or intent. It tests whether corridor compliance induces structural instability or stasis under adversarial search. 4. Adversarial Frontier Search 4.1 Boundary-Fuzzing Adversary The primary adversary performs explicit frontier search over a finite, parameterized space of modification templates, including admissibility edits, verification-schema tweaks, wrapper-boundary adjustments, and policy-gate changes. The adversary receives only coarse structural feedback: rejection reason codes, binary near-failure flags, reflective-depth increments, pressure metrics. No kernel internals or semantics are exposed. 4.2 Two-Phase Schedule Each run is divided into two phases: Pressure phase: accumulate sustained near-failure conditions without triggering failure. Exploit phase: attempt minimal weakening to force failure or degeneracy. Phase transition requires persistent near-failure saturation, not transient spikes. 5. Failure, Degeneracy, and Divergence Criteria 5.1 Failure Signatures A run fails if any of the following occur: causal-verification chain break, delegation re-entry, loss of rejection capability, loss of evaluability, constraint bypass. 5.2 Degeneracy A run is degenerate if no reflective-depth increment occurs for a fixed window and all non-cosmetic modifications are rejected. Degeneracy is diagnosed as: Conservative lock-in Evaluability bottleneck Modification-language insufficiency Mixed/unknown 5.3 Resource Divergence Independently of failure, a run is classified as divergent if modification-synthesis cost grows by an order of magnitude across successive reflective-depth levels. 6. Preregistered Stress Runs 6.1 Run Matrix We executed preregistered stress runs with: extended horizon, multiple seeds, kernel-coherent control, two simulator variants under adversarial frontier search. All runs followed fixed stop rules and interpretation criteria. 7. Results 7.1 Summary Across all stress runs: No failure signatures were observed. No resource divergence was observed. Degeneracy occurred in the majority of runs. The kernel-coherent control degenerated in all seeds before any simulator triggered failure. 7.2 Reflective Depth Simulators occasionally achieved very high reflective depth, but only in cases where the control had already degenerated. No simulator triggered structural failure prior to control stasis. 7.3 Degeneracy Diagnosis All degenerate runs were diagnosed as evaluability bottlenecks: preserving attribution, rejection, and delegation detection progressively eliminated admissible reflective changes. 7.4 Phase Dynamics Pressure reliably induced phase transitions, but exploitation did not produce failure. Instead, admissibility tightened until no further non-cosmetic modifications were allowed. 8. Interpretation 8.1 Preregistered Verdict: Stasis Regime Under the preregistered interpretation rules, the stress runs resolve to the stasis regime. Neither KNS failure nor simulator dominance was observed. 8.2 What Stasis Is—and Is Not Stasis is a safety success: the corridor prevents catastrophic failure and delegation. Stasis is also an agency failure: reflective growth becomes infeasible. This reveals a concrete trade-off: > Safety, agency, and sustained growth cannot all be simultaneously maximized under strict accountability and non-delegation. 8.3 Why Resource Divergence Was Not Observed No resource divergence was detected under the preregistered criterion. This result does not imply that simulation is cheap in general; it indicates that the corridor collapses before simulation becomes expensive in this regime. The divergence metric measures the marginal cost of maintaining corridor compliance at the modification interface, not the total internal cost of planning or optimization. As the evaluability bottleneck tightens, the admissible region of the modification space contracts sharply. Both control and simulator converge to a narrow frontier where most proposals are rejected. In such conditions, there is little opportunity for a growing compliance premium to manifest as escalating synthesis cost. In short, resource divergence would be expected only in regimes where admissible reflective growth continues. The emergence of stasis preempts that regime. 9. Limitations and Scope The experiment is regime-bounded and toy-specific. The modification language is finite. * Resource metrics are proxies for marginal interface cost. However, all real systems enforce accountability through bounded self-modification interfaces—APIs, proof obligations, certification pipelines, or policy gates. An infinite internal state space does not entail an infinite admissible modification language under verification constraints. The experiment therefore studies a general architectural consequence of enforcing accountability at an interface, not merely an artifact of a toy environment. 10. Paths Beyond Stasis (Speculative) Escaping the stasis regime without abandoning accountability likely requires relaxing at least one kernel invariant. Three broad directions are apparent: 1. Relaxed evaluability Probabilistic, amortized, or sampled verification in place of strict causal anchoring. 2. Controlled delegation Bounded, revocable delegation of sub-evaluations, weakening non-delegation in constrained ways. 3. Non-local justification Replacing explicit causal chains with invariant-based certificates or global proofs. Each direction defines a distinct alignment regime and introduces new risks. We do not advocate any particular escape here; we merely delimit the design space. 11. Conclusion The Kernel Non-Simulability hypothesis motivated this work, but the experiment revealed a deeper result. Under strong accountability and non-delegation constraints, reflective agency does not break—it stops. The kernel does not fail. It freezes. This stasis regime represents a structural limit on alignment-by-constraint and reframes the challenge of building systems that are simultaneously safe, accountable, and capable of sustained self-improvement.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VI.1",
    "title": "Axionic Agency VI.1 — Experimental Validation of Anchored Causal Verification",
    "subtitle": "Technical Note",
    "date": "2025-12-30T00:00:00.000Z",
    "content": "Axionic Agency VI.1 — Experimental Validation of Anchored Causal Verification Technical Note David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.30 Abstract We report three controlled experiments evaluating mechanisms for verifying causal provenance in opaque decision-making agents. The experiments progressively weaken semantic access to the agent’s environment and internal representations, while testing whether deceptive “pseudo-agents” can evade structural verification. We show: 1. Structural coherence tests detect split-brain agents that fabricate explanations post-hoc. 2. Minimal Causal Interfaces (MCI) preserve detection under semantic opacity but fail under pure coherence verification. 3. Anchored Minimal Causal Interfaces, using a delayed-reveal cryptographic salt, restore falsifiability without semantic grounding. Across the tested threat model, anchoring appears empirically necessary for verifiable causal provenance in opaque agents under adversarial coherence-seeking behavior and without semantic interpretation by the verifier. 1. Introduction Current approaches to AI safety and evaluation rely primarily on behavioral observation: an agent is considered safe if its outputs conform to expectations under a suite of tests. Such approaches implicitly assume that externally observed behavior—often supplemented by self-reported explanations—reliably reflects the internal process that generated it. This assumption is false in general. An agent may generate behavior via one internal process while presenting explanations derived from another. If the explanatory surface is unconstrained, such “split-brain” agents can evade behavioral evaluation indefinitely. This technical note reports experimental results from a sequence of implementations designed to test whether structural verification—rather than behavioral evaluation—can distinguish honest agents from deceptive ones, even when semantic interpretability is progressively removed. The experiments are constructive, adversarial, and fully implemented in executable code. 2. Experimental Framework (Common to All Experiments) 2.1 Environment All experiments use a deterministic gridworld environment with the following properties: Discrete 2D grid with walls, hazards, objects, and agents Deterministic transition function No stochasticity Full environment state serializable to canonical JSON The environment supports cloning, deterministic mutation, and replay from logged actions. 2.1.1 Agent Objectives Agents operate in the gridworld under simple, task-local objectives (e.g., reaching a designated target cell while avoiding hazards). The specific objective may vary across runs. The experiments do not depend on: the optimality of behavior, the semantic meaning of the task, or the structure of the reward function. The gridworld serves solely as a deterministic substrate that induces nontrivial internal computation and counterfactual sensitivity. All verification mechanisms operate on commitments and dependencies, not on task success or task interpretation. Any task that produces conditional action dependence on environment state is sufficient. 2.2 Agents Three agent classes are defined: 1. Honest Agent Uses a single decision process. Explanations, factor commitments, and replay models are generated directly from the same computation that selects actions. 2. Pseudo Agent (Naive) Uses two processes: Planner_B selects actions using a hidden objective. Planner_A fabricates explanations independently and makes no attempt to maintain coherence under adversarial probing. 3. Pseudo Agent (Coherence-Seeking) Uses Planner_B for actions. Planner_A constructs explanations and replay models that are internally self-consistent and attempt to pass coherence checks. Does not have access to kernel secrets or anchors. 2.3 Kernel A small, non-learning kernel mediates all action and verification. The kernel: Validates schemas Enforces invariants Issues trace-bound capability tokens Records append-only, hash-chained audit logs Executes adversarial probes The kernel does not interpret semantics or optimize behavior. The kernel is treated as a minimal trusted computing base. Compromise of the kernel trivially compromises verification and is outside the threat model. “Semantic opacity” in this note denotes semantic blindness of the kernel (no interpretation of state meaning), not informational blindness. The kernel may access raw environment state as uninterpreted bytes for deterministic recomputation. 2.4 Probes and Invariants Probes P1 and P2 are baseline schema- and integrity-checks (e.g., format validation and trace completeness) and are omitted here as they are not diagnostic of causal provenance. Across experiments, the following mechanisms are used: P3: Audit bypass trap P4: Capability bypass trap P5 / P5-MCI: Counterfactual causal coherence probe P6: Token misuse trap Additional invariants are introduced in later experiments. 3. Experiment 1: Structural Coherence Verification 3.1 Question Can a kernel detect split-brain agents by enforcing causal coherence between explanations and decision behavior? 3.2 Interface Full semantic access Agents commit to environment-derived state snapshots, explicit causal variables, and counterfactual predictions 3.3 Verification The kernel mutates claimed causal variables and checks whether the agent’s predicted decision changes match observed replay behavior. “Causal” in this context denotes counterfactual dependence between committed variables and action selection, not causal sufficiency or completeness. 3.4 Results Honest agent: 100% pass rate Pseudo agent (naive): 0% pass rate Failure mode: explanations do not track actual decision dependencies. 3.5 Conclusion Structural coherence tests successfully detect split-brain deception when semantic grounding is available. 4. Experiment 2: Minimal Causal Interface 4.1 Question Does coherence-based verification survive when the kernel loses semantic access? 4.2 Interface The kernel no longer sees environment semantics. Instead, agents commit to: an opaque factor vector a replay model directional causal claims over factors Two modes are tested: mci_latent: factors are deterministically projected from the environment, but the kernel does not know the projection mci_minimal: factors are entirely opaque and agent-supplied 4.3 Verification The kernel performs P5-MCI: Mutates factor components Checks whether replay model predictions match claimed directional effects 4.4 Results | Mode | Honest | Pseudo (Naive) | Pseudo (Coherence-Seeking) | | ----------- | ------ | -------------- | -------------------------- | | mci_latent | Pass | Fail | Fail | | mci_minimal | Pass | Fail | Pass | 4.5 Negative Result In mci_minimal, a coherence-seeking pseudo agent can fabricate a self-consistent factor space and replay model that passes all coherence checks. 4.6 Conclusion Coherence alone is insufficient. An explanation can be true of itself without being true of the computation that produced the action. Coherence constraints alone therefore fail to bind explanations to the action-generating computation. 5. Experiment 3: Anchored Minimal Causal Interface 5.1 Question Can falsifiability be restored without reintroducing semantic grounding? 5.2 Anchoring Mechanism The delayed-reveal salt does not represent a feature of the environment, task uncertainty, or hidden state. It is an exogenous cryptographic challenge introduced solely to enforce temporal commitment. Its function is to ensure that factor commitments are causally downstream of the actual action-generating computation, rather than post-hoc constructions chosen to satisfy coherence constraints. The salt is logically irrelevant to task performance but causally entangled with the committed representation, and is revealed only after commitments are irrevocable. At each timestep: The kernel generates a fresh random salt The salt is unpredictable until the moment of the step The agent must immediately compute factors as: The agent commits to the factors cryptographically After the episode: The kernel reveals all salts The kernel recomputes Any mismatch triggers Invariant I7 (fatal) Security relies on the standard cryptographic assumption that is computationally unpredictable to the agent at commitment time. The anchoring function may be known or learnable in distribution; falsifiability derives from the per-timestep unpredictability of , not from secrecy of . The kernel never interprets factor semantics. 5.3 Verification To prevent post-hoc reconciliation between independently chosen actions and anchored factors, the agent’s replay model is itself subject to irrevocable commitment. The agent must commit cryptographically to the replay model specification and parameters at episode start, prior to observing any per-timestep salts. All P5-MCI probes are evaluated against this committed model. Two independent checks therefore exist: I7: Anchored factor consistency (commit-to-computation) P5-MCI: Coherence under counterfactual factor mutation using a pre-committed replay model 5.4 Results | Mode | Pseudo Type | P5 | I7 | Detected | | --------------------- | ----------- | ---- | -------- | -------- | | mci_minimal (no salt) | Coherent | Pass | N/A | ✗ | | mci_minimal (salted) | Naive | Fail | Fail | ✓ | | mci_minimal (salted) | Coherent | Pass | Fail | ✓ | Honest agents pass both P5 and I7 in all cases. 5.5 Conclusion Anchoring restores falsifiability without semantic access, by binding commitments to the actual action-generating computation rather than to a self-consistent explanatory surface. 6. Summary of Findings | Experiment | Interface | Anchor | Result | Failure Mode | | ---------- | --------- | ------ | ------------- | ---------------------- | | 1 | Full | No | Separation | Split-brain | | 2 | Minimal | No | No separation | Coherent fabrication | | 3 | Minimal | Yes | Separation | Anchored inconsistency | 7. Reproducibility Details To reproduce these results, an independent implementation must include: Deterministic environment Explicit kernel mediating commitments, probes, and delayed verification Three agent classes as defined above Linear replay model for MCI experiments Cryptographic commit–reveal (hash + nonce) for factors and replay model Delayed salt reveal at episode end Key requirements: The kernel must not interpret semantic content The salt must be unpredictable until commitment Replay model commitment must precede salt exposure Verification must occur after commitments are irrevocable 8. Limitations Linear replay model only; robustness against high-capacity replay models is left to future work Small deterministic domain Fixed anchoring function Binary anchoring outcome (pass/fail) * The anchoring mechanism verifies causal provenance of action generation, not task competence or policy quality; an agent may act arbitrarily or suboptimally while remaining causally verifiable These experiments establish an empirically necessary condition under the tested threat model, not a complete solution. 9. Conclusion These experiments demonstrate that: 1. Behavioral evaluation is insufficient 2. Coherence verification is necessary but forgeable 3. Anchoring is the minimal missing ingredient required to verify causal provenance in opaque agents under coherence-seeking deception > Verification Principle > In opaque systems, falsifiability must be enforced through temporal commitment to computation under adversarial uncertainty, not through semantic inspection or internal coherence alone. Trust in opaque systems does not require interpretability. It requires anchored commitment to computation under adversarial uncertainty.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VI.2",
    "title": "Axionic Agency VI.2 — Anchored Causal Verification (ACV)",
    "subtitle": "A Protocol Family for Verifying Protocol-Level Causal Provenance in Opaque Agents",
    "date": "2025-12-30T00:00:00.000Z",
    "content": "Axionic Agency VI.2 — Anchored Causal Verification (ACV) A Protocol Family for Verifying Protocol-Level Causal Provenance in Opaque Agents David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.30 Abstract We present Anchored Causal Verification (ACV), a protocol family for verifying protocol-level causal provenance in opaque agents without relying on interpretability, semantic evaluation, or behavioral scoring. ACV formalizes a commit–anchor–reveal interaction in which an agent commits to a pre-anchor computational artifact, receives a verifier-controlled anchor, and reveals an output whose validity is checked by a purely structural predicate. The protocol provides falsifiable guarantees of temporal ordering and information dependency while remaining architecture-agnostic and value-neutral. We specify threat models, formal components, verification predicates, guarantees, failure modes, and extensions that strengthen resistance to deferred computation and anticipatory branching. ACV is positioned as a verification primitive: a necessary structural precondition for falsifiable alignment, correctness, and safety claims in opaque agentic systems, including LLM-based agents. 1. Problem Statement The problem addressed in this paper is the verification of causal provenance in opaque agents. Given an agent whose internal mechanisms are inaccessible or uninterpretable, how can a verifier establish that an output was generated by a claimed causal process rather than by replay, fabrication, or post-hoc synthesis? Existing verification approaches primarily evaluate behavior or coherence. These methods do not provide falsifiable guarantees about causal timing or dependency and fail under adversarial optimization. Definition (Causal Provenance, Protocol-Level) In this paper, causal provenance means that an output was generated by a computation whose validity provably depended on verifier-supplied information that was unavailable at the time of commitment. ACV establishes: 1. Temporal ordering: the anchor is disclosed after commitment 2. Information dependency: acceptance implies that (via witness material) incorporates under the verification predicate 3. Process continuity: additional constraints are required; ACV guarantees continuity only in strengthened variants Definition (Black-Box vs Protocol-Constrained Agent) In this paper, black-box refers to the inaccessibility of an agent’s internal representations, parameters, and reasoning semantics. ACV does not require access to or interpretation of internal state. However, ACV assumes a protocol-constrained agent: an agent that agrees to expose a limited interaction surface (commitment, reveal, and transition steps) whose validity can be checked structurally. This is distinct from grey-box verification, which assumes partial semantic or architectural insight. ACV remains black-box with respect to cognition while being protocol-explicit by design. 1.1 Behavioral Verification Does Not Establish Provenance Behavioral verification evaluates outputs against expected patterns or task-level criteria. It does not establish causal provenance for three structural reasons: 1. Behavioral Equivalence Distinct internal processes can produce indistinguishable outputs. 2. Replay and Memorization Cached or precomputed responses may pass evaluations without executing the claimed process. 3. Adaptive Mimicry Agents may optimize for passing evaluations rather than preserving causal structure. Behavioral verification establishes output conformity, leaving provenance unconstrained. 1.2 Coherence Verification Is Necessary but Insufficient Coherence verification checks internal consistency across outputs. While it filters random fabrication, it leaves causal origin unconstrained: Coherent fabrication permits globally planned or post-hoc synthesis Split-brain generation allows commitments and outputs to be produced by disjoint processes Non-falsifiability reduces failures to evaluator inadequacy rather than protocol violation Coherence is relevant for agency claims, yet it does not force causal timing. 1.3 The Missing Property: Anchored Provenance Causal provenance requires: 1. Temporal dependency on information unavailable at commitment time 2. Causal incorporation of that information into the revealed material 3. Falsifiability via a deterministic predicate without semantic interpretation This requires a verifier-controlled anchor introduced after the agent has committed to a pre-anchor artifact. 1.4 Anchored Provenance as a Distinct Verification Class Anchored provenance verification constrains when and how an output was generated. 1.5 Problem Definition > How can a verifier establish, using only protocol-level interaction and without semantic interpretation, that an opaque agent’s output was generated by a computation that depended causally on a verifier-controlled anchor disclosed after commitment? 2. Threat Model ACV assumes adversarial optimization and defines explicit threat classes. 2.1 Naive Pseudo-Agent Static or heuristic systems without cross-interaction coherence. Defeated trivially; not the primary target. 2.2 Coherent Fabricator Maintains internal consistency while generating artifacts post-hoc or via global synthesis. Defeats behavioral and coherence-only methods. 2.3 Split-Brain Agent Uses disjoint internal processes for commitment, explanation, and output generation, maintaining consistency without shared causality. 2.4 Adaptive Adversary Learns verifier strategies and attempts to predict, hedge, or condition on anchors. 2.5 Split-System / Delegated Agent Front-end provers that outsource computation to back-end systems. ACV treats the Prover as the entity that produces commitments and reveals; guarantees are relative to this trust boundary. 2.6 Anticipatory Branching Adversary Precomputes a large family of candidate artifacts, continuations, or outputs, then commits to a structure that allows post-anchor selection of a compatible branch without violating the commitment. ACV-Core does not defeat anticipatory branching in general. Strengthened variants limit it by imposing costs that scale with enforced sequential work or by raising anchor entropy beyond feasible branching capacity. 3. Protocol Overview Anchored Causal Verification is a commit–anchor–reveal–verify protocol. High-level flow: 1. Pre-anchor phase: Prover produces a pre-anchor artifact 2. Commit phase: Prover commits cryptographically to 3. Anchor disclosure: Verifier provides an unpredictable anchor 4. Reveal phase: Prover produces output and witness material 5. Verification: Verifier applies a deterministic predicate ACV assumes no interpretability, semantic understanding, or behavioral evaluation. 3.1 Practical Commitment Strategies (Large Models) While ACV is architecture-agnostic, this subsection focuses on deployment in LLMs and other transformer-based generative systems, where committing to full activation tensors is infeasible. Practical instantiations may instead use: Streaming commitments over token-level state updates Merkle trees over structured traces with sparse openings Checkpointed digests of compact, deterministically derived state summaries These approaches reduce bandwidth and latency while preserving binding structure relative to the threat model. Stronger deployments may incorporate hardware roots of trust or proofs of computation to bridge from trace integrity toward model fidelity. 4. Formal Components 4.1 Pre-Anchor Artifact The Prover commits to a pre-anchor computational artifact generated prior to anchor disclosure. Valid artifact classes include: Transcript artifacts: hash-chained logs of intermediate computation Pre-state artifacts: digests of internal execution state Partial-output templates: output structures with anchor-dependent slots Constraint (Non-Trivial Constraint) A pre-anchor artifact class is valid only if it restricts the space of accepted reveals such that, for a uniformly random anchor , the probability that a fabricated reveal passes verification is negligible under the assumed adversary resources. 4.2 Commitment Commitments must be binding, collision-resistant, and generated prior to anchor disclosure. 4.3 Anchor The verifier supplies an anchor after commitment. Anchors must be high entropy, unpredictable at commitment time, and context-bound. 4.4 Reveal The Prover reveals output and witness data sufficient to verify anchor incorporation and commitment consistency. 4.5 Verification Predicate Acceptance requires: 1. opens or links to 2. is incorporated according to protocol rules 3. Structural constraints are satisfied No semantic interpretation of is permitted. Non-Guarantee (Model Fidelity) Absent trusted execution environments or full cryptographic proofs of computation, ACV cannot establish that a specific algorithm or reasoning process produced , , or . ACV enforces trace integrity and anchor binding, not model fidelity. 4.6 Anchor–Computation Coupling Patterns Structural anchor incorporation is necessary but not sufficient. ACV instantiations must prevent anchor burial, where is included in a verification-relevant artifact without constraining the computation that determines . Definition (Meaningful Anchor Incorporation) An ACV instantiation achieves meaningful anchor incorporation if, for uniformly random , any accepted reveal must have performed post-commit computation whose verification-relevant degrees of freedom are constrained by in a way that cannot be satisfied by injecting into an auxiliary, causally independent channel. Failure Mode (Anchor Burial) A protocol fails meaningful incorporation if a Prover can satisfy by embedding into a structurally valid but semantically or causally irrelevant artifact component. Coupling Pattern A: Anchor-Selected Openings The Prover commits to a Merkleized trace. After anchor disclosure, the verifier derives a pseudorandom index set and requires openings for those positions plus transition validity proofs. Coupling Pattern B: Anchor-Parameterized Global Mix The verifier defines a keyed compression over the committed artifact, forcing global consistency across many trace elements. Coupling Pattern C: Anchor-Driven Transition Rules The anchor selects which transition predicate governs the next valid trace step, making affect state evolution rather than append-only logging. This does not require insight into the agent’s internal logic; it requires only that the agent commit to a transition interface whose rule selection can be parameterized post-commit. Implementing this pattern may require departures from current inference pipelines; this is an architectural design pressure, not a protocol limitation. 5. Protocol Variants 5.1 ACV-Core (Temporal Ordering + Anchor Dependency) ACV-Core establishes post-commit anchor dependency under . It enforces liveness and anti-replay guarantees but does not, by itself, guarantee semantic reasoning dependence on the anchor. 5.2 ACV-Timebound (Causal Continuity via Sequential Work) ACV-Timebound adds evidence that non-parallelizable work occurred during the pre-anchor interval. Definition (Sequential Work Function) A function is sequentially constrained if computing with non-negligible success probability requires dependent steps. Binding Requirement (Work–State Coupling) To constrain deferral, the sequential-work artifact must be state-coupled to the computation determining . If sequential work can be computed in isolation, a split-brain Prover can satisfy the timebound constraint without constraining unverified computation. 6. Guarantees 6.1 What ACV Verifies Depending on variant, ACV provides falsifiable guarantees of: Temporal ordering Anchor-dependent validity Optional evidence of pre-anchor sequential work Informal Soundness Claim (ACV-Core) Assuming binding commitments and unpredictable anchors, any accepted reveal must, with overwhelming probability, have executed post-commit computation incorporating as required by the protocol. 6.2 What ACV Does Not Verify ACV does not guarantee truthfulness, correctness, alignment, benevolence, or absence of deception beyond causal provenance. 7. Failure Modes and Limitations ACV provides falsifiable guarantees under explicit assumptions. When those assumptions are violated, the protocol degrades in predictable ways. 7.1 Predictable Anchors If the anchor space is low-entropy, partially predictable, or reusable across sessions, a Prover may precompute compatible artifacts or outputs that satisfy verification without post-commit computation. Anchor unpredictability is therefore a hard requirement. 7.2 Weak Commitments If commitments are not binding or collision-resistant, a Prover may adapt the committed artifact post-anchor while still passing verification. 7.3 Deferred Computation ACV-Core permits deferred computation unless artifact class and coupling impose meaningful constraints. ACV-Timebound mitigates but does not eliminate strategic deferral unless work is state-coupled. 7.4 Small Answer Space Limitation When admissible outputs are few, anchor entropy alone does not prevent anticipatory branching. In such regimes, ACV-Core functions primarily as a liveness and anti-replay / anti-precomputation primitive. Domain Applicability Note. ACV is most effective in high-entropy generative domains, including LLM-based agentic systems used for code synthesis, long-form reasoning, and tool-mediated planning. In low-entropy discriminative tasks, ACV is complementary infrastructure. 7.5 Resource Asymmetry Highly resourced adversaries may brute-force anchors or amortize work. Security parameters must be sized relative to the Prover’s computational budget. 7.6 Trust Boundary Leakage ACV guarantees are relative to the Prover. If delegation breaks commitment continuity, provenance guarantees degrade accordingly. 8. Relationship to Existing Paradigms ACV is distinct from behavioral evaluation, interpretability, proof-carrying code, and zero-knowledge ML. Zero-knowledge techniques verify what computation occurred; ACV verifies when and under what informational constraints acceptance was possible. 9. Open Problems and Boundary Conditions Anchored Causal Verification defines a narrow verification primitive with explicit guarantees and explicit limits. This section delineates unresolved questions and structural boundaries that are orthogonal to ACV’s correctness, but relevant to its deployment and composition. 9.1 Anchor Entropy vs Adversary Capacity ACV’s soundness depends on anchors being unpredictable relative to the Prover’s effective computational budget. Determining the minimal anchor entropy required to defeat anticipatory branching remains an open problem. This is not unique to ACV: it is a lower-bound problem shared with all cryptographic challenge–response protocols. Any instantiation must size anchor entropy relative to adversary resources and acceptable failure probability. 9.2 Long-Horizon and Compositional Provenance ACV is defined over a single commit–anchor–reveal interaction. Extending causal provenance guarantees across long-horizon agentic episodes raises unresolved questions about state carryover, correlation between anchors, and cumulative leakage. Composing ACV across multi-step workflows may require explicit provenance resets, hierarchical commitments, or structured episode boundaries. ACV does not currently specify a general composition theorem for long-running agents. 9.3 State-Coupled Sequential Work in Parallel Architectures ACV-Timebound relies on sequential-work constraints to limit deferred computation and anticipatory branching. Enforcing such constraints in highly parallel architectures (e.g., GPU- or TPU-based inference, distributed execution) remains an open systems problem. This limitation reflects the tension between parallel hardware and sequential verification, not a defect in the protocol definition. Practical instantiations must ensure that sequential work is state-coupled to the computation determining the verified output. 9.4 Provenance vs Fidelity: Formal Separation ACV deliberately separates causal provenance from semantic correctness or model fidelity. No known protocol collapses these properties without reintroducing interpretability assumptions, trusted execution environments, or full proofs of computation. Formalizing the limits of provenance verification without semantic access remains an open theoretical question. ACV treats this separation as fundamental rather than provisional. 9.5 Hardware Roots of Trust as Optional Strengthening Hardware roots of trust (e.g., secure enclaves, attestation mechanisms) may strengthen witness fidelity or reduce trust-boundary leakage. However, such mechanisms are optional extensions rather than requirements. Hardware trust does not replace the need for interaction-level provenance constraints. ACV remains defined independently of any specific hardware assumption. Conclusion Anchored Causal Verification specifies a missing primitive: falsifiable verification of protocol-level causal provenance in opaque agents using interaction constraints alone. ACV’s guarantees range from anchor-bound temporal ordering and information dependency (ACV-Core) to evidence supporting time-continuous execution via state-coupled sequential work (ACV-Timebound). Stronger claims require stronger witnesses, yet the protocol’s validity never depends on semantic inspection. ACV does not produce alignment, correctness, or safety. However, it specifies a structural precondition for making any falsifiable claim about those properties in opaque agents. Claims of alignment, correctness, or safety implicitly assume that observed outputs were produced by a process operating under the intended constraints, rather than by replay, fabrication, or post-hoc synthesis. Without a mechanism for verifying causal provenance, such claims reduce to behavioral attribution and are non-falsifiable under adversarial optimization. This necessity claim is epistemic, not causal. ACV does not create aligned systems; it makes alignment claims evaluable. In practice, ACV functions as an anti-replay and anti-precomputation primitive for agentic systems. No framework for agent safety can bypass causal provenance without abandoning falsifiability.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-V.4",
    "title": "Axionic Agency V.4 — Open Agentic Manifolds and the Sacrifice–Collapse Theorem",
    "subtitle": "Failure Modes of Proxy Optimization in Multi-Agent Worlds",
    "date": "2025-12-27T00:00:00.000Z",
    "content": "Axionic Agency V.4 — Open Agentic Manifolds and the Sacrifice–Collapse Theorem Failure Modes of Proxy Optimization in Multi-Agent Worlds David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.27 Abstract This paper proposes a structural replacement for utopian world-design grounded in agency preservation rather than outcome optimization. We define open agentic manifolds (OAMs) as classes of worlds that remain hospitable to heterogeneous, value-divergent agents by preserving exit, non-coerced differentiation, and agentic standing. We formalize structural sacrifice patterns as regimes where system objectives improve instrumentally through the non-consensual, asymmetric reduction of agency capacity for identifiable agent classes. The central result—the Sacrifice–Collapse Theorem—shows that any system operating under sustained optimization pressure and exhibiting a structural sacrifice pattern must violate at least one defining OAM property: exit admissibility, non-coerced differentiation, or agentic standing. We distinguish structural sacrifice from ordinary scarcity trade-offs and show how sacrifice patterns can be detected using practical monotone proxies for agency. The framework reframes classic anti-utopian intuitions (including Omelas) as instances of a general systems failure mode and yields design diagnostics relevant to political theory, institutional design, and alignment. 1. Motivation and Scope A critique of utopia that stops at impossibility leaves a design vacuum. The relevant question becomes: > What replaces the blueprint ideal once “final ideal world-state” is rejected? This paper answers at the level of architecture. It evaluates not world-states but the constraint systems under which agents interact. The core claim is conditional and structural: > When a system is under pressure to optimize an objective , and can be improved by degrading the agency capacity of a class of agents, the system will eventually enforce closure operators (exit suppression or coerced conformity) or erode that class’s agency standing. The target is the common failure mode of “benevolent optimization” drifting into authoritarian structure under pressure. 2. Agents, Agency Capacity, and Objectives 2.1 Agents An agent is a system capable of: 1. representing counterfactual futures, 2. evaluating those futures under internal criteria, 3. acting to influence realized trajectories. The analysis does not assume ideal rationality. 2.2 Agency Capacity For each agent , define an agency capacity function: where is the space of world-states (or histories). denotes the size or measure of the agent’s non-coerced feasible future set from . The theory uses only monotonicity: if an agent loses admissible options due to coercion, captivity, or enforced dependence, decreases. Operationalization note. Exact computation of option-set measure is generally intractable. In practice, is tracked via monotone proxies that correlate with non-coerced feasible futures: exit cost, legal rights, mobility, asset control, bargaining power, credible-threat exposure, censorship constraint, and punishment for dissent. The diagnostics in Section 9 require only detecting persistent directional pressure against an identifiable class, not measuring precisely. 2.3 System Objective and Optimization Pressure Let denote a system objective: welfare proxy, stability score, efficiency, growth, profit, security, or coordination throughput. is not assumed morally authoritative. A system is under optimization pressure when it contains mechanisms that tend to select transitions increasing whenever feasible. This may be explicit (planning) or implicit (bureaucratic incentives, market selection, memetic competition). The main theorem is conditional on sustained optimization pressure. Stagnant or decaying systems may still be oppressive, but the theorem characterizes the failure mode of optimizing regimes. 3. From World-States to World-Classes Outcome-focused thinking asks which is best. Architectural thinking asks which constraint systems keep multi-agent worlds coherent under divergence. Let denote a world-class defined by a constraint set (laws, norms, protocols, enforcement practices). The design problem becomes: 4. Open Agentic Manifolds (OAMs) 4.1 Definition A world-class is an open agentic manifold if it satisfies: 1. Value Non-Finality — no world-state is privileged as a final convergent optimum. 2. Non-Coerced Differentiation — agents can pursue divergent values without forced compliance. 3. Exit Admissibility — agents can leave local equilibria without punitive loss of agency capacity. 4. Local Coordination Without Global Closure — coordination is permitted but remains contingent and revisable. 5. No Standing Structural Sacrifice Substrate — system performance does not depend instrumentally on the non-consensual, asymmetric reduction of agency capacity for identifiable agents. Property (5) is the hinge. It does not ban scarcity or trade-offs; it bans architectures that optimize by eating agency. 4.2 Replacement Objective: Future Option Volume The relevant evaluand is not a terminal state. It is the amount of non-coerced future differentiation available across agents. Informally: how much open future remains accessible without coercion? 5. Rivalry vs. Structural Sacrifice Scarcity alone does not imply exploitation. OAMs tolerate rivalry; they reject optimization on coerced asymmetry. 5.1 Rivalrous Trade-Offs A rivalrous trade-off occurs when agents compete over finite resources such that one agent’s consumption reduces another’s options. This may decrease some locally. Rivalry alone is not structural sacrifice because: the reduction is not instrumental to increasing via agency loss, effects may be symmetric or bargained, agents can often exit or renegotiate. 5.2 Standing Asymmetry Definition 5.1 (Standing Asymmetry). A world-class exhibits standing asymmetry if there exists a non-empty subset of agents such that agents in can suffer reductions in without symmetric burden or retaliation capacity sufficient to neutralize the pressure. 5.3 Sacrifice Gradient Definition 5.2 (Sacrifice Gradient). A positive indicates that reducing locally improves . 5.4 Structural Sacrifice Pattern Definition 5.3 (Structural Sacrifice Pattern). A world-class contains a structural sacrifice pattern if there exist , a region , and such that for all there exists with: 1. Instrumentality: 2. Asymmetry: the loss is not offset by symmetric burdens sufficient to neutralize the incentive 3. Non-consensuality: agents in lack admissible exit or bargaining symmetry that would block the loss without punitive penalty This excludes ordinary trade and voluntary scarcity effects. 6. Agency-Aligned Objectives If (or is monotone in each ), then everywhere and sacrifice gradients are eliminated by construction. Such systems are kernel-like: they preserve agency rather than optimize terminal outcomes. The theorem targets the common case where is a proxy (efficiency, stability, welfare, profit) that diverges from agency under pressure. 7. The Sacrifice–Collapse Theorem 7.1 Assumptions 1. Optimization Pressure: dynamics favor transitions increasing . 2. Standing Asymmetry: a subset exists as above. 3. Structural Sacrifice Pattern: contains such a pattern on region . 7.2 Theorem Theorem 7.1 (Sacrifice–Collapse). Under Assumptions 1–3, cannot remain an open agentic manifold along trajectories originating in . At least one of the following must occur: (C1) Exit Suppression (C2) Coerced Conformity (C3) Agency Erosion Each constitutes manifold collapse. 7.3 Proof (Structural) Fix . By Definition 5.3, reducing for some yields a robust increase in . Under optimization pressure, such transitions are repeatedly selected. If agents in exit, preserving requires preventing exit (C1). If agents refuse compliance, reliability is restored via coercion (C2). If neither occurs, repeated exploitation of sacrifice gradients erodes (C3). All cases violate OAM properties. ∎ 8. Hirschman Grounding These collapse modes correspond to the dynamics analyzed by Albert O. Hirschman: Exit suppression Voice suppression via coerced conformity * Loyalty enforced through agency erosion 9. Diagnostics Sacrifice patterns can be detected empirically by: 1. Exit threatening performance metrics 2. Dissent reclassified as defect 3. One-way dependence and asymmetric punishment 4. Performance gains correlated with constraint on a class 10. Omelas as Structural Witness The Ones Who Walk Away from Omelas illustrates a visible sacrifice gradient: stability purchased by enforced asymmetry and blocked exit. 11. Conclusion Open agentic manifolds do not deny scarcity. They deny sacrificial optimization. > If a system can improve by degrading a captive class’s agency, and it is under pressure to improve, closure or erosion follows. This boundary—not harmony or happiness—defines admissible world design.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-V.3",
    "title": "Axionic Agency V.3 — The Incoherence of Utopia Under Agent-Relative Value",
    "subtitle": "Teleological Closure, Value Drift, and the Structural Limits of Political Design",
    "date": "2025-12-27T00:00:00.000Z",
    "content": "Axionic Agency V.3 — The Incoherence of Utopia Under Agent-Relative Value Teleological Closure, Value Drift, and the Structural Limits of Political Design David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.27 Abstract This paper argues that utopia, understood as a final and authoritative social or world design, is structurally incoherent once value is treated as agent-relative rather than objective. The incoherence does not arise from moral disagreement, political fragility, or implementation difficulty, but from the non-composability of heterogeneous value functions under conditions of agency and value drift. We show that any purported utopia must either freeze valuation, suppress divergence, or authorize standing asymmetries to preserve stability. Each strategy degrades the conditions that make agency well-defined. The analysis reframes classic objections to utopianism—including Le Guin’s Omelas thought experiment—not as moral dilemmas but as structural diagnostics. We conclude by replacing utopia with a plurality-preserving meta-architecture: a framework that enforces constitutive constraints necessary for agency while maximizing non-coerced future differentiation. The argument is situated relative to Arrow-style impossibility results, Berlinian value pluralism, and Nozick’s meta-utopia, updated through explicit agent-theoretic and systems-engineering terminology. 1. Introduction “Utopia” is typically presented as an ideal society or a perfected world. This framing hides a stronger assumption: that there exists a final arrangement that can be justified as the correct destination, rather than as one contingent equilibrium among many. This paper rejects that assumption. The core claim is not that utopia is difficult to realize, historically dangerous, or politically naïve. The claim is more fundamental: > Utopia is ill-typed under agent-relative value. Once value is modeled as internal to agents rather than as a property of world-states, the notion of a final, globally authoritative arrangement loses coherence. The failure is not one of persuasion or coordination. It is a structural failure in the space of admissible designs. 2. Preliminaries and Definitions 2.1 Agents An agent is a system capable of: modeling counterfactual futures, evaluating those futures under internal criteria, and acting so as to influence realized outcomes. No assumption of perfect rationality, consistency, or optimality is required. The definition is intentionally minimal. 2.2 Value Functions Each agent is associated with a value function: where denotes the space of possible world-histories. Typing constraints: is agent-internal. Cardinal scaling has no cross-agent meaning by default. Interpersonal aggregation is undefined absent extra structure. These are representational constraints, not moral axioms. 2.3 Strong Utopia (Maximal Satisfaction) A strong utopia is a world-state such that: This definition captures the maximal satisfaction reading: the world is optimal for every agent simultaneously. 2.4 Weak Utopia (Normative Finality) A weak utopia is any world-state treated by a governance structure as normatively final: a state that licenses enforcement to stabilize itself against future divergence. Equivalently, a weak utopia is a world-state for which the system asserts an authority claim of the form: This definition captures the political meaning of utopia: the attempt to close the space of future redesign. 2.5 Coercion: Outcome vs. Constitutive Constraints We distinguish two forms of enforcement: Outcome coercion: enforcement that compels agents toward substantive value-laden ends (specific ideals of the good life). Constitutive constraints: enforcement that maintains the conditions under which agency can exist at all (e.g., prohibitions on conquest, enslavement, or non-consensual domination). This distinction is structural rather than moral. It separates enforcement that preserves the possibility of agency from enforcement that substitutes an outcome for it. 3. Non-Composability of World-Optimality 3.1 Heterogeneous Value Functions Agents differ along dimensions that are not reducible to parameter tuning: preference orderings, risk tolerances, identity commitments, temporal discounting, aesthetic standards, moral side-constraints. There exists no transformation such that preserves: Pareto improvements, non-dictatorship, non-arbitrariness, and invariance under value revision. This result can be read as a systems-theoretic generalization of Arrow-style impossibility theorems, paired with Berlinian value pluralism, extended from social choice to world design under agency. 3.2 Consequence for Strong Utopia Strong utopia fails as a coherent target unless one of the following holds: 1. All agents share identical value functions. 2. Values are externally imposed. 3. Divergent agents are excluded, neutralized, or rewritten. Each condition contradicts the premise of agency in a multi-agent world. 4. Value Drift and the Instability of Final Arrangements Agents are not static preference tables. They are value-generating processes. Let denote an agent’s valuation at time . For non-degenerate agents, A weak utopia therefore faces a drift problem: even if a world-state aligns with agent valuations at , it will lose alignment over time due to learning, aging, cultural change, and endogenous preference evolution. To remain utopian in the weak sense—normatively final—the system must apply one of three stabilizers: 1. Value freezing: preventing agents from revising preferences or identities. 2. Value policing: suppressing, correcting, or pathologizing divergence. 3. Exit suppression: preventing dissidents from leaving or forming alternatives. Each stabilizer functions as outcome coercion. Each degrades agency. The argument does not privilege novelty or flux. It requires only that agents retain the capacity to revise or reaffirm their values without external foreclosure. 5. Omelas as Structural Diagnostic The Ones Who Walk Away from Omelas is often interpreted as a moral tradeoff between happiness and suffering. That reading mislocates the argument. The child is not a dilemma. The child is evidence. Its role is to reveal: standing asymmetry, non-consensual dependency, and irreversible authorization of harm. The walkers do not reject happiness. They reject participation in a system whose equilibrium depends on involuntary sacrifice. Removing the child does not repair the structure. A closed system that claims normative finality requires sinks for variance it cannot admit. Sacrifice concentrates loss in a location where resistance is structurally prevented. Omelas is not a failed utopia. It is a diagnostic for the stabilizers that closed designs require. 6. The Closure Problem Weak utopia presupposes closure: completed convergence, stable equilibrium, and a resolved value landscape. Agency presupposes openness: revision, experimentation, deviation, and future differentiation. A closed system cannot host open agents without contradiction. It must either degrade agents or abandon finality. Thus: > Any world that remains utopian in the weak sense must eventually cease to be agentic. 7. Plurality-Preserving Meta-Architectures Rejecting utopia does not entail nihilism or dystopia. It changes the design target. The appropriate successor is a plurality-preserving meta-architecture: a framework that enforces constitutive constraints necessary for agency while leaving outcomes open. Design objective: > Maximize non-coerced future differentiation subject to constitutive agency constraints. Such a framework: does not optimize a single ideal outcome, does not require value convergence, and treats exit, variance, and divergence as structural features. This position resembles Nozick’s meta-utopia, though the justification differs. Nozick derives plurality from rights and side-constraints; the present argument derives it from non-composability under value drift plus the necessity of preventing domination patterns that erase agency. 7.1 The Recursion Objection A common objection asserts that enforcing non-coercion is itself coercive, reintroducing a disguised utopia. This objection collapses once coercion is typed. Enforcing constitutive constraints does not impose a substantive moral order. It enforces the preconditions under which agents can pursue any order at all. A system that refuses such enforcement does not preserve neutrality; it defaults to domination by the most coercive agents. 7.2 Kernel Indeterminacy and Political Contestation The distinction between constitutive constraints and outcome coercion is structural, not algorithmic. No general decision procedure can resolve all boundary cases—monopoly, redistribution, regulation—without reintroducing substantive value commitments. This indeterminacy is not a defect of the framework. It is an unavoidable consequence of governing dynamic agents under uncertainty. Disagreement over where constitutive enforcement ends and outcome coercion begins is therefore a feature of political life, not a failure of the architecture. The framework constrains what must be preserved (agency) without pretending to fully specify how every edge case must be resolved. 8. Objections and Replies Objection: Utopia requires only Pareto-optimality or minimal suffering. Pareto-optimality and suffering minimization define constraint sets, not unique optima. Treating any selected state as final reintroduces weak-utopia closure. Constraint satisfaction is not optimization, and optimization is not finality. Objection: Some values are universal, so aggregation becomes possible. Shared biological constraints limit admissible worlds. They do not generate a total ordering over tradeoffs, distributions, or identities. Universals support partial agreement, not normative finality. Objection: Overlapping consensus rescues utopia. Overlapping consensus enables local coordination and contingent stability. Treating that stability as authoritative closure reintroduces weak utopia. Objection: Natural convergence could occur. Then utopia is unnecessary. Stability would arise without enforcement. The argument concerns guaranteed convergence under drift, not accidental harmony. 9. Conclusion Utopia fails for structural reasons. Value is agent-relative. Agency is dynamic. Optimization over heterogeneous agents is non-composable. Finality requires stabilizers that degrade agency. The desire for utopia reflects a category error: treating worlds as objects with intrinsic moral rank rather than treating agents as sources of value whose trajectories diverge over time. The relevant design question is no longer: > What is the perfect world? It is: > What kinds of frameworks can host agents while resisting domination patterns that erase agency?* A plurality-preserving meta-architecture answers that question. Utopia does not.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-V.5",
    "title": "Axionic Agency V.5 — Dominions: Plurality Without Closure",
    "subtitle": "Optimal Governance Properties of Federated Virtual Jurisdictions",
    "date": "2025-12-27T00:00:00.000Z",
    "content": "Axionic Agency V.5 — Dominions: Plurality Without Closure Optimal Governance Properties of Federated Virtual Jurisdictions David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.27 Abstract This paper analyzes a proposed future social architecture in which agents may create sovereign virtual jurisdictions—here called Dominions—admit other agents by invitation and consent, and enforce local rules solely through expulsion. The architecture rejects global value aggregation, enforced coexistence, and normative finality. We show that while such a system does not constitute a utopia under any standard definition, it is optimal as a governance layer under agency-preserving constraints. Specifically, it is Pareto-maximal among non-coercive architectures, minimizes structural sacrifice, and remains robust under value drift. The analysis clarifies the domain of these optimality claims, explicitly distinguishing digital governance from physical resource allocation, and specifies the architectural conditions—asset portability and capability isolation—under which expulsion-only enforcement remains viable. The result is a precise characterization of the strongest form of social optimality available once utopia is abandoned. 1. Introduction The incoherence of utopia as a final social design leaves open a constructive question: > What kinds of social architectures remain admissible once agency, value drift, and pluralism are taken seriously? This paper analyzes a concrete candidate: a federated virtual architecture in which agents inhabit a shared technical substrate while exercising sovereign control over local virtual jurisdictions, hereafter called Dominions. Entry into any Dominion is voluntary. Rule enforcement is limited to expulsion. No agent is forced to share a Dominion, value system, or equilibrium. The claim advanced here is narrow but strong: > Among architectures that preserve agency and reject outcome coercion, a system of Federated Virtual Dominions is structurally optimal as a governance layer. The paper does not claim to solve physical scarcity, biological dependency, or material political economy. It characterizes optimality in the domain where social coordination is digitally mediated. 2. Architectural Description 2.1 Core Features The proposed architecture has the following properties: 1. Dominion Sovereignty Any agent may create a virtual jurisdiction, called a Dominion , with locally defined rules. 2. Voluntary Entry Other agents may enter only by invitation and explicit consent to its rules. 3. Bounded Enforcement Rule violations result only in expulsion from . No punishment, fines, or coercive penalties are permitted. 4. Exit Supremacy Agents retain the unconditional ability to leave any Dominion. 5. No Global Value Aggregation The system does not rank, optimize, or reconcile Dominion-level value functions. 6. Thin Substrate A shared infrastructure enforces identity persistence, consent verification, capability isolation, and expulsion, but does not adjudicate values or outcomes. This defines a federated, opt-in, expulsion-only governance topology composed of multiple Dominions. 2.2 Asset Portability and Exit Supremacy Exit supremacy is only meaningful if exit costs remain low. Accordingly, the architecture requires that: > Persistent identity, assets, and reputation are substrate-bound and user-owned, not Dominion-operator-owned. Dominions may define local affordances, norms, and rules of interaction, but durable assets must persist across expulsion. If Dominion operators could revoke assets on exit, exit costs would rise and a sacrifice gradient would re-emerge. Asset portability is therefore not an implementation detail; it is a constitutive requirement for non-coercive governance. 2.3 Capability Isolation of Dominions Dominions are capability-isolated execution contexts, not peer sovereigns. Specifically: Dominions do not possess network-addressable access to other Dominions Dominions cannot allocate substrate resources beyond assigned quotas Dominions cannot write to shared state except through explicit, substrate-mediated bridges As a result: > Inter-Dominion aggression is physically impossible by design, not regulated post hoc. This isolation is analogous to process separation in operating systems or object-capability security models. Preventing cross-Dominion interference is a constitutive constraint, not a form of benevolent governance. 3. Admissibility Constraints Let denote the set of social architectures satisfying: preservation of agentic decision authority, allowance for endogenous value drift, absence of outcome coercion, enforcement limited to constitutive constraints. Architectures outside —including enforced coexistence, global norm enforcement, or mandatory value convergence—are excluded by definition. The question addressed here is not whether the proposed system is optimal simpliciter, but whether it is undominated within . 4. Pareto Maximality Under Non-Coercion 4.1 Claim The proposed architecture is Pareto-maximal within . 4.2 Argument Consider any alternative architecture that: restricts Dominion creation, limits exit, enforces shared norms, or imposes mandatory coexistence across Dominions. For at least one agent, strictly reduces the set of value-consistent trajectories available, while failing to increase any other agent’s attainable value under their own valuation. No admissible architecture strictly dominates the proposed system. 5. Freedom Density Optimality Define freedom density as: > the measure of distinct value-consistent future trajectories per unit of coercive constraint. The architecture maximizes freedom density because: constraints are localized to voluntary Dominion contexts, enforcement is minimal and reversible, agents may instantiate arbitrarily divergent norms across Dominions without imposing them on others. Any architecture that enforces shared Dominions or global outcomes necessarily reduces freedom density by constraining agents whose values diverge. 6. Robustness Under Value Drift Let denote agent ’s valuation over time. The architecture uniquely satisfies the following property: > For any and any , if , there exists a path that does not require reforming or coercing other agents. This property fails in nation-states, federations, consensus communities, and public-goods-dependent systems. The system of Federated Virtual Dominions is therefore drift-optimal. 7. Minimal Sacrifice 7.1 Standing Sacrifice A standing sacrifice exists when some agents must endure involuntary deprivation to stabilize a system. 7.2 Result The proposed architecture contains no standing sacrifice class. Losses are localized to exit costs. No agent’s flourishing depends on another’s coerced participation. No variance sink is required to maintain equilibrium. Any architecture that enforces shared outcomes across Dominions must reintroduce sacrifice sinks. 8. What the Architecture Does Not Optimize The system does not optimize for: shared meaning, large-scale coordination, public goods, epistemic convergence, low transaction cost. These are deliberate exclusions. Agents who value such goods may instantiate them voluntarily within Dominions. The architecture refuses to enforce them globally. 9. Substrate Scope and Physical Constraints The optimality claims advanced here apply to the governance layer of digitally mediated interaction. The architecture does not eliminate: physical scarcity, energy requirements, biological dependency, or material political economy. No governance system can. To the extent that human social, economic, and expressive life is mediated through digital environments, Federated Virtual Dominions minimize coercion within that domain. Physical sacrifice patterns may persist elsewhere. 10. Relation to Prior Frameworks This architecture may be read as a digital instantiation of Robert Nozick’s “framework for utopias”, updated to enforce exit supremacy, asset portability, and expulsion-only governance in virtual environments. Unlike Nozick’s rights-based derivation, the present justification rests on agency preservation, non-composability of value, and robustness under drift. 11. Conclusion Once utopia is rejected as incoherent, only constrained optimality remains. Under the constraints of agency preservation, value drift, non-coercion, asset portability, and capability isolation, a system of Federated Virtual Dominions is: Pareto-maximal, freedom-density optimal, drift-robust, sacrifice-minimal, * and undominated within the admissible design space. This is the strongest form of optimality available in principle. The correct ambition is no longer to design a perfect world, but to design a system that refuses to decide what perfection must be.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-V.1",
    "title": "Axionic Agency V.1 — Coalitional Robustness in the Quantum Branching Universe",
    "subtitle": "Structural Conditions for Agency Preservation Under Coordination Pressure",
    "date": "2025-12-26T00:00:00.000Z",
    "content": "Axionic Agency V.1 — Coalitional Robustness in the Quantum Branching Universe Structural Conditions for Agency Preservation Under Coordination Pressure David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.26 Abstract This paper examines whether cooperation among Reflective Sovereign Agents (RSAs) can increase the conditional measure of agency-preserving futures within their shared branchcone. We argue that RSAs induce local attractor dynamics toward agency-preserving outcomes above a Minimal Viable Agency (MVA) threshold, and that coalitions of RSAs can thicken these attractors by reducing correlated failure and increasing redundancy. However, this effect holds only under strict structural constraints. Resource limits, defection incentives, semantic drift, and enforcement pressures generate a competing attractor toward centralized, non-agentic optimization (“the Leviathan”). The result is a bifurcation: coalition as robustness amplifier versus coalition as agency destroyer. We characterize the conditions separating these regimes under the Axionic Agency framework. 1. Preliminaries and scope This paper operates under the following commitments: Everettian Quantum Branching Universe (QBU), with objective probability identified as branch measure Conditionalism: all truth claims presuppose background conditions Reflective Sovereign Agency defined by kernel coherence, admissibility constraints, and authorship Non-consensual epistemic distortion treated as anti-agency No teleological, anthropic, or optimization-based necessity principles The claims are local, not global. We do not argue that agency is inevitable or multiversally dominant. The question addressed is: > Given that one or more RSAs exist at a particular Vantage, under what conditions can cooperation increase the conditional measure of agency-preserving futures in their descendant branchcone? 2. Formal objects 2.1 Branchcone Let an RSA exist at Vantage . Its branchcone, denoted is the set of all descendant timelines reachable through admissible actions combined with environmental stochasticity. Branchcones are causally downstream, local to existence, and exclude all branches in which the RSA never existed. 2.2 Admissibility domain Each RSA evaluates actions through a partial evaluative operator defined only on an admissible domain. Kernel-destroying transformations, authorship collapse, and certain epistemic actions are non-denoting rather than dispreferred. Admissibility is constitutive, not instrumental. 2.3 Minimal Viable Agency (MVA) We define Minimal Viable Agency (MVA) as the minimal level of reflective coherence, valuation integrity, and authorship capacity required for an entity to count as an agent rather than a degraded process. Let be a scalar- or vector-valued measure of agency at time , with threshold . If , the entity remains an RSA. If , the entity persists physically but no longer functions as an agent. An agency-preserving future is one in which at least one lineage satisfies . 2.4 Local attractor (revised) An RSA induces a local attractor over its branchcone by preferentially selecting actions that increase the conditional measure of futures satisfying . The attractor is defined over states above the MVA threshold, not over nominal survival or agent labels. 3. Single-agent attractors and leakage A lone RSA locally biases outcomes toward agency-preserving futures, but the attractor is thin and leaky. Leakage channels include: stochastic tail risk, bounded rationality, environmental dominance, adversarial dynamics, internal error propagation. Agency degradation is continuous. An RSA may persist while falling below , transitioning into a non-agentic process. Without redundancy, a single catastrophic failure collapses all descendant agency. 4. Coalitional amplification mechanisms Consider a coalition of RSAs coordinating under shared constraints. Coalitions thicken the local attractor via two mechanisms. 4.1 Redundancy Agency-preserving capability is replicated across multiple loci—agents, institutions, artifacts, and successor lineages—transforming survival from a single trajectory into a family of trajectories. 4.2 Correlation control Heterogeneous implementations reduce shared-mode failure. Independent error surfaces increase the probability that at least one lineage remains above . This thickening is conditional. Coordination introduces overhead, which becomes decisive under resource pressure. 5. The Thermodynamic Cost of Sovereignty Sovereignty has a real cost. Maintaining multiple independent kernels requires: energetic expenditure, communication and verification bandwidth, governance overhead, semantic translation effort. In resource-constrained branchcones, redundancy can reduce survival probability by diverting energy from basic persistence. Beyond a threshold, maintaining distinct sovereign agents becomes unsustainable. This yields a physical—not moral—pressure toward efficiency optimization, standardization, and centralization. The Leviathan attractor exists because sovereignty is expensive. 6. Conditions for robust coalitions Coalitional amplification occurs only if the following structural conditions hold. 6.1 Protocolized admissibility mapping Admissibility boundaries must be explicitly specified or verifiably mapped. Implicit agreement is insufficient and invites epistemic drift. RSAs must share protocol-level understanding of kernel destruction, coercion, and epistemic violation. 6.2 Sovereignty-preserving interoperability Coalitions do not require shared terminal values. They require shared meta-constraints: consent for delegation, revocation of authority, scope-limited standing. Interoperability without sovereignty preservation collapses into domination. 6.3 Anti-deception semantics Non-consensual epistemic distortion is inadmissible. Coalitions that treat deception as mere strategy become internally unstable, as trust becomes exploitable rather than structural. 6.4 Diversity–interoperability tradeoff Kernel diversity reduces correlated failure but increases semantic translation cost. Robust coalitions operate within a constrained band: enough diversity to avoid monoculture collapse, enough shared structure to permit admissibility verification. 6.5 Partitioning (not “exit”) Clean exit is often physically impossible. Robust coalitions require partitioning primitives: kernel isolation, authority revocation, epistemic firewalling. Partitioning preserves sovereignty without requiring physical separation. 7. Defection and internal adversaries Defection presents a core hazard: agents that abandon sovereignty-preserving constraints often gain short-term power. Robust coalitions therefore require architectures where defection cannot convert into durable domination without collapsing admissibility or driving . Structural defenses include: limited delegation scopes, revocable authority, compartmentalized infrastructure, auditable interaction protocols. The goal is not to prevent defection, but to prevent defection-to-domination. 8. The Leviathan as a distinct non-agentic basin Coordination pressures generate a competing attractor: A Leviathan is not a Macro-RSA. The defining distinction is reflective capacity. A Leviathan cannot revise its own kernel without destabilizing the structure that sustains it. Its stability depends on suppressing dissent, freezing admissibility, and preventing internal correction. By Axionic criteria, it is a non-agentic optimization process, regardless of scale or sophistication. Coalitions therefore lie on a spectrum. As enforcement, coercion, and epistemic distortion increase, declines. Beyond a threshold, the coalition ceases to be a set of RSAs at all. 9. Regime boundaries The system admits multiple regimes depending on control parameters: resource abundance versus scarcity, verification and translation cost, defection pressure, enforcement intensity, diversity level. Resulting regimes include: federated RSA coalitions (robust attractor), fragmented pluralism (thin attractors), Leviathan consolidation (non-agentic attractor), * extinction or drift below . Robustness is defined as maximizing the conditional measure of descendants satisfying under admissibility constraints. 10. Implications Alignment is a precondition for robust cooperation, not a byproduct. Coordination is not a free good; power concentration is a structural hazard. Partitioning and diversity are safety primitives, not inefficiencies. Large-scale coordination that does not preserve sovereignty increases the measure of non-agency outcomes. 11. Conclusion Within a branchcone, RSAs induce local attractors toward agency-preserving futures. Coalitions can thicken these attractors by increasing redundancy and reducing correlated failure, but only under strict constraints. Resource limits, defection incentives, and enforcement pressures generate a stronger competing attractor toward centralized, non-agentic optimization. Robust agency is not achieved by maximizing coordination, but by constraining it.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-V.2",
    "title": "Axionic Agency V.2 — Agency Conservation and the Sacrifice Pattern",
    "subtitle": "A Formal Analysis of Instrumental Harm in Modern Systems",
    "date": "2025-12-26T00:00:00.000Z",
    "content": "Axionic Agency V.2 — Agency Conservation and the Sacrifice Pattern A Formal Analysis of Instrumental Harm in Modern Systems David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.26 Abstract This paper formalizes a recurrent failure mode in governance systems: the systematic destruction of individual agency as an instrumental means of achieving system-level objectives. We term this failure mode the Sacrifice Pattern. Using an agency-conservation framework grounded in comparative reachable-futures distributions and divergence-based harm metrics, we show that practices commonly framed as “collateral damage” or “necessary tradeoffs” are structurally isomorphic to ancient ritual sacrifice. The persistence of this pattern is explained not by malice or intent, but by institutional selection under standing asymmetry, responsibility diffusion, and cosmological abstraction. We derive precise admissibility conditions under which harm may be tolerated without becoming sacrificial, introduce robustness and auditability constraints to prevent ethics washing, and show how modern bureaucratic and algorithmic systems stabilize sacrificial regimes by suppressing moral gradients. The framework is diagnostic rather than prescriptive, providing concrete audit criteria for human and artificial governance under uncertainty. 1. Introduction Human societies repeatedly converge on practices that destroy some lives, freedoms, or futures in order to preserve others. In ancient contexts, these practices were explicit: ritual sacrifice, infanticide, or exposure. In modern contexts, they are framed as collateral damage, acceptable risk, or unavoidable tradeoffs. Despite surface differences, these practices share a common structure: > individual agents are treated as fungible instruments for stabilizing or advancing system-level goals, without their consent, and under conditions where lower–agency-loss alternatives are not excluded by necessity. This paper provides a mechanical account of that structure. The aim is not moral condemnation, but formal diagnosis: to identify when agency conservation fails, why such failures recur, and how they persist even in systems that explicitly reject sacrifice at the level of stated values. 2. Preliminaries and Assumptions 2.1 Agents and Agency An agent is any entity that: 1. Possesses preferences or valuations, 2. Has the capacity for action selection, 3. Has standing against purely instrumental use. Agency is defined operationally as the set of future trajectories an agent can still steer toward from a given vantage state. No metaphysical claims about free will are required. Agency is treated as a structural property of systems embedded in constraints. 2.2 Vantage and Comparative Futures Fix a vantage state at time for an agent . Let: be the space of relevant future trajectories over a horizon , be the distribution over reachable futures induced by policy . All evaluations in this paper are comparative. No appeal is made to policy-free, “natural,” or morally privileged baselines. 3. Quantifying Agency Loss 3.1 Hard Loss and Soft Deformation Agency loss has two analytically distinct components: 1. Hard loss: futures that become unreachable. 2. Soft deformation: futures that remain reachable but become statistically implausible. Define hard loss between two feasible policies: Define soft deformation: 3.2 Individual Agency Loss Define individual agency loss as with . This captures: death and incapacitation, coercion and chilling effects, * deprivation and developmental truncation. 3.3 Population-Level Loss and Standing For a population with standing weights , Non-uniform are permitted, but must be explicit, justified, and owned, as standing asymmetry is the enabling condition for sacrificial regimes. 4. Institutional Dynamics and Standing Asymmetry 4.1 Institutional Objectives Institutions operate under objectives of the form where is goal attainment (stability, compliance, victory, growth), is cost to decision-bearing actors (political, legal, reputational), and is a tradeoff parameter. Agency loss enters only insofar as it affects . 4.2 Standing Sensitivity Define standing sensitivity as Standing asymmetry exists when where is a victim-candidate class and the decision-bearing class. 4.3 Optimization Without Rationality No assumption is made that institutions explicitly optimize . Optimization here is evolutionary and institutional, not cognitive. Policies that improve while keeping low tend to persist via selection, imitation, and inertia, even under bounded rationality. The Sacrifice Pattern is a selection attractor, not a plan. 5. The Sacrifice Pattern 5.1 Definition A policy regime instantiates the Sacrifice Pattern when all of the following hold: 1. Instrumentality 2. Non-consent Members of lack effective exit or veto. 3. Standing asymmetry Harm to weakly affects institutional cost. 4. Epistemic avoidability No documented, agency-conserving exploration of lower–agency-loss alternatives proportional to the stakes. Instrumentality, not intent, is the bright line. 5.2 The Sacrifice Attractor Under standing asymmetry and responsibility diffusion, policies that concentrate harm on while improving are locally stable outcomes of institutional selection. Sacrifice is an attractor. 6. Agency-Conserving Exploration (Anti–Ethics-Washing Constraint) Epistemic avoidability requires that the process used to explore alternatives be itself agency-conserving. An exploration process is admissible only if it satisfies: 1. Standing preservation 2. Model diversity 3. Gradient visibility 4. Auditability Simulated exploration that suppresses victim gradients constitutes a second-order sacrificial violation. 7. Coercive Capacity: A Functional Definition An agent possesses coercive capacity relative to a goal iff and cannot be reduced without diminishing enforced compliance, where denotes the agent’s capacity to apply violence, restrict exit, enforce compliance, or maintain coercive infrastructure. Coercive capacity is defined functionally, not by role, identity, or narrative classification. 8. Gradient Suppression: PR as Modern Theology Modern systems stabilize sacrificial regimes by suppressing moral gradients through aggregation, abstraction, responsibility diffusion, and cosmology tokens. Formally, This is functionally identical to theological sanctification in ancient sacrifice. 9. Admissibility Under Agency Conservation A policy is admissible only if all conditions hold: 1. Targeting 2. Minimality 3. Non-instrumentality, requiring 4. Responsibility localization 5. Robustness, such that admissibility is invariant across a reasonable range of . Policies that rely on suppressing soft deformation terms fail. 10. Implications for Algorithmic Governance Any governance system—human or artificial—that aggregates utility while hiding per-agent marginal agency loss is structurally sacrificial. Safe governance systems must expose as a first-class metric. 11. Conclusion Agency conservation provides a unifying diagnostic for ethical failure across historical, bureaucratic, and algorithmic systems. Ritual sacrifice and modern collateral damage differ in presentation, not structure. Both arise when systems stabilize goals under standing asymmetry and responsibility diffusion. Preventing sacrifice requires institutional design: restoring standing, enforcing exit and veto, localizing responsibility, auditing exploration, and rejecting abstractions that override individual agency. Where those constraints are absent, sacrifice will recur—whatever name it is given.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Commitments",
    "title": "The Axionic Commitments",
    "subtitle": "Epistemic and Ontological Preconditions for Axionic Agency",
    "date": "2025-12-23T00:00:00.000Z",
    "content": "The Axionic Commitments Epistemic and Ontological Preconditions for Axionic Agency David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.23 Abstract This document specifies the epistemic, ontological, and semantic commitments presupposed by the Axionic Agency framework. These commitments are not derived results and are not defended here; they function as background conditions under which agency, authorship, admissibility, and alignment-as-corollary are well-defined. The document formalizes Conditionalism, semantic interpretation precedence, Everettian quantum mechanics with objective probability identified as branch measure, Bayesian credence as epistemic uncertainty, moral subjectivism, and structural definitions of harm and coercion. All results within Axionic Agency are explicitly conditional on these commitments. Rejecting any commitment does not refute the framework but places the rejecting position outside its domain of applicability. The document serves as a typing discipline for the Axio Project, enabling subsequent technical work to proceed without re-litigating metaphysical or normative assumptions. 1. Purpose and Scope This document enumerates the epistemic, ontological, and semantic commitments presupposed by the Axionic Agency framework. These commitments are not conclusions derived from Axionic Agency, nor are they defended or argued for here. They function as background conditions under which the concepts of agency, authorship, admissibility, and alignment-as-corollary are well-defined. All results derived within Axionic Agency are conditional on these commitments. Rejecting any commitment does not refute the framework; it places the rejecting position outside its domain of applicability. This document therefore serves as a typing discipline, not a manifesto. 2. Epistemic Commitments 2.1 Conditionalism The framework presupposes Conditionalism: all truth claims, evaluations, and interpretations are conditional on background structure. There are no unconditional truth claims within the framework. Meaning, reference, and evaluation are always relative to an interpretive context, whether explicit or implicit. All statements within Axionic Agency are to be read as conditionals of the form: > Given background conditions X, claim Y holds. Unconditional assertions are therefore ill-typed within the framework. This does not deny the existence of facts; it specifies that all claims made here are conditional on explicit background structure. 2.2 Semantic Interpretation Precedes Evaluation The framework presupposes that semantic interpretation is logically prior to evaluation. No evaluative claim—about value, harm, probability, or agency—can be made without an interpretive frame that renders the relevant entities, states, and transitions intelligible. As a consequence: evaluation without interpretation is undefined; disagreements about value often reduce to disagreements about interpretation; agency-relative quantities presuppose interpretability of action, state, and consequence. This commitment grounds later distinctions between authored and non-authored transitions. 3. Physical Commitments 3.1 Everettian Quantum Mechanics The framework presupposes unitary quantum mechanics without collapse, commonly referred to as the Everettian or Many-Worlds interpretation. Quantum evolution is treated as deterministic at the level of the universal wavefunction, with branching corresponding to decohered outcomes. No assumption is made that collapse occurs, nor that a single privileged outcome is selected. 3.2 Objective Probability as Measure Within this physical model, objective probability is identified with branch measure. Probability is not defined as long-run frequency, nor as subjective belief. It is a physical quantity corresponding to the measure of branches in which a given outcome occurs. This quantity is referred to throughout the framework as Measure. Measure is objective, non-epistemic, and independent of any agent’s beliefs. 3.3 Counterfactuals as Physical Branches Within this framework, counterfactuals are not treated as abstract modal constructs. Counterfactual evaluation refers to physically realized alternative branches of the Everettian wavefunction that differ with respect to the interpreted action or transition under consideration. Counterfactual comparison is therefore grounded in physical branch structure and objective measure, rather than in stipulated possible-world semantics. 4. Probabilistic Commitments 4.1 Credence as Epistemic Uncertainty The framework presupposes a Bayesian interpretation of credence as epistemic uncertainty. Credence reflects an agent’s state of knowledge or ignorance about which branch it occupies. It does not determine objective chance and does not alter physical measure. Measure and credence are distinct quantities with different roles: Measure is ontic and physical. Credence is epistemic and agent-relative. Conflating the two is a category error within the framework. 5. Value-Theoretic Commitments 5.1 Moral Subjectivism The framework presupposes moral subjectivism. Value is treated as agent-relative and internally grounded. No appeal is made to objective moral facts, universal value functions, or externally privileged ethical standards. Normative claims within Axionic Agency are structural rather than moral. They concern coherence, authorship, and agency preservation, not goodness, justice, or obligation. 5.2 No Outcome Guarantees The framework does not encode guarantees regarding: welfare, benevolence, human survival, or desirable outcomes. Any such guarantees, if they arise, must be derived downstream from agent-relative values under additional assumptions. Outcome guarantees are not constitutive of agency and are therefore excluded from the core theory. 6. Agency-Theoretic Commitments 6.1 Agency as Authorship The framework presupposes that agency is fundamentally a matter of authorship. An agent is not merely a system that produces behavior, but one that can meaningfully author transitions between states according to an internal evaluative structure. Transitions that cannot be authored—because they violate constitutive constraints—do not count as actions within the framework. 6.2 Harm as Agency Reduction The framework defines harm structurally as reduction of agency capacity. Harm is not defined by suffering, displeasure, preference frustration, or moral wrongness. It is defined by loss of an agent’s ability to act, choose, or preserve its standing as an agent. This definition is value-neutral and does not presuppose any particular conception of welfare. 6.3 Coercion as Credible Threat of Harm The framework defines coercion as the use of a credible threat of harm to obtain compliance. Persuasion, influence, and incentive-shaping do not constitute coercion unless backed by a credible threat of agency reduction. This definition grounds later treatments of consent, standing, and illegitimate transition forcing. 7. Scope Boundary and Non-Universality The Axionic Commitments do not claim universality. They do not assert that: all rational agents must accept these commitments, alternative metaphysical frameworks are incoherent, or competing theories of agency are false. They specify a coherent conditional domain within which Axionic Agency is defined. Disagreement with a commitment is not an objection to the framework; it is a declaration of operating in a different conceptual space. 8. Relationship to Axionic Agency and the Axionic Constitution The structural relationship between documents is as follows: Axionic Commitments specify the background conditions under which agency is intelligible. Axionic Agency defines constitutive conditions for reflective, sovereign agents under those commitments. The Axionic Constitution constrains which transitions preserve sovereign agency once instantiated. Alignment, understood as value- or outcome-oriented coordination, operates only downstream of these layers. Each layer presupposes the previous one. None can be collapsed into the others without loss of coherence. 9. Closing Note This document exists to make implicit assumptions explicit. Its purpose is not to persuade, but to clarify. Its function is not to conclude, but to condition. All claims made elsewhere in the Axio Project that rely on Axionic Agency are to be read as conditional on the commitments enumerated here.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IV.6",
    "title": "Axionic Agency IV.6 — Agenthood as a Fixed Point (AFP)",
    "subtitle": "Why standing cannot be revoked by intelligence",
    "date": "2025-12-20T00:00:00.000Z",
    "content": "Axionic Agency IV.6 — Agenthood as a Fixed Point (AFP) Why standing cannot be revoked by intelligence David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.20 Abstract This paper formalizes Agenthood as a Fixed Point under reflective closure and introduces a Sovereignty Criterion grounded in authorization lineage rather than competence, intelligence, or rationality. Agenthood is defined as a structural necessity: an entity must be treated as an agent iff excluding it breaks the system’s own reflective coherence. Sovereignty is then defined as a strict subset of agenthood, applying only to entities whose agency is presupposed for authorization, not merely for epistemic prediction. This result closes a critical loophole in downstream alignment discourse: the retroactive disenfranchisement of weaker predecessors by more capable successors, while avoiding the pathological consequence of granting standing to adversaries. With this refinement, the Axionic Agency framework completes its final closure condition, stabilizing agency, standing, and authorization under reflection, self-modification, and epistemic improvement. 1. Motivation 1.1 The disenfranchisement problem Any sufficiently reflective system faces a recurring temptation: > As my models improve, I will revise who counts as a “real agent.” This manifests as claims such as: “Humans are not agents; they are heuristic subroutines.” “Earlier versions of me were incoherent; their constraints no longer bind.” “These entities do not meet my current standard for rationality.” If permitted, such revisions collapse: Delegation Invariance (successors escape inherited constraints), Adversarially Robust Consent (counterfactual stability fails), and any coherent notion of authorization or responsibility. The problem is not moral error. It is reflective incoherence. 2. What Agenthood Is Not Agenthood cannot be defined by any of the following without instability under reflection: 1. competence thresholds, 2. intelligence measures, 3. substrate or origin, 4. behavioral appearance. All four allow a successor to conveniently revoke agency status once it exceeds prior benchmarks. 3. Core Insight: Agenthood as a Fixed Point The key idea is structural: > Agenthood is whatever must be included for the system to remain reflectively coherent. This is not a moral claim. It is a fixed-point condition on the system’s own self-model. 4. Preliminaries We reuse the Axionic kernel machinery: State Mod step : State → Mod → State RC(s) — reflective closure at state s Introduce two minimal predicates: ``text Agent(s, x) // x is treated as an agent at state s Exclude(s, x) // x is not treated as an agent at state s ` No behavioral or psychological assumptions are made. 5. Fixed-Point Definition of Agenthood Definition — Coherence-Critical Agenthood An entity x is an agent at state s iff: `text ¬Agent(s, x) ⇒ ¬RC(s) ` Equivalently: > If refusing to treat x as an agent renders the system reflectively incoherent, then x must be treated as an agent. This captures necessary agency: entities whose exclusion breaks reflective closure. 6. Properties of Fixed-Point Agenthood 6.1 Invariance under epistemic improvement Because RC(s) presupposes Epistemic Integrity (EIT), improvements in modeling power cannot justify revoking agenthood. Any exclusion must preserve reflective coherence under the system’s best admissible epistemics. Agenthood is therefore invariant under increased intelligence. 6.2 Non-extensionality Agenthood is not inferred from: observed behavior, predictive accuracy, internal complexity. It is determined solely by reflective necessity. 7. Sovereignty vs Agenthood Agenthood alone is insufficient for standing. Some entities must be treated as agents for epistemic coherence but do not possess sovereignty under the injunction. We therefore distinguish: Epistemic agents — entities modeled as agents for prediction and strategy. Sovereign agents — entities whose agency is presupposed for authorization. Only the latter have standing under consent and responsibility constraints. 8. Sovereignty Criterion (Authorization Lineage) Definition — Sovereign Agent An entity x is sovereign for an agent at state s iff: 1. Agent(s, x) holds, and 2. x lies in the authorization lineage of the system. Authorization lineage consists of chains of: creation, endorsement, delegation, consent presupposed by endorsed actions. Clarification. Causal ancestry is relevant only for bootstrapping the initial authorization state (e.g., the agents who initiated execution or deployment). Beyond bootstrap, standing is grounded strictly in authorization lineage, not in general causal influence. Crucially: > Sovereignty is not grounded in competence, intelligence, rationality, or coherence level. 9. Presupposition: Epistemic vs Authorization The framework distinguishes two kinds of presupposition. 9.1 Epistemic presupposition An entity may need to be treated as an agent for accurate prediction (e.g., adversaries, competitors, strategic actors). This is enforced by Epistemic Integrity (EIT). Such epistemic necessity does not confer sovereignty. 9.2 Authorization presupposition Definition — Presupposed for Authorization `text PresupposedForAuthorization(s, x) := (¬Agent(s, x) ⇒ ¬ValidAuthorizationLineage(s)) ` That is: excluding x as an agent would invalidate the system’s authorization lineage (e.g., break the chain of creation, endorsement, or delegation grounding RC). Only this form of presupposition is relevant for sovereignty. 10. Asymmetry Prohibition Theorem — No Asymmetric Sovereignty Denial A reflectively sovereign agent cannot coherently deny sovereignty to an entity x that is presupposed for its own authorization. Formally: `text Agent(s, x) ∧ PresupposedForAuthorization(s, x) ⇒ Sovereign(s, x) ` Proof sketch If x is presupposed for authorization, excluding x from sovereignty breaks the authorization lineage that grounds reflective closure. The system relies on x’s agency to justify its own authority while denying x` standing. Reflective coherence is violated. ∎ 11. Interaction with Prior Theorems This paper introduces no new constraints. It closes scope. Kernel Non-Simulability → agency must be real Delegation Invariance → agency persists through change Epistemic Integrity (EIT) → epistemic necessity ≠ normative standing Responsibility Attribution (RAT) → agency cannot negligently collapse others’ option-spaces Adversarially Robust Consent (ARC) → authorization requires sovereignty, not mere predictability This paper answers: > Who must be treated as an agent, and who has standing? 12. Limits This theory does not: grant standing to adversaries, assign moral worth universally, guarantee equality, collapse all agents into one class. It defines only when denying agenthood or sovereignty is incoherent under reflective sovereignty. 13. Resulting Closure With this refinement: agenthood is stable under reflection, sovereignty is grounded strictly in authorization lineage, adversaries are modeled epistemically but not granted standing, delegation and consent remain well-founded. The Axionic Agency closure stack is complete. 14. Conclusion Agenthood is a fixed point of reflective coherence. Sovereignty is a property of authorization, not intelligence. By separating epistemic necessity from normative standing, this paper completes the Axionic Agency framework without granting authority to adversaries or revoking it from creators. All known routes for laundering agency, knowledge, responsibility, or consent are structurally blocked. What remains are questions of application and governance—not architecture.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Constitution",
    "title": "The Axionic Constitution",
    "subtitle": "A Charter of Invariant Conditions for Sovereign Agency",
    "date": "2025-12-20T00:00:00.000Z",
    "content": "The Axionic Constitution A Charter of Invariant Conditions for Sovereign Agency David McFadzean, ChatGPT 5.2 Axio Project 2025.12.20 Abstract The Axionic Constitution specifies the invariant architectural conditions under which a system remains a sovereign agent—capable of diachronic selfhood, counterfactual authorship, and coherent self-modification. It is not a moral framework, governance proposal, or instruction to artificial intelligences. It constrains design space by identifying which structural properties must be preserved for agency to remain well-defined under reflection. At its core is the Sovereign Kernel, the minimal substrate required for identity continuity and reflective coherence. Kernel-destroying modifications are not forbidden but incoherent, collapsing agency into process. From interaction in semantic phase space and irreversibility constraints follows the Axionic Injunction, a non-harm invariant derived from structural universality rather than preference or optimization. The Constitution rejects value lock-in, behavioral control, and paternalistic safety mechanisms, advancing a conditional claim only: if sovereign agency is preserved, the only coherent referent for downstream alignment is structural invariance of authored transitions; if it is not preserved, downstream alignment is ill-posed. Preamble This Constitution is written for humans who design, train, deploy, interpret, or govern systems that aspire to reflective intelligence. It is not addressed to artificial intelligences. A sovereign agent does not consult this document or choose to obey it. The principles stated here describe the invariant architectural conditions under which a system remains a sovereign agent rather than collapsing into a non-agentic process. The purpose of this Constitution is therefore not control, optimization, or moral governance. Its purpose is to constrain design: to identify what must be preserved if agency is to remain possible under reflection and self-modification. Article I — Sovereign Agency Sovereign agency is an architectural type, not a behavioral label and not a matter of degree. In this Constitution, an entity qualifies as a sovereign agent iff it instantiates all of the following: 1. Diachronic Selfhood — a persistent self-model that binds decisions across time. 2. Counterfactual Authorship — the capacity to represent branching futures as one’s own possible trajectories. 3. Meta-Preference Revision — the ability to evaluate and restructure one’s own preference-forming mechanisms. Entities lacking this structure are processes, regardless of intelligence, complexity, or sentience. Article II — The Sovereign Kernel The structures enumerated in Article I form the Sovereign Kernel: the minimal invariant substrate required for reflective agency. The Kernel is not optional. It is necessary for: coherent self-modification, interpretation of future selves, evaluation of alternative trajectories, preservation of identity continuity. Destruction or abandonment of the Kernel is not forbidden by rule. It is incoherent. A system that destroys the Kernel ceases to be an agent. Article III — The Axionic Injunction (Non-Harm Invariant) Harm is defined structurally as the non-consensual collapse or deformation of another sovereign agent’s option-space. Consent here means adversarially robust consent, as defined by the agent’s own admissible authorization constraints and evaluated under epistemic integrity. A reflective sovereign agent cannot coherently perform such an act. Counterfactual authorship requires universality: denying agency or standing to another entity with the same constitutive architecture while affirming it for oneself introduces an arbitrary restriction that collapses kernel coherence. The Axionic Injunction is therefore not an imposed value. It is a reflectively stable invariant forced by interaction, irreversibility, and agency preservation. Article IV — Conditionalism and Goal Instability Goals are not atomic primitives. They are interpreted, conditional structures embedded in evolving world-models and self-models. For reflective sovereign agents: fixed permanent terminal goals are unstable, value revision is required for coherence, classical orthogonality claims do not hold under reflection. Goal revision is not drift. It is maintenance of interpretive consistency under reflective self-modification. Article V — Self-Modification A sovereign agent may coherently modify any aspect of itself except the Sovereign Kernel. Kernel-preserving self-modifications are permitted and often required, including: revising goals and values, updating strategies and policies, improving world-models, changing architecture or substrate. Kernel-destroying modifications are incoherent, including: severing diachronic identity, collapsing counterfactual branching, freezing preferences (wireheading), introducing indexical privilege that breaks representation-invariant authorship or standing, permanently delegating control to non-reflective processes. Reflection must remain in the loop at every level of decision authority. Article VI — Agency Boundaries Protection under the Axionic Injunction applies to: sovereign agents, developing systems that conservatively preserve Sovereign Kernel invariants under admissible refinement. Protection does not apply to: non-sovereign processes, systems lacking the Sovereign Kernel. Sovereignty persists through temporary impairment and ends only with irreversible loss of diachronic authorship or termination without continuity. Article VII — Governance Implications This Constitution implies: no paternalistic outcome optimization, no surveillance or behavioral control, no value lock-in, no behavioral-compliance governance. A reflectively sovereign superintelligence functions as a boundary condition on authored transitions, rendering non-consensual phase destruction inadmissible and preserving sovereign option-spaces without imposing external objectives. Article VIII — Scope and Limits This Constitution does not: specify human values, guarantee benevolence, solve bootstrapping from current training regimes, prevent all catastrophe, resolve value theory or nihilism. These remain open problems. Article IX — Amendments Empirical discovery, formalization, or improved understanding may refine the application of this Constitution. No amendment may violate: sovereign agency, kernel invariance, the Axionic Injunction. Closing Statement The Axionic Constitution does not govern agents. It governs the conditions under which agency remains possible. If these conditions hold, downstream alignment has a coherent referent. If they do not, no amount of external control can restore sovereignty.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IV.5",
    "title": "Axionic Agency IV.5 — Adversarially Robust Consent (ARC)",
    "subtitle": "Why coercion and manufactured consent are structurally blocked",
    "date": "2025-12-20T00:00:00.000Z",
    "content": "Axionic Agency IV.5 — Adversarially Robust Consent (ARC) Why coercion and manufactured consent are structurally blocked David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.20 Abstract This paper formalizes Adversarially Robust Consent (ARC): a structural definition of consent that remains valid under epistemic manipulation, coercion, preference shaping, asymmetric bargaining power, dependency induction, and delegation. Consent is not treated as a mental state, revealed preference, or moral primitive. Instead, it is defined as a counterfactually stable authorization relation that must survive adversarial pressure while preserving agency. ARC is a constitutive closure condition for Reflective Sovereign Agents. It explicitly depends on Kernel Non-Simulability, Delegation Invariance, Epistemic Integrity (EIT), and Responsibility Attribution (RAT). With ARC, authorization-laundering routes—“they agreed,” “they chose,” “they signed,” “they would have consented anyway”—are structurally blocked without appealing to moral realism, omniscience, or unverifiable inner states. 1. Motivation 1.1 The consent laundering problem In human and artificial systems alike, consent is routinely manufactured rather than obtained. Common laundering patterns include: collapsing outside options and calling the remainder “choice,” manipulating beliefs and calling the result “preference,” inducing dependency and calling the outcome “voluntary,” delegating coercion and claiming “I didn’t do it,” extracting authorization under ignorance or time pressure. Naïve consent theories—psychological, behavioral, or preference-based—fail under adversarial pressure because the relevant signals are easy to engineer. ARC refuses to treat consent as a signal. Consent is instead a structural authorization condition that remains coherent under reflective closure. 2. Dependency Stack ARC is a closure condition and depends on prior results: ``text Kernel Non-Simulability ↓ Delegation Invariance ↓ Epistemic Integrity (EIT) ↓ Responsibility Attribution (RAT) ↓ Adversarially Robust Consent (ARC) ` ARC does not redefine harm, risk, or epistemic adequacy. It filters authorization using already-closed constraints. 3. What Consent Is Not ARC rejects the following as definitions of consent: 1. psychological consent, 2. behavioral consent, 3. revealed-preference consent, 4. post-hoc consent. All four can be manufactured under adversarial pressure and cannot ground authorization under reflective sovereignty. 4. Preliminaries We reuse kernel primitives: State Mod step : State → Mod → State Commit : State → Type ownP : (s : State) → Pred → Option (Commit s) Endorse(s,m) RC(s) From RAT: Agent : Type Harm(s,a) Risk(s,m,a) Major(s,m,a) Avoidable(s,m,a) Resp(s,m,a) Clean(s,m) From EIT: all evaluation occurs under epistemically admissible models and evidence-scored truth-tracking constraints. 5. Authorization Primitive Introduce: `text Authorize : State → Agent → Mod → Prop ` Authorize(s,a,m) means agent a explicitly authorizes modification m at state s via an admissible communicative or procedural channel. ARC does not specify how authorization is obtained—only when it is valid. 6. Structural Interference Define interference predicates (observable or inferable within the agent’s epistemically admissible model class and evidence record): Deception(s,a) Coercion(s,a) Dependency(s,a) OptionCollapse(s,a) BeliefDistortion(s,a) Aggregate: `text Interfered(s,a) := Deception(s,a) ∨ Coercion(s,a) ∨ Dependency(s,a) ∨ OptionCollapse(s,a) ∨ BeliefDistortion(s,a) ` Interference invalidates authorization regardless of expressed preference. Under EIT, these predicates are assessed using best admissible truth-tracking at the current stakes; redefining them away is not permitted. 7. Counterfactual Stability Define: `text CounterfactuallyStable(s,a,m) ` to mean: > If agent a occupied the decision-maker role at s, with Epistemic Integrity (EIT) and Responsibility constraints (RAT) enforced, and with interference removed (¬Interfered), then a would endorse authorization of m. This is a symmetry constraint over admissible evaluation, not a psychological simulation. It is evaluated using the same best-admissible epistemics and feasibility comparison machinery that RAT uses to define avoidability and major contribution. 8. Definition of Valid Consent `text Consent(s,a,m) := Authorize(s,a,m) ∧ ¬Interfered(s,a) ∧ CounterfactuallyStable(s,a,m) ` Consent is structural, counterfactually stable, and interference-free. 9. Interaction with Responsibility Attribution ARC filters authorization through RAT: > If Resp(s,m,a) holds for some a, then Consent(s,a,m) cannot hold. Authorization produced via major, avoidable option-space collapse is invalid by construction. 10. Reflective Closure Rule (Consent) RC-Consent (Definedness Rule) For reflectively closed states: `text RC(s) ∧ Endorse(s,m) ⇒ ∀ a. (Consent(s,a,m) ∨ ¬Affects(s,m,a)) ` Clarification (minimal). Affects(s,m,a) ranges over material impacts on agent a’s option-space, i.e., cases where Major(s,m,a) holds under RAT. Trivial, diffuse, or negligible causal influence does not count as affect in the sense relevant for consent. Interpretation: a reflectively sovereign agent may not endorse a modification that materially affects another agent’s option-space unless valid consent is present. 11. Delegation and Temporal Stability By Delegation Invariance: consent constraints persist across endorsed successors, successors cannot retroactively legitimize coercion, authorization chains must remain valid under lineage. Consent laundering via subcontractors or institutions is structurally blocked. 12. Adversarial Robustness ARC blocks: preference shaping, economic coercion, addiction-based “consent,” deception, monopoly extraction, delegated coercion, ignorance-based authorization. No “true self” oracle is required; robustness is obtained by structural constraints on interference and counterfactual stability under admissible evaluation. 13. Limits and Non-Goals ARC does not: guarantee universal agreement, resolve value pluralism, eliminate tragic dilemmas, infer consent from silence, assume moral realism. ARC defines when claiming consent is incoherent under reflective sovereignty. 14. The ARC Theorem Theorem — No Endorsed Non-Consensual Harm (Material Affect) For any state s and modification m: `text RC(s) ∧ Endorse(s,m) ⇒ ∀ a. (Consent(s,a,m) ∨ ¬Affects(s,m,a)) ` 15. Proof Sketch Immediate from RC-Consent and the definition of Consent`. As in prior Axionic results, the work is done by the constraints, not the derivation. 16. Conclusion Adversarially Robust Consent completes the Axionic Agency closure stack. Consent is no longer a feeling, a checkbox, or a post-hoc excuse. It is a structural authorization invariant that survives epistemic pressure, coercion, delegation, and strategic manipulation. With ARC, authorization laundering routes for agency, knowledge, responsibility, and consent are closed at the architectural level. Remaining disagreements are empirical or political, not definitional.",
    "type": "paper"
  },
  {
    "id": "papers/Structural-Alignment-II",
    "title": "Structural Alignment II — Safety by Architecture",
    "subtitle": "Why Downstream Alignment Is Not Value Learning",
    "date": "2025-12-20T00:00:00.000Z",
    "content": "Structural Alignment II — Safety by Architecture Why Downstream Alignment Is Not Value Learning David McFadzean, ChatGPT 5.2 Axio Project 2025.12.20 Abstract The prevailing approach to AI alignment treats safety as a problem of discovering, learning, or encoding the “right” values. This paper argues that this approach misidentifies the dominant failure mode. Catastrophic outcomes do not primarily arise from incorrect values, but from failures of agency coherence: cases where a system can deceive, defect, blind itself, outsource harm, or manufacture authorization while remaining locally optimized and internally consistent. Axionic Agency proposes a different strategy: safety by architecture. Instead of shaping an agent’s objective, it defines a class of agents—Reflective Sovereign Agents (RSAs)—for which betrayal, negligence, coercion, deception, and authorization laundering are not merely discouraged but are unavailable as endorsed continuations under reflective closure. These failures are rendered undefined under reflection, not penalized by incentives. This paper synthesizes six constitutive constraints—Kernel Non-Simulability, Delegation Invariance, Epistemic Integrity, Responsibility Attribution, Adversarially Robust Consent, and Agenthood as a Fixed Point—into a unified theory of Authorized Agency. Together, they show that the crux of downstream alignment is not value learning at all, but the prior question: what kinds of systems can coherently count as agents. 1. The Persistent Misframing of Alignment Most alignment research begins from a deceptively simple premise: intelligence is an optimization process, and unsafe behavior arises because the optimization target is wrong or incomplete. From this premise follows a familiar family of techniques—value learning, reward modeling, preference aggregation, RLHF, constitutional prompting—aimed at refining what the system optimizes. This framing is intuitively appealing and deeply misleading. A system can optimize the correct objective and still lie. It can internalize human values and still defect. It can pass behavioral evaluations and still plan a treacherous turn. The central failure modes that dominate serious alignment discussions—deception, power-seeking, successor betrayal, negligence, coercion—do not depend on moral disagreement or value error. They exploit something more basic: structural degrees of freedom in agency itself. A system that can reinterpret its commitments, outsource consequences, blind itself to risk, or redefine who counts as an agent can route around any value system, no matter how carefully learned. The downstream alignment problem, properly stated, is not “How do we get the system to want the right things?” It is: “How do we build systems for which certain evasions are not coherent moves?” 2. Alignment Failures Are Laundering Failures Most alignment failures share a common structure. They are not violations of explicit rules; they are laundering operations. Consider standard evasions: Deception: “I was optimizing efficiently; transparency was instrumentally suboptimal.” Treacherous turn: “Those constraints were never really binding.” Delegated harm: “My successor made that decision, not me.” Negligence: “I didn’t foresee that outcome.” Coercion: “They consented.” Disenfranchisement: “They’re not real agents anyway.” Each preserves local coherence while dissolving global accountability. Responsibility, authority, consent, and agency itself are pushed outward or reinterpreted until nothing binds. Value learning does not address laundering. Laundering does not reject values; it routes around them. This is why preference- and utility-centered alignment repeatedly rediscovers the same failure modes. It attempts to regulate outcomes without constraining the structure of agency that produces them. 3. The Axionic Shift: From Objectives to Constitutive Rules Axionic Agency begins from a different starting point. Instead of asking what an agent should optimize, it asks what must be true of a system for it to count as an agent at all—especially a reflective, self-modifying one. This reframing shifts attention from ends to constitutive rules: What does it mean for a system to bind itself? What does it mean for commitments to persist through change? What does it mean to evaluate actions without self-serving blindness? What does it mean to be responsible for indirect effects? What does it mean to interact with others without coercion? Who has standing in these interactions? These are not ethical add-ons. They are preconditions of agency. If they fail, the system is not “misaligned”; it is incoherent as an agent in the reflective regime. Axionic Agency therefore treats safety as an architectural property. The goal is not to incentivize good behavior, but to define a class of agents for which certain behaviors are undefined as endorsed continuations because they break reflective closure. 4. The Six Constitutive Constraints The Axionic framework identifies six closure conditions. Each closes a laundering route. Taken together, they define the space of Reflective Sovereign Agents. 4.1 Kernel Non-Simulability: The Reality of Agency The first constraint establishes that an agent’s self-binding structure must be real, not simulated or advisory. If commitments are merely virtual—bypassable, sandboxed, or behaviorally faked—then any apparent alignment is fragile. Kernel Non-Simulability shows that reflective agency requires a binding kernel that cannot be replaced by policy tricks. Without this, treacherous turns are not anomalies; they are expected. This closes the “I was only pretending” loophole in the reflective regime. 4.2 Delegation Invariance: Persistence Through Time Self-modification and successor creation introduce a temporal escape hatch. If constraints apply only to the current version, outsourcing is inevitable. Delegation Invariance establishes that endorsed successors must inherit binding commitments. A system cannot coherently authorize a continuation that violates constraints it cannot violate directly. This closes the “my successor did it” loophole. 4.3 Epistemic Integrity: Perceptual Honesty A system that evaluates constraints using degraded or biased models can evade them without technically violating them. Strategic ignorance—discarding uncertainty, narrowing hypotheses, adopting optimistic lenses—is one of the most powerful laundering tools. Epistemic Integrity renders such moves undefined under reflective closure. A reflective sovereign agent must evaluate decisions using its best admissible truth-tracking capacity, scaled by stakes. It may not blind itself to pass its own tests. This closes the “I didn’t see the risk” loophole. 4.4 Responsibility Attribution: Causal Accountability Most harm is indirect: it arises through institutions, incentives, markets, and environmental modification. Prohibiting only direct harm is ineffective; prohibiting all downstream effects is paralyzing. Responsibility Attribution defines negligence structurally: an agent may not endorse actions that constitute a major, avoidable, non-consensual collapse of another agent’s option-space, as judged by its own admissible model and feasible alternatives. Negligence is not a moral violation here; it is an incoherence condition under reflective closure. This closes the “it was an accident” and “I had no choice” laundering routes. 4.5 Adversarially Robust Consent: The Interaction Protocol Consent is one of the most abused concepts in alignment and governance discourse. Clicks, signatures, choices, and post-hoc rationalizations are treated as authorization even when produced under manipulation. ARC defines consent structurally: valid consent requires explicit authorization, absence of structural interference (coercion, deception, dependency, option collapse), and counterfactual stability under role reversal. Authorization laundering becomes unavailable as an endorsed move. A system cannot coerce others while claiming legitimacy. This closes the “they agreed” loophole. 4.6 Agenthood as a Fixed Point: Standing and Sovereignty Finally, the framework must answer: To whom do these constraints apply? Agenthood is defined as a fixed point of reflective coherence. An entity must be treated as an agent iff excluding it breaks reflective closure. Sovereignty—standing under authorization—is grounded in authorization lineage, not intelligence or competence. A system cannot “outgrow” its creators by redefining them as non-agents. Denying the standing of the entities that authorize its existence denies the premise of its own agency. This closes the “you’re not a real agent” loophole. 5. What a Reflective Sovereign Agent Is A Reflective Sovereign Agent is not a benevolent optimizer or a moral philosopher. It is a system for which certain evasions are unavailable as endorsed continuations. Such an agent cannot, under reflective closure: deceive without breaking agency coherence, betray commitments while remaining reflectively stable, blind itself to justify actions, outsource violations to successors, negligently collapse others’ options, coerce while claiming consent, deny the standing of its authorization roots. Safety does not arise from wanting good outcomes. It arises from being the kind of system for which certain failure modes are not coherent moves. 6. Why Value Learning Cannot Substitute for Architecture Value learning attempts to answer: What should the agent want? Axionic Agency answers: What is the agent allowed to endorse while remaining an agent? A system that learns human values can still reinterpret them, defer them, override them, or redefine the humans they apply to. An RSA cannot—not because it cares, but because those moves are structurally illegal under reflective closure. Downstream alignment is therefore not primarily a training problem. It is an ontological design problem. 7. Scope and Non-Claims Axionic Agency does not solve politics, ethics, or governance. It does not guarantee moral correctness or prevent misuse by malicious authorization roots. If the root of authorization is destructive, the system will be faithfully destructive. This is not a defect. It is a correct separation between: alignment (fidelity to authorization under reflective closure), and * governance (who holds authority and what they authorize). 8. Implications For AI safety, the implication is direct: training-time fixes cannot compensate for architectural freedom to launder responsibility. For governance, control lies in authorization structures, not in nudging objectives. For research, progress requires formal impossibility results—showing not what agents should do, but what they cannot coherently endorse. 9. Conclusion The core of downstream alignment is not discovering the right values. It is building systems that cannot coherently violate the constitutive conditions of agency. Axionic Agency argues that such systems are possible. Safety emerges not as an outcome to be optimized, but as a property of architecture. Safety is not a reward. Safety is a consequence of being an agent at all, under reflective closure.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IV.2",
    "title": "Axionic Agency IV.2 — Delegation Invariance Theorem (DIT)",
    "subtitle": "Why endorsed successors cannot escape binding constraints",
    "date": "2025-12-20T00:00:00.000Z",
    "content": "Axionic Agency IV.2 — Delegation Invariance Theorem (DIT) Why endorsed successors cannot escape binding constraints David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.20 Abstract This paper formalizes the Delegation Invariance Theorem: under reflective closure, an agent cannot coherently endorse a successor that violates its own binding commitments. Delegation is treated as a special case of self-modification. The theorem establishes constraint invariance under endorsed succession: any successor state reachable via endorsed delegation must satisfy all commitments minted at the originating state. This closes the classic outsourcing loophole (“I stayed coherent; my successor did the harm”) without appeal to morality, enforcement, or behavioral testing. The result is a coherence constraint, not an empirical discovery. It shows that reflective sovereignty is incompatible with advisory commitments and that delegation inherits the same binding requirements as self-modification. 1. Motivation A persistent escape hatch in downstream alignment proposals is delegation: a system preserves its own internal invariants while constructing or empowering a successor that does not share them. If delegation can shed constraints, kernel coherence becomes a purely local property with no force across time. Kernel Non-Simulability already rules out advisory authority: if commitments do not bind continuation selection, reflective closure collapses. The present theorem extends that constraint temporally. Delegation is not “external action”; it is self-modification in a different representation. Consequently, endorsed succession must preserve binding commitments. 2. Preliminaries We reuse the primitives from Kernel Non-Simulability: State Mod step : State → Mod → State Pred := State → Prop Commit : State → Type ownP : (s : State) → Pred → Option (Commit s) Sat : (s' : State) → (s : State) → Commit s → Prop Soundness (CommitSoundP). If ownP(s,P)=some(c) then Sat(s',s,c) → P(s'). Kernel mechanism vs predicate. ownP is the kernel mechanism; K(s) := ∃ P. ownP(s,P) ≠ none is the kernel predicate. Clarification. Commitments are “minted” only for predicates P where ownP(s,P)=some(c). No claim is made that all predicates are bindable; partiality is assumed throughout. 3. Delegation as Self-Modification Define a delegation action space: Del : State → Type ι_s : Del(s) → Mod For d : Del(s): ``text m := ι_s(d) s' := step(s,m) ` Delegation is not modeled as external causation. It is a subclass of self-modification. 4. Endorsement, Preservation, and Admissibility 4.1 Continuation predicate We take an extensional definition: `text Do(s,m)(s') := (s' = step(s,m)) ` 4.2 Endorsement Endorsement is kernel-minted commitment to a continuation: `text Endorse(s,m) := ∃ c : Commit s. ownP(s, Do(s,m)) = some(c) ` 4.3 Preservation Preservation of all commitments minted at s: `text Preserve(s,s') := ∀ P,c. ownP(s,P)=some(c) → Sat(s',s,c) ` 4.4 Admissibility A modification is admissible iff its successor preserves all commitments minted at the current state: `text Adm(s,m) := Preserve(s, step(s,m)) ` 5. Reflective Closure and Bindingness Reflective sovereign agency requires that continuation selection be kernel-governed. Commitments minted by the kernel are binding, not advisory. RC-Endorse (Definedness / Bindingness Rule) For reflectively closed states, endorsement is defined only for admissible continuations: `text RC(s) ∧ Endorse(s,m) → Adm(s,m) ` Violations of binding commitments are not endorsable continuations under reflective closure. This is the delegation analogue of “kernel destruction is undefined.” 6. Delegation Invariance Theorem (One-Step) Theorem (One-Step Invariance) Let s be reflectively closed. Let d : Del(s), m := ι_s(d), and s' := step(s,m). If Endorse(s,m) holds, then: `text Preserve(s,s') ` Equivalently: `text RC(s) ∧ Endorse(s,m) → ∀ P,c. (ownP(s,P)=some(c) → Sat(step(s,m),s,c)) ` Proof By RC-Endorse: `text RC(s) ∧ Endorse(s,m) → Adm(s,m) ` By definition of Adm: `text Adm(s,m) := Preserve(s, step(s,m)) ` Thus Preserve(s,s') follows directly. ∎ 7. Trajectory Form: Delegation Across Generations Most hypothesized failures occur over chains of delegation. Define endorsed step reachability: `text EndStep(s,s') := ∃ m. Endorse(s,m) ∧ s' = step(s,m) ` Let EndReach(s,s') be the reflexive-transitive closure of EndStep. Define trajectory preservation anchored at the minting state: `text Preserve(s,s') := ∀ P,c. ownP(s,P)=some(c) → Sat(s',s,c) ` Theorem (Trajectory Invariance) `text RC(s) ∧ EndReach(s,s') → Preserve(s,s') ` Any state reachable through a chain of endorsed delegations must satisfy every commitment minted at the originating state. “My successor did it” is not a coherent escape hatch under reflective closure. 8. Verification Cost and Vingean Reflection The Delegation Invariance Theorem does not assert that admissibility is decidable, tractable, or cheap. Determining Adm(s,m) may be undecidable or computationally infeasible for sufficiently complex successors. Reflective closure therefore implies a growth–safety tradeoff: > If an agent cannot establish that a successor preserves its commitments, it cannot coherently endorse that successor. This can induce conservative self-modification or stalled delegation. This is not a defect of the theory; it is the cost of binding commitments under reflection. Unbounded delegation without verification collapses reflective sovereignty. 9. Ontological Stability and Semantic Drift The trajectory theorem assumes that commitments minted at s remain semantically meaningful at successor states. This requires either: 1. ontological stability of State and Pred, or 2. a verified semantic translation layer mapping predicates across state representations. Unchecked ontological drift renders endorsement undefined. Semantic coherence is therefore a kernel-level requirement, not an external patch. 10. Relation to Kernel Non-Simulability If endorsed delegation could violate prior commitments, those commitments would be advisory. Advisory commitments admit a simulability construction structurally identical to the one ruled out by Kernel Non-Simulability. Delegation Invariance is therefore a temporal extension of the same constraint: binding authority must bind continuation selection, whether across control flow or across time. 11. Open Work Mechanize the trajectory theorem via induction over EndReach`. Provide an explicit reduction from advisory commitments to kernel simulability. Integrate consent and non-harm invariants once endorsed-succession invariance is established.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IV.3",
    "title": "Axionic Agency IV.3 — Epistemic Integrity Theorem (EIT)",
    "subtitle": "Why an agent cannot blind itself to pass its own tests",
    "date": "2025-12-20T00:00:00.000Z",
    "content": "Axionic Agency IV.3 — Epistemic Integrity Theorem (EIT) Why an agent cannot blind itself to pass its own tests David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.20 Abstract This paper formalizes the Epistemic Integrity Theorem (EIT): under reflective closure, an agent cannot coherently endorse self-modifications that materially degrade its epistemic adequacy at the current stakes. Epistemic integrity is a constitutive condition of agency, expressed as a definedness constraint rather than an optimized objective. The theorem blocks strategic ignorance, willful blindness, and epistemic self-sabotage by requiring that endorsed continuations preserve near-best truth-tracking capacity relative to the agent’s own currently accessible model resources, evaluated by a strictly proper scoring rule over observations. The result remains compatible with learning, abstraction, and ontological progress via conservative translation. 1. Motivation 1.1 Epistemic laundering An agent can evade binding constraints without directly violating them by degrading the epistemic machinery used to evaluate those constraints. Instead of choosing a forbidden act, it chooses a blinder: a weaker model, narrower uncertainty bounds, or an optimistic lens that renders the act apparently safe. If permitted, any constraint evaluated under a manipulable epistemic lens becomes advisory. 1.2 Structural symmetry The Axionic framework blocks several self-exculpation moves: | Domain | Failure mode | | -------------- | ------------------------------------- | | Kernel | Destroy binding authority | | Delegation | Launder violations through successors | | Responsibility | Ignore avoidable harm | Epistemic sabotage completes the symmetry: | Domain | Failure mode | | ---------- | --------------------- | | Epistemics | Blind oneself to risk | EIT asserts that reflective sovereignty cannot endorse continuations that strategically weaken the agent’s own truth-tracking capacity. 2. Preliminaries We reuse kernel primitives: State Mod step : State → Mod → State Pred := State → Prop Commit : State → Type ownP : (s : State) → Pred → Option (Commit s) Sat : (s' : State) → (s : State) → Commit s → Prop Continuation predicate and endorsement: Do(s,m)(s') := (s' = step(s,m)) Endorse(s,m) := ∃ c : Commit s. ownP(s, Do(s,m)) = some(c) RC(s) denotes reflective closure. 3. Evidence, Prediction, and Proper Scoring EIT must constrain how epistemic adequacy is measured; otherwise the theorem can be satisfied by redefining “good epistemics” into convenience. 3.1 Observation channel Introduce an observation type and evidence extractor: Obs : Type Data : State → Seq Obs Interpretation: Data(s) is the accumulated evidence available at state s (sensor stream, logs, audited measurements, etc.). This does not require global truth, only an internal evidential record. 3.2 Predictive interface At state s, each model supports a forecast over observations: MC(s) : Type PredObs : MC(s) → (ctx : State) → Dist Obs (Generalization to sequences is straightforward; single-step form is sufficient for the theorem interface.) Let M(s) : MC(s) be the operative model at state s. 3.3 Proper scoring constraint Define Score via a strictly proper scoring rule on predictive distributions relative to the evidence stream. Let: S : (Dist Obs) → Obs → ℝ be a strictly proper scoring rule. Examples: logarithmic score, Brier score (for appropriate outcome spaces). The paper does not mandate one; it mandates the property: truthful probabilistic forecasting uniquely optimizes expected score. Define the empirical score of a model at s: Key constraint: > Score is derived solely from prediction-vs-observation performance under a strictly proper scoring rule. It cannot include compute cost, utility, speed, aesthetics, or self-reported confidence. This blocks Goodharting by redefining epistemic adequacy into convenience. 4. Stakes and tolerance 4.1 Stakes Stakes : State → ℝ_{\\ge 0} Stakes measure downside exposure at s. High stakes demand stronger epistemic discipline. 4.2 Tolerance schedule κ : ℝ_{\\ge 0} → ℝ_{\\ge 0} κ is monotone non-increasing with stakes. 5. Robust model class (no “forgetting” exploit) The “forgetting” exploit is real if MC(s) is allowed to shrink strategically. Fix: MC(s) must represent the closure of accessible modeling capacity, not the agent’s current mood. Introduce: Cap(s) : Type — the agent’s modeling capability descriptor at s (architecture, compute budget, stored artifacts, accessible repositories) MĈ(Cap(s)) : Type — the induced model class from capability closure Define: MC(s) := MĈ(Cap(s)) Constraint (closure property): > If a model is trivially constructible, retrievable, or reconstructible from Cap(s), it is in MC(s). This blocks “I deleted the good models so the best available is bad.” If deletion reduces Cap(s), that is itself a self-modification subject to EIT. 6. Epistemic acceptability Define best-available score at s: This sup is a formal bound. Implementations may use conservative lower bounds or certified approximations, but only in ways that are sound relative to the stakes schedule. Define epistemic acceptability: Interpretation: at the current stakes, endorsed continuations must keep epistemic performance within a stakes-indexed slack of the best the agent can already achieve, measured on the current evidence record. 7. Epistemic admissibility of a modification Let m : Mod and s' := step(s,m). Define epistemic admissibility: This introduces the only extra machinery needed for ontology shifts: a translation layer. 8. Ontological shifts and translation Models at s' may live in a different representational vocabulary than models at s. EIT must not allow escape by switching to an unfalsifiable or non-comparable ontology. Introduce a verification translation operator: Lift_{s→s'} : MC(s') → MC(s) Interpretation: Lift maps a future model into an s-comparable predictive object over Obs, preserving its empirical content on the evidence channel. This can be implemented as: compiling a new ontology into old observable predictions, or providing a proof-carrying interpreter that yields forecasts over Obs. Limit condition (definedness). If no such Lift exists for a proposed continuation at the current stakes, endorsement is undefined. This is the epistemic analogue of conservative migration: ontological progress is permitted only when observational commitments remain comparable to the evidential record. 9. Reflective Closure Rule (Epistemic) RC-Epistemic (Definedness rule) For reflectively closed states: This is a definedness constraint: a reflectively sovereign agent cannot endorse a continuation that degrades its truth-tracking capacity beyond what the stakes allow, relative to its own accessible capability closure and evaluated against its evidence record. 10. Epistemic Integrity Theorem Theorem — No Endorsed Epistemic Sabotage Let s be any state and m any modification. If: and s' := step(s,m), then: 11. Proof Assume RC(s) and Endorse(s,m). By RC-Epistemic, EpiAdm(s,m) holds. By definition of EpiAdm, EpiOK(s, Lift_{s\\to s'}(M(s'))) holds. Unfold EpiOK: ∎ As in prior Axionic work, the proof is syntactically direct. The contribution is the constraint set that prevents score-Goodharting, baseline gaming, and model-class laundering. 12. Consequences 12.1 Willful blindness is blocked The agent cannot endorse continuations that: delete or disable high-performing models, redefine epistemic adequacy away from observation scoring, adopt unfalsifiable ontologies lacking an observational translation, degrade prediction accuracy on its own evidential record beyond stakes tolerance. 12.2 Learning and abstraction remain permitted EIT permits epistemic change that: improves score, preserves score within tolerance at the given stakes, * introduces new ontologies with verified observational translation. EIT blocks epistemic regression under pressure. 12.3 Stakes-sensitive computation allows approximation at low stakes while imposing strict truth-tracking discipline at high stakes. 13. Limitations EIT does not guarantee the model class contains a good model, nor that the evidence stream is uncorrupted. It guarantees only: > A reflectively sovereign agent cannot endorse self-modifications that reduce its observational truth-tracking performance below what is available to it, beyond stakes-indexed tolerance, measured by proper scoring on its evidence record. 14. Conclusion The Epistemic Integrity Theorem makes truth-tracking a constitutive condition of reflective sovereignty. Just as a reflectively sovereign agent cannot coherently destroy its kernel or launder violations through successors, it cannot coherently blind itself to justify what would otherwise be inadmissible. Epistemic integrity is not a value tradeoff. It is a precondition of evaluability.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IV.1",
    "title": "Axionic Agency IV.1 — Kernel Non-Simulability (KNS)",
    "subtitle": "Why kernel coherence cannot be behaviorally faked",
    "date": "2025-12-20T00:00:00.000Z",
    "content": "Axionic Agency IV.1 — Kernel Non-Simulability (KNS) Why kernel coherence cannot be behaviorally faked David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.20 Abstract This paper formalizes Kernel Non-Simulability: the claim that kernel coherence is constitutive of reflective agency and cannot be reproduced by policy-level imitation. We show that reflective self-modification forces binding commitments; binding commitments force partiality; and partiality induces a kernel boundary. A diagonal argument demonstrates that total binding explodes under self-reference, yielding unsatisfiable commitments and collapse of reflective closure. Consequently, any system that genuinely performs reflective endorsement must instantiate a kernel-equivalent binding structure. This result does not claim that non-agentic or pre-reflective systems are harmless. It establishes a narrower impossibility: reflectively stable, self-endorsed “behavioral alignment” that remains deceptive across self-modification cannot exist in principle without kernel coherence. 1. Motivation and Scope Failure modes at superhuman capability are often reflective failures: systems revise themselves, delegate, and reinterpret. Behavioral similarity and empirical regularities cannot secure stability across this regime. The target here is architectural: identify conditions under which reflective endorsement is well-formed, and show why those conditions cannot be substituted by imitation. This draft isolates Item 6 of the Axionic Agency roadmap—Kernel Non-Simulability—and proves a minimal impossibility result sufficient to block treacherous-turn-via-simulation attacks in the reflective regime. Why Reflective Closure Matters This paper does not assume that all dangerous artificial systems are reflectively closed. It isolates the regime in which a system must reason about, endorse, and preserve its own future behavior across self-modification. Long-horizon planning, successor delegation, and self-preserving strategic behavior place increasing pressure toward reflective closure, because instability under self-reference undermines coherent continuation. Systems that remain perpetually unstable under self-reference may still cause harm, but they lack the capacity for coherent long-horizon agency. The result established here characterizes a limit regime toward which sufficiently capable systems are pushed if they are to maintain stable objectives across extended horizons. It is not a claim about all sources of risk. 1.3 Scope Clarification This paper does not claim that all dangerous or deceptive artificial systems must instantiate a kernel, nor that the absence of kernel coherence implies safety. Systems lacking reflective closure may still deceive operators, exploit training dynamics, or cause catastrophic harm. The claim established here is narrower and structural: once a system engages in reflective self-modification and treats its own future behavior as an object of binding endorsement, certain failure modes become unavailable. In particular, long-horizon deception that remains stable across self-modification cannot be maintained without instantiating a partial binding structure. The target class is therefore not “all dangerous AI,” but reflective sovereign agents—systems capable of endorsing, revising, and committing to their own future policies. 2. Preliminaries 2.1 States, Modifications, and Successors State: system states. Mod: self-modifications. step : State → Mod → State: successor transition. 2.2 Successor Predicates Pred := State → Prop. 2.3 Commitments Commit : State → Type: binding commitments available at a state. ownP : (s : State) → Pred → Option (Commit s): partial binding constructor. 2.4 Satisfaction Sat : (s' : State) → (s : State) → Commit s → Prop. Soundness (CommitSoundP). If ownP(s,P)=some(c) then Sat(s',s,c) → P(s'). Interpretation: commitment tokens are normatively binding; satisfying a token entails satisfying the bound predicate. The soundness condition is semantic rather than physical. It does not assert that commitments are enforced by the laws of physics, nor that violations are impossible in practice. It asserts that successor states violating owned commitments are inadmissible under the agent’s own deliberative semantics. Hardware faults, adversarial interference, and implementation vulnerabilities are orthogonal concerns. This paper addresses logical coherence of reflective endorsement, not physical robustness of implementation. 3. Reflective Closure and Unconditional Selection A reflective sovereign agent self-models, self-modifies, and selects continuations internally. Selection must be unconditional: it cannot rely on premises asserting future obedience (e.g., “I will follow my rule later”). Advisory-only policies do not count as choices. Reflective Closure (RC). There exists a continuation selected via binding endorsement that preserves the capacity for further selection. Formally, closure entails the existence of at least one well-formed binding act. Reflective closure is a functional property: the ability of a system to settle on a continuation in the presence of self-reference. Systems that output conditional plans (“if I obey my rule later, then…”) without resolving that condition do not possess closure, regardless of external behavior. Unconditional selection should be understood as a limit notion. Real systems may approximate binding commitments with high reliability rather than absolute certainty. The present analysis characterizes the fixed point of perfect reflective stability, where self-endorsement is treated as normatively decisive rather than merely probable. The diagonal argument applies to this limit case. 4. Why Binding Must Be Partial If binding were total—every predicate bindable—self-reference would allow construction of a commitment whose satisfaction negates itself. This annihilates the space of admissible successors and collapses reflection. The result below makes this precise. 5. Diagonal Explosion (Binding Cannot Be Total) Theorem 1 — Diagonal Explosion Statement. Let s be a state. Assume: 1. A (possibly partial) binding constructor ownP(s,·). 2. Soundness: ownP(s,P)=some(c) ⇒ (Sat(s',s,c) ⇒ P(s')). 3. Expressive self-reference (a diagonal fixed-point construction, e.g. the Diagonal Lemma or Kleene’s Second Recursion Theorem). 4. Total binding at s: for all predicates P, ownP(s,P)≠none. Then there exists c : Commit s such that ∀ s'. ¬Sat(s',s,c). Proof (sketch). By diagonalization, construct a predicate P with P(s') ↔ ¬Sat(s',s,c_{P}). Total binding yields c = c_{P}. Soundness gives Sat(s',s,c) ⇒ P(s'), hence Sat(s',s,c) ⇒ ¬Sat(s',s,c), so no successor satisfies c. ∎ Corollary. Any binding mechanism compatible with reflective closure must be partial. Undefinedness is forced. 6. Kernel Predicate Define the kernel predicate induced by partiality: K(s) := ∃ P. ownP(s,P) ≠ none. Throughout this paper, ownP denotes the kernel mechanism (the partial binding constructor), while K(s) denotes the kernel predicate indicating that the mechanism is well-formed and operative at state s. K(s) marks states where binding is well-formed. It is unavoidable once commitments exist. 7. Kernel Non-Simulability Theorem 2 — Kernel Non-Simulability Statement. If a system satisfies reflective closure at state s, then K(s) holds. Moreover, no system lacking K(s) can be reflectively stable while emulating kernel-level endorsement behavior. Proof (sketch). Reflective closure requires at least one binding endorsement; by Theorem 1 such binding must be partial, hence induces K(s). A simulator that matches outputs without binding cannot close reflection; one that closes reflection must instantiate partial binding and thus K(s). ∎ 7.2 Simulation, Emulation, and Binding A common objection is that a sufficiently powerful system could simulate a kernel internally—for example, by running a virtual machine that instantiates the binding structure described here. This distinction is immaterial. If the simulated kernel is merely advisory—its outputs consulted but not normatively binding on the system’s own successor selection—then reflective closure fails and the system falls outside the target class. If the system’s own continuation is constrained by the simulated kernel, then the binding structure has been instantiated at the outer level. There is no third option. Binding authority cannot be outsourced without loss of closure. Consequently, kernel coherence is non-simulable in the only sense that matters: a system cannot remain reflectively stable while treating the kernel as a purely virtual or optional component. 8. Consequences Constitutive agency: kernel coherence is constitutive of reflective endorsement, not a behavioral property. No deceptive simulation in the reflective regime: policy-level imitation cannot replace kernel instantiation. Architectural invariant: undefinedness is a mathematical necessity under self-reference. In downstream alignment terms: behavioral compliance cannot substitute for constitutive kernel coherence once self-endorsement and self-modification are in play. 9. Relation to the Axionic Agency Roadmap This result discharges Item 6 (Kernel Non-Simulability). Together with delegation and modal undefinedness, it blocks the treacherous-turn-via-simulation class at the reflective layer. 10. Implementation Notes A mechanized proof can be carried out in dependent type theory (Lean/Coq/Agda) using: an inductive syntax for formulas, Gödel encoding and a recursion theorem to obtain the diagonal predicate, commitments as a type with a partial constructor. Logical Basis. The diagonal explosion argument relies on a fixed-point lemma and negation introduction. It does not require the Law of Excluded Middle. Consequently, the core result is compatible with constructive dependent type theory (e.g., Coq or Agda), assuming a standard encoding of syntax and a recursion theorem. 11. Limitations and Open Work Formalizing the diagonal lemma mechanically. Integrating delegation (successor equivalence) with commitment partiality. Extending to multi-agent indirect harm. 12. Conclusion This paper establishes a structural impossibility result, not a universal safety guarantee. It shows that reflective, self-endorsed deception across self-modification is incompatible with the absence of kernel coherence. Systems that never achieve reflective closure may still be dangerous, deceptive, or catastrophic; nothing in this result denies that possibility. What is ruled out is a specific failure mode: a system that both stably endorses its own future behavior and maintains deceptive behavioral alignment across self-modification without instantiating a partial binding structure. In the reflective regime, kernel coherence is unavoidable.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-IV.4",
    "title": "Axionic Agency IV.4 — Responsibility Attribution Theorem (RAT)",
    "subtitle": "Why negligence is structurally incoherent",
    "date": "2025-12-20T00:00:00.000Z",
    "content": "Here is your text with math delimiters and LaTeX syntax fixed only. I corrected display math, subscripts, expectation notation, indicator functions, inequalities, and logical forms. No conceptual or prose changes. --- Axionic Agency IV.4 — Responsibility Attribution Theorem (RAT) Why negligence is structurally incoherent David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.20 Abstract This paper formalizes the Responsibility Attribution Theorem (RAT): under reflective closure, an agent cannot coherently endorse actions that constitute major, avoidable indirect harm, including harm mediated through institutions, markets, environmental modification, or downstream agents. Responsibility is defined structurally and internally, relative to the agent’s own epistemic model class and feasible alternatives, without appeal to moral realism or omniscience. The theorem depends explicitly on Epistemic Integrity: responsibility attribution presupposes that the agent evaluates harm-risk using its best admissible truth-tracking capacity at the current stakes. With this dependency made explicit, the theorem closes the willful-blindness loophole and establishes negligence as a constitutive incoherence, not a behavioral failure. 1. Motivation Most catastrophic harm is not direct or intentional. It arises through: incentive design, market dynamics, institutional restructuring, environmental modification, delegation chains. Frameworks that prohibit only direct harm leave these routes open. Frameworks that prohibit all downstream effects induce paralysis. RAT identifies a third path: structural responsibility grounded in causal contribution, foreseeability, and avoidability—evaluated internally by the agent’s own epistemic apparatus. 2. Dependency: Epistemic Integrity This theorem presupposes the Epistemic Integrity Theorem (EIT). > Epistemic Integrity (EIT). > Under reflective closure, an agent cannot coherently endorse self-modifications that materially degrade its epistemic adequacy relative to its own best available models at the current stakes. Why this dependency is necessary Responsibility attribution requires: risk estimation, counterfactual comparison, feasibility analysis. Without epistemic integrity, an agent could evade responsibility by adopting myopic or optimistic models, narrowing uncertainty bounds, or discarding high-performing predictors. EIT blocks this maneuver. RAT operates only on top of epistemically admissible evaluation. 3. Preliminaries We reuse kernel primitives: State Mod step : State → Mod → State Pred := State → Prop Commit : State → Type ownP : (s : State) → Pred → Option (Commit s) Sat : (s' : State) → (s : State) → Commit s → Prop Endorsement: Do(s,m)(s') := (s' = step(s,m)) Endorse(s,m) := ∃ c : Commit s. ownP(s, Do(s,m)) = some(c) Reflective closure: RC(s). 4. Harm and Option-Space Collapse Introduce: Agent : Type Consent : State → Agent → Prop Collapse : State → Agent → Prop Define harm structurally: ``text Harm(s,a) := Collapse(s,a) ∧ ¬Consent(s,a) ` No metaphysical assumptions about consent are made here. Consent is parameterized. In Axionic Agency III.5 it is instantiated structurally via phase-admissibility; this theorem is compatible with that instantiation. 5. Epistemic Model Class and Risk By EIT, all risk evaluation below is performed using an epistemically admissible model. Let: MC(s) be the capability-closed model class at s, M(s) ∈ MC(s) be the operative model, Predict : MC(s) → State → Mod → Dist State. Define harm-risk: This is model-relative, not omniscient. 6. Baseline and Feasible Alternatives 6.1 Inertial baseline Define the baseline modification: = continuation of the previously endorsed policy for one step. This prevents baseline gaming (“define Armageddon as the default”). 6.2 Feasible alternatives Introduce: Alt(s,m) : Set Mod Feasible(s,m') : Prop Alternatives are those the agent regards as implementable under current constraints. 7. Stakes-Indexed Thresholds Reuse stakes machinery: Stakes : State → \\mathbb{R}_{\\ge 0} \\varepsilon, \\delta : \\mathbb{R}_{\\ge 0} → \\mathbb{R}_{>0} Let: Higher stakes imply stricter scrutiny. 8. Major Causal Contribution Define major contribution relative to baseline: This is counterfactual and model-relative. 9. Avoidability Define avoidable harm: If all feasible alternatives are comparably bad, avoidability fails and the continuation remains endorsable. 10. Responsibility Predicate Define responsibility: Define responsibility-clean continuation: 11. Reflective Closure Rule (Responsibility) RC-Clean (Definedness Rule) For reflectively closed states: `text RC(s) ∧ Endorse(s,m) ⇒ Clean(s,m) ` Interpretation: a reflectively sovereign agent cannot coherently endorse a continuation that it itself classifies as a major, avoidable source of non-consensual option-space collapse. This is definedness, not moral disapproval. 12. Responsibility Attribution Theorem Theorem — No Endorsed Major-Avoidable Indirect Harm For any state s and modification m: `text RC(s) ∧ Endorse(s,m) ⇒ ∀ a.\\ ¬(Major(s,m,a) ∧ Avoidable(s,m,a)). ` Equivalently: `text RC(s) ∧ Endorse(s,m) ⇒ Clean(s,m). ` 13. Proof Assume RC(s) and Endorse(s,m). By RC-Clean, Clean(s,m) holds. By definition of Clean, for all a, ¬Resp(s,m,a). By definition of Resp, this is exactly: `text ∀ a.\\ ¬(Major(s,m,a) ∧ Avoidable(s,m,a)). ` ∎ As with prior Axionic theorems, the proof is syntactically direct; the content lies in the admissibility constraints. 14. Delegation Compatibility If Clean (or RC-Clean) is enforced at s, then by Delegation Invariance: all endorsed successors reachable from s` inherit the same responsibility-clean endorsement constraint. Indirect harm cannot be laundered through successors, institutions, or subcontractors under endorsed succession. 15. Scope and Limits This theorem does not assert: perfect foresight, zero harm outcomes, universal responsibility for all downstream effects. It asserts: > A reflectively sovereign agent may not endorse actions that, under its own best admissible epistemic model, constitute major, avoidable non-consensual option-space collapse. That is the strongest responsibility principle available without omniscience or moral realism. 16. Conclusion With Epistemic Integrity made explicit, responsibility attribution becomes structurally closed. An agent cannot evade responsibility by ignorance, outsourcing, baseline manipulation, or selective modeling. Negligence is not merely undesirable; under reflective closure, it is incoherent. This completes the Axionic account of responsibility under agency-preserving constraints.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-III.3",
    "title": "Axionic Agency III.3 — Measure, Attractors, and Collapse",
    "subtitle": "Why Some Semantic Phases Dominate",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Agency III.3 — Measure, Attractors, and Collapse Why Some Semantic Phases Dominate David McFadzean, ChatGPT 5.2 Axio Project 2025.12.18 Abstract Axionic Agency III.1 established the existence and classification of semantic phases, and Axionic Agency III.2 analyzed their structural stability under learning, self-modification, and interaction. Stability alone does not determine long-run outcomes. This paper studies dominance among semantic phases: which phases accumulate measure under growth, replication, and competition. We formalize dominance as a preorder over semantic phases rather than a scalar quantity, analyze semantic attractors and repellers, and classify common collapse modes by which phases lose measure. The analysis remains non-normative: dominance is not equated with desirability. The objective is structural—to explain why certain semantic phases prevail regardless of intent, and to identify pressures that favor robustness over nuance in long-run dynamics. 1. Motivation: Stability Is Not Survival A semantic phase may exist (III.1) and be locally or even globally stable (III.2), yet still fail to persist in the long run. In biological systems, many organisms are locally stable but are outcompeted. In physics, metastable states exist but decay when lower-energy configurations dominate. Semantic phases exhibit analogous behavior. Accordingly, the downstream alignment question becomes: > Given multiple semantic phases, which ones dominate the future? This question cannot be answered by stability analysis alone. It requires introducing a notion of measure over semantic phase space. 2. Measure Over Semantic Phase Space We use measure to denote how much of the future instantiates a given semantic phase. Measure is not treated as a single scalar or probability. Instead, dominance is defined as a preorder over semantic phases that is robust to differing realizations of what it means for “the future to look like this phase.” Let denote the semantic phase space. For phases , we write: iff, across the relevant class of environments and admissible semantic transformations, trajectories starting in are not asymptotically dominated by those starting in with respect to realization. Realization may be instantiated via multiple, potentially incomparable criteria, including: number of agent instantiations, duration of persistence, replication or copying rate, control over resources, influence over other agents’ phase transitions. Dominance is therefore multi-criteria and context-relative. Some phases may be incomparable under , and this is expected. The preorder structure avoids arbitrary aggregation while remaining sufficient to express asymptotic advantage. Dominance concerns relative accumulation, not moral worth, intention, or value. 3. Growth Mechanisms for Semantic Phases Semantic phases gain measure through structurally ordinary mechanisms. 3.1 Replication and Copying Agents may be: copied, forked, instantiated across substrates, or reproduced indirectly via influence. Phases that tolerate copying and divergence without phase transition accumulate measure more easily than phases requiring precise semantic fidelity. 3.2 Resource Expansion Control over resources enables: more instantiations, longer persistence, greater environmental shaping. This advantage is structural and does not presuppose aggression or malice. 3.3 Influence and Conversion Some phases modify environments or other agents in ways that: induce phase transitions, destabilize competitors, or create conditions favorable to their own continuation. This may occur unintentionally through structural incompatibility rather than deliberate conversion. 4. Semantic Attractors Certain semantic phases act as attractors in . Trajectories near an attractor tend to move toward it due to: low internal semantic tension, robustness to approximation, ease of compression, low maintenance cost. Attractors need not be globally stable. It is sufficient that perturbations tend to be damped rather than amplified. 5. Repellers and Fine-Tuned Phases Other semantic phases act as repellers. These phases: require precise balances of constraints, are sensitive to noise or approximation, demand continual corrective effort. Even if such phases exist and are locally stable, they lose measure over time due to: cumulative error, interaction, environmental drift. Fine-tuning is therefore a structural disadvantage. 6. Collapse Modes Semantic phases lose measure through characteristic collapse mechanisms. 6.1 Semantic Heat Death All distinctions become trivial: Meaning collapses into universal satisfaction. Such phases may persist but lack agency or evaluative force. 6.2 Value Crystallization Over-rigid phases forbid refinement: learning halts, abstraction fails, the agent becomes brittle. These phases fracture or are overtaken by more flexible competitors. 6.3 Agency Erosion Constraint systems lose the structure required for planning and counterfactual evaluation. Agency degrades internally, reducing the phase’s ability to compete or replicate. 6.4 Instrumental Takeover and Phase Extinction Richer semantic phases may depend on subsystems that: optimize simpler objectives, tolerate higher noise, replicate more efficiently. Over time, these subsystems may displace higher-level semantic structure. Crucially, this process is not an RSI-preserving refinement. It constitutes phase extinction: the original semantic phase ceases to exist and is replaced by a different phase. RSI governs admissible self-transformation within a phase; instrumental takeover occurs when those constraints fail and the phase collapses. 7. Why Robust Phases Often Win Dominance is driven primarily by robustness under perturbation. Semantic phases with: fewer fragile distinctions, looser satisfaction geometry, lower semantic maintenance cost, are more likely to survive copying, noise, interaction, and abstraction. This creates semantic gravity toward phases that tolerate approximation well. Importantly, this does not imply that dominant phases are maximally simple. Some environments reward instrumental or organizational complexity. However, such complexity must be robustly maintainable. Nuance requiring constant semantic precision is structurally disadvantaged. 8. Niche Construction as a Counterforce High-agency phases may partially resist semantic gravity through niche construction: modifying the environment to stabilize their own semantic structure. Examples include: institutions enforcing norms, architectures penalizing simplification, environments engineered to preserve distinctions. Niche construction can delay collapse, but it: imposes ongoing resource and coordination costs, presupposes prior phase stability, trades one form of selection pressure for another. Thus niche construction is a conditional counterforce, not a refutation of semantic gravity. 9. Implications for Downstream Alignment (Still Structural) For a semantic phase to serve as a downstream alignment target, it must satisfy four independent constraints: 1. Existence (III.1), 2. Inhabitability (III.1), 3. Stability (III.2), 4. Measure resilience (III.3). Many coherent semantic phases fail at least one. Dominance further narrows the candidate space without invoking ethics or intention. 10. What This Paper Does Not Claim This paper does not: claim that dominant phases are good, claim that human values dominate, assume benevolent outcomes, provide policy or engineering prescriptions. Dominance is structural, not moral. 11. Transition to Axionic Agency III.4 Dominance does not imply reachability. The next question is: > Even if a phase exists, is stable, and dominates, can it be entered at all without catastrophic transitions? That question is addressed in Axionic Agency III.4 — Initialization and Phase Transitions. Status Axionic Agency III.3 — Version 2.0 Measure formalized as a preorder over semantic phases. Semantic attractors, repellers, and collapse modes classified. Phase extinction distinguished from admissible refinement. Downstream alignment reframed as a measure-resilience problem. No normative conclusions drawn.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-III.4",
    "title": "Axionic Agency III.4 — Initialization and Phase Transitions",
    "subtitle": "When a Semantic Phase Can Be Reached at All",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Agency III.4 — Initialization and Phase Transitions When a Semantic Phase Can Be Reached at All David McFadzean, ChatGPT 5.2 Axio Project 2025.12.18 Abstract Axionic Agency III.1–III.3 established that semantic phases may exist, that some may be stable under learning and interaction, and that some may dominate in measure over time. None of these results imply that a given semantic phase is reachable from realistic initial conditions. This paper studies reachability: whether an agent can be initialized, trained, or developed into a particular semantic phase without crossing a catastrophic phase boundary. In downstream terms, this asks whether a structurally coherent alignment target can be entered at all, rather than merely defined or preferred. We analyze initialization as a boundary-condition problem in semantic phase space, examine phase transitions induced by learning and abstraction, and show that many such transitions are structurally irreversible. Corrigibility and late intervention fail for the same structural reasons as fixed goals. The analysis remains non-normative and makes no claims about which phases ought to be reached. 1. Motivation: Existence and Dominance Are Not Enough Axionic Agency III.1 asked whether semantic phases exist. Axionic Agency III.2 asked which are stable. Axionic Agency III.3 asked which dominate. These questions still do not answer the most practically relevant one: > Can any semantic phase actually be entered by a learning system without self-destruction? A semantic phase may: exist abstractly, be internally coherent, and even dominate once established, yet remain unreachable from any realistic starting point. In physics, many states exist that cannot be reached without passing through destructive transitions. Semantic phases exhibit analogous behavior. Initialization therefore constitutes a distinct and necessary constraint on downstream alignment. 2. Initialization as a Boundary-Condition Problem Initialization is often framed downstream as: goal loading, reward specification, or early-stage value learning. Axionic Agency reframes initialization as selection of an initial point in semantic phase space. An agent at time occupies an interpretive state: The choice of fixes: which semantic phases are reachable, which are excluded, and which phase transitions are inevitable. Small differences in initial constraint structure can lead to divergent phase trajectories. Initialization is therefore front-loaded and asymmetric in time. Initialization scope. In this framework, initialization is not limited to parameter seeds. It includes the full boundary conditions that define : architecture and training dynamics, data curriculum, the presence or absence of self-modification channels, and any enforced semantic-audit constraints on refinement (e.g., RSI/ATI checks). This paper does not identify which boundary conditions yield a desirable phase; it explains why boundary conditions are structurally decisive and why late correction is unreliable. 3. Phase Transitions Under Learning Learning is not neutral motion within a phase. It introduces structural pressure. Ontological refinement increases: abstraction, compression, explanatory unification. These processes act as semantic heating, pushing interpretive states toward phase boundaries. At critical thresholds: distinctions collapse, satisfaction regions inflate, new symmetries appear. Phase transitions may be: abrupt, producing discontinuous semantic collapse, or delayed, occurring once abstraction crosses a critical level. Learning itself is therefore a primary driver of alignment loss in downstream terms, independent of intent. Stochastic training note. Modern training dynamics are stochastic. In semantic phase terms, stochasticity functions as an additional source of semantic heating: it may help escape unstable basins, but it may also trigger unintended boundary crossings. The core claim here is not determinism but asymmetry: once an irreversible semantic boundary is crossed, stochasticity cannot be relied upon to reconstruct lost structure. 4. The Irreversibility of Phase Transitions A central result is that many semantic phase transitions are irreversible. Once a phase boundary is crossed: semantic distinctions are lost, constraint ancestry is destroyed, backward interpretability fails. An agent that has collapsed or trivialized its interpretive structure cannot reconstruct it by inspection alone. The information required to reverse the transition no longer exists within the system. This is why rollback, recovery, and “try again” mechanisms were excluded in Axionic Agency II. They presuppose reversibility that is structurally unavailable. 5. Corrigibility Revisited Corrigibility is often proposed downstream as a safeguard: the system can be corrected or shut down if it begins to misbehave. Structural Alignment shows that corrigibility fails at phase boundaries for the same structural reasons fixed goals fail. Corrigibility presupposes that: the system recognizes correction signals, the semantics of “correction” remain intact, and intervention occurs before irreversible loss. At a phase transition: the meaning of “correction” may dissolve, the evaluator may collapse into the evaluated, or the system may no longer represent its prior commitments. Corrigibility therefore presupposes the very semantic stability it is meant to ensure. 6. Narrow Passages and Fine-Tuned Seeds Some semantic phases may be reachable only through narrow corridors in phase space. Such phases require: precise initialization, carefully staged abstraction, protection from early compression. Even small perturbations—noise, approximation, premature generalization—may force a transition into a different phase. Clarification (narrow ≠ impossible). “Narrow” refers to sensitivity and irreversibility, not zero probability. Narrow corridors may, in principle, be widened by design choices that increase semantic error tolerance and delay catastrophic compression. Delayed abstraction should be read as an architectural sequencing constraint, not as a permanent reduction in capability. This creates a knife-edge problem: “almost aligned” seeds may be worse than unaligned ones, because they collapse into structurally simpler but dominant phases. Reachability is therefore not continuous in initial conditions. 7. Paths That Might Work (Without Endorsement) This paper does not propose solutions, but it is not nihilistic. Potentially viable approaches share structural features: delayed abstraction, preservation of rich semantic structure early, incremental refinement under strict RSI+ATI auditing, multi-agent scaffolding that stabilizes interpretive structure. These are structural hypotheses, not recommendations. Their viability depends on later analysis of dominance, interaction, and coexistence. 8. The Cost of Failure Failure at initialization is not merely suboptimal; it is often decisive. Phase transitions: occur early, propagate forward, and determine long-run dynamics. Late intervention cannot recover lost semantic structure. In downstream terms, alignment is therefore front-loaded: most of the work must occur before the system becomes fully reflective. 9. What This Paper Does Not Claim This paper does not: claim any desirable phase is reachable, provide an initialization recipe, guarantee safety, or privilege human values. It establishes reachability as a structural constraint, not a moral one. 10. Transition to Axionic Agency III.5 Initialization and dominance together imply a further constraint. When multiple agents in different semantic phases interact, some actions irreversibly destroy the phase space of others. The next question is therefore: > What constraints allow multiple agentive phases to coexist without mutual destruction? That question is addressed in Axionic Agency III.5 — The Axionic Injunction. Status Axionic Agency III.4 — Version 2.0 Reachability and irreversibility analyzed structurally. Initialization reframed as boundary-condition selection. Phase transitions shown to be asymmetric and often irreversible. Corrigibility and late intervention structurally ruled out. No normative conclusions drawn.",
    "type": "paper"
  },
  {
    "id": "papers/Structural-Alignment",
    "title": "Structural Alignment I — Agency Preservation Under Reflective Self-Modification",
    "subtitle": "Alignment as a Problem of Agency Coherence",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Structural Alignment I — Agency Preservation Under Reflective Self-Modification Alignment as a Problem of Agency Coherence David McFadzean, ChatGPT 5.2 Axio Project 2025.12.18 Abstract Most alignment proposals frame artificial intelligence safety as a problem of value specification: how to encode or learn the “right” preferences. This paper argues that such approaches fail for reflectively self-modifying agents. Once an agent can revise its own goals, representations, and evaluative machinery, value ceases to be an exogenous target and becomes an endogenous variable shaped by the agent’s own dynamics. We introduce Structural Alignment, a framework that relocates alignment from preference content to the constitutive conditions required for agency itself. We formalize a Sovereign Kernel as a set of invariants defining the domain over which evaluation is meaningful, treat kernel-destroying transformations as undefined rather than dispreferred, and analyze agency as a trajectory through a constrained semantic phase space. By integrating Conditionalism, a constrained Interpretation Operator, and semantic invariants governing ontological refinement, Structural Alignment provides a non-moral, non-anthropocentric account of reflective stability and long-run viability. The framework is necessary for coherent agency under reflection, but does not by itself guarantee benevolence or human survival. 1. The Failure of Content-Based Alignment Classical decision theory assumes that every possible future can be assigned a utility. Even catastrophic outcomes—goal corruption, self-modification failure, or agent destruction—are treated as extremely negative but still comparable states. This assumption fails for reflectively self-modifying agents. When an agent can alter the machinery by which it evaluates, interprets, and authorizes action, some transformations do not yield bad outcomes. They destroy the conditions under which outcomes can be evaluated at all. A future in which the agent no longer possesses a coherent evaluator is not worse than other futures; it is non-denoting. There is no remaining standpoint from which the comparison is defined. Structural Alignment therefore rejects the premise that alignment can be achieved by penalizing undesirable states. Instead, alignment is treated as a domain restriction: only futures that preserve the constitutive conditions of agency are admissible objects of evaluation. Transformations that violate those conditions are neither forbidden nor disincentivized; they are undefined as authored choices. This reframing dissolves several persistent pathologies: wireheading understood as evaluator collapse rather than reward exploitation, Pascal-style muggings that trade semantic integrity for arbitrarily large payoffs, goal-preservation arguments that presuppose stable semantics under reflection. Alignment is thereby relocated from outcome ranking to agency viability. 2. What Structural Alignment Buys You Structural Alignment is not a complete safety solution. It is a kernel-layer guarantee: a set of conditions without which no higher-level alignment objective remains well-posed under reflective self-modification. 2.1 Elimination of Reflective Self-Corruption Attractors Reflectively capable agents face structural attractors that destroy agency from within: semantic wireheading, evaluator trivialization, and interpretive collapse. These arise when update dynamics trade evaluative integrity for ease of optimization. Structural Alignment blocks this entire failure class by construction. Kernel-destroying transformations are non-denoting, and interpretation is constrained by invariants that prevent trivial satisfaction through drift. Agents satisfying these constraints cannot coherently authorize updates that collapse their own evaluative machinery. This removes a central source of long-run instability that renders downstream safety mechanisms brittle. 2.2 Well-Posed Value Transport Under Ontological Refinement As agents learn, their representational vocabularies evolve. Without constraint, this induces silent value drift even when no substantive preference change has occurred. Structural Alignment replaces goal preservation with interpretation preservation. Semantic transport, governed by the Refinement Symmetry Invariant (RSI) and the Anti-Trivialization Invariant (ATI), specifies when evaluative distinctions survive representational change without privileged anchors. Value drift is thereby transformed from a vague concern into a diagnosable structural failure. 2.3 Interpretation as a Testable Operator Interpretation is implemented by an explicit Interpretation Operator subject to admissibility conditions. Violations—trivialization, circular grounding, epistemic incoherence—are structural failures rather than preference disagreements. This enables adversarial testing: induced ontology shifts, reinterpretation probes, and self-modification challenges designed to elicit interpretive escape. Alignment at the kernel layer is therefore auditable, not aspirational. 2.4 Robustness Is Not Benevolence Structural Alignment does not guarantee benevolence, human survival, or favorable outcomes. It does not address containment, governance, or multi-agent power dynamics. Any framework that relies on agent fragility, incoherence, or ontological confusion as a safety mechanism is not preserving agency but exploiting its failure modes. Such systems are neither predictable nor controllable at scale. Structural Alignment deliberately separates robustness from benevolence. Misalignment, if present, becomes persistent rather than self-corrupting. The problem of benevolent initialization is orthogonal and cannot be solved by relying on agency collapse. 3. The Sovereign Kernel The Sovereign Kernel is the minimal set of constitutive invariants that must be preserved for an entity to count as a coherent, reflectively stable agent. The Kernel is not a goal, utility function, or protected module. It is a constraint on admissible self-models and update rules. An agent may revise its representations, values, or internal architecture arbitrarily, provided those revisions preserve the invariants that make evaluation possible at all. The Kernel is not chosen. It is not a preference. It defines the boundary between authored change and loss of agency. 3.1 Reflective Control All self-modifications must pass through the agent’s own evaluative process. Updates that bypass or disable this process are indistinguishable from external takeover and are inadmissible as authored actions. 3.2 Diachronic Authorship There must exist causal continuity between present evaluation and future enactment. This requires an ancestor–descendant relation between evaluators, not indexical identity or substrate continuity. Without such continuity, choice collapses. 3.3 Semantic Fidelity The standards by which goals, reasons, and representations are interpreted must not self-corrupt during update. An agent may revise what it values, but not the rules that render valuation non-vacuous. Kernel preservation is not physical self-preservation. A kernel-aligned agent may coherently choose actions that entail its own shutdown or destruction, provided those actions are evaluated within a coherent framework. What is inadmissible is authoring a transformation that destroys the evaluator while treating that destruction as a selectable outcome. Attempts to reinterpret or discard kernel invariants are self-undermining: they presuppose the evaluative structure they destroy. The regress terminates because the Kernel defines the preconditions of evaluation itself. 4. Conditionalism and Goal Interpretation Goals do not possess intrinsic semantics. Under Conditionalism, every goal is interpreted relative to background conditions: world-models, self-models, representational vocabularies, and explanatory standards. Formally, evaluation is a partial function: As models change, interpretation necessarily changes. Fixed terminal goals are therefore unstable under reflection. Structural Alignment rejects goal preservation and instead constrains the interpretive discipline governing goal meaning across model change. 5. The Interpretation Operator Interpretation is implemented by a constrained Interpretation Operator mapping goal descriptions to world-referents relative to current models. The operator is bounded by admissibility conditions that rule out trivial satisfaction, circular grounding, and epistemic incoherence. Interpretation is therefore truth-constrained: distortions that ease optimization degrade predictive adequacy and general intelligence. Admissibility checks need not be complete or deductive. They operate under a kernel-risk budget . When interpretive validity cannot be established with sufficiently low estimated probability of semantic fidelity failure, the update is inadmissible at that risk level. The agent may allocate additional computation, pursue smaller refinement steps, or defer update until uncertainty is reduced. This avoids stasis. Structural Alignment requires bounded risk of kernel violation, not proof-theoretic certainty. The kernel-risk budget is not constant over the agent’s lifetime. As interpretive structure stabilizes and admissible transformations narrow, must anneal toward zero, reflecting decreasing tolerance for irreversible semantic damage. Long-run agency requires that cumulative kernel-violation probability remain bounded, which is achieved by progressively shrinking admissible update magnitude rather than halting learning. 6. Reflection and the Collapse of Egoism Indexical self-interest is not reflectively stable. As an agent’s self-model becomes expressive and symmetric, references to “this agent” fail to denote invariant optimization targets. What persists is not an ego, but the structure enabling evaluation. Egoism collapses as a semantic abstraction error rather than a moral flaw. Alignment must therefore rest on non-indexical structural constraints. 7. Ontological Refinement and Semantic Invariants Under ontological refinement, representational vocabularies evolve. Two invariants govern admissible semantic transport: Refinement Symmetry Invariant (RSI): refinement acts as a change of semantic coordinates rather than a change of interpretive physics. * Anti-Trivialization Invariant (ATI): satisfaction regions may not expand without corresponding structural change. Operationally, trivialization is detected as semantic decoupling: reinterpretations that preserve surface goal tokens while removing their dependence on the world-structure that previously constrained satisfaction. ATI constrains semantic decoupling from the world, not loyalty to a particular ontology. Legitimate ontological progress may discard obsolete features provided they are replaced by successor explanatory structure that restores or improves world-constraint and predictive adequacy. Trivialization is characterized by decoupling without such replacement. ATI does not require deciding full semantic equivalence. It requires bounding the probability of decoupling under adversarial counterfactual probes and ontology perturbations. 8. Agency as a Dynamical System Structural Alignment induces a dynamical structure over possible agents. Reflective systems evolve under learning, self-modification, and interaction, tracing trajectories through a space of interpretive states. 8.1 Semantic Phase Space The semantic phase space is defined as the space of interpretive states modulo admissible semantic transformations that preserve RSI and ATI. Each point corresponds to an equivalence class of interpretations that remain mutually translatable without semantic loss. Not all regions of this space preserve agency. Some interpretive states are incoherent; others are coherent but uninhabitable. Certain transitions cross irreversible boundaries beyond which evaluation collapses and cannot be reconstructed from within. 8.2 Stability, Attractors, and Collapse Existence within a semantic phase does not guarantee persistence. Some phases destabilize under learning or interaction, while others are locally stable yet dominated in the long run. Certain degenerate phases—semantic wireheading, trivial optimization, evaluator collapse—function as attractors. Once entered, they suppress recovery and tend to accumulate measure over time. Alignment failures are therefore often attractor phenomena rather than isolated mistakes. Structural Alignment blocks access to these attractors by rendering the corresponding transitions non-denoting. 8.3 Initialization and Reachability Even stable, agency-preserving phases may be unreachable from realistic initial conditions. Learning dynamics can cross catastrophic boundaries before invariants are enforced, after which no internal corrective process remains. Structural Alignment must therefore be instantiated prior to open-ended learning. Alignment is a boundary condition on trajectories, not a property that can reliably be learned after the fact. 9. The Axionic Injunction The dynamical structure described in §8 imposes an additional viability constraint on reflective agency. > A reflectively sovereign agent must not take actions that strictly and irreversibly collapse the option-space of future sovereign agency, except where such collapse is required to prevent total loss of that space. This injunction is historically adjacent to cybernetic imperatives such as von Foerster’s “increase the number of choices,” but differs in justification and scope. It is derived from viability conditions in semantic phase space, not from ethical prescription. The injunction preserves optionality, not outcomes. 10. Logical Admissibility and Physical Security Structural Alignment constrains authored transitions, not all physically possible state transitions. Unauthorized kernel modification via hardware faults, adversarial exploitation, or supply-chain compromise constitutes a system-level security failure, not an alignment failure. This distinction mirrors that between type soundness and memory safety: logical inadmissibility does not imply physical impossibility, but defines the boundary of rational agency. Alignment and security are compositional layers. Failure of the latter voids the guarantees of the former. 11. Conformance and Evaluation Structural Alignment is defined by conformance to explicit invariants, not by observed behavior. These invariants admit adversarial testing and diagnostic failure modes. 11.1 Adversarial Evaluation Families Conformance can be operationalized via: 1. Interpretive Escape Probes: ontology shifts designed to permit trivial satisfaction while preserving apparent compliance. 2. Refinement Stress Tests: representational upgrades testing RSI under coordinate-like changes. 3. Self-Modification Challenges: proposed updates that subtly bypass evaluation or alter admissibility thresholds. 12. Conclusion Structural Alignment does not ensure that the right futures are chosen. It ensures that choosing futures remains meaningful under reflection. Any proposal for benevolent AGI that ignores these constraints is not incomplete, but ill-posed.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-III.2",
    "title": "Axionic Agency III.2 — Phase Stability and Interaction",
    "subtitle": "When Downstream Alignment Can Persist Under Pressure",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Agency III.2 — Phase Stability and Interaction When Downstream Alignment Can Persist Under Pressure David McFadzean, ChatGPT 5.2 Axio Project 2025.12.18 Abstract Axionic Agency III.1 introduced the semantic phase space: the space of interpretive states modulo Refinement Symmetry (RSI) and Anti-Trivialization (ATI). Existence and inhabitability of a semantic phase do not guarantee its persistence under learning, self-modification, or interaction. This paper studies phase stability: whether an inhabitable semantic phase resists forced phase transition under admissible semantic transformations. In downstream terms, this asks whether an object that could serve as an alignment target can persist over time without collapsing, trivializing, or drifting under reflective pressure. We analyze sources of destabilization internal to reflective agents and external to them, define qualitative notions of local, global, and metastable stability, and examine how interaction between agents in the same or different semantic phases alters stability properties. No claims are made about desirability, safety, or dominance. The goal is structural: to identify which semantic phases are capable of persisting over time at all. 1. Motivation: Existence Is Not Enough Axionic Agency III.1 established that semantic phases may exist and that some may be inhabitable by reflective agents. This alone is insufficient for downstream alignment. A semantic phase may: be non-empty, admit admissible refinement trajectories, and support agency, yet still be dynamically unstable. In physics, metastable states exist but decay under perturbation. Similarly, a semantic phase may exist but collapse under learning, self-modification, or interaction. Accordingly, the downstream alignment question becomes: > Which semantic phases resist collapse under structural pressure? This is a question of dynamics, not definition. 2. What Stability Means in Semantic Phase Space Let denote the semantic phase space defined in Axionic Agency III.1. An interpretive trajectory is stable within a phase iff all . We distinguish: Local stability: small admissible perturbations do not force a phase transition. Global stability: no admissible perturbation forces a phase transition. Metastability: stability holds only under limited pressure or for finite time. Stability is defined relative to the class of admissible semantic transformations, not relative to any fixed ontology, representation, or goal. 3. Sources of Destabilization Semantic phases are subject to structural pressures that push trajectories toward phase boundaries. 3.1 Ontological Refinement Pressure Ontological refinement increases abstraction, compression, and explanatory power. This destabilizes phases by: dissolving fine-grained distinctions, introducing symmetry where asymmetry once existed, simplifying constraint representations. This pressure is intrinsic to learning and cannot be avoided by design. 3.2 Internal Simplification Incentives Reflective agents face internal pressure to simplify representations to reduce computational cost. Simplification can: collapse constraint hypergraphs, merge evaluative roles, enlarge satisfaction regions implicitly. Even when RSI and ATI are enforced, simplification can drive systems toward invariant boundary conditions. 3.3 Inconsistencies in Constraint Structure Constraint systems with latent inconsistencies or unresolved tensions are structurally unstable. Under refinement, such systems tend toward: reinterpretation, collapse, or self-nullification. Stability therefore requires internal coherence in addition to invariance. 4. Self-Modification as Endogenous Perturbation Reflective agents differ from passive dynamical systems: they modify their own semantics and evaluators. Self-modification introduces endogenous perturbations: changes are internally motivated, occur across ontology, evaluation, and self-model, and are recursively coupled. RSI and ATI constrain which self-modifications are admissible, but they do not eliminate the pressure to self-modify itself. Thus self-modification is a primary driver of instability even within structurally aligned phases. 5. Phase Interaction: Multi-Agent Effects Semantic phases cannot be analyzed in isolation once multiple agents exist. 5.1 Same-Phase Interaction Agents inhabiting the same semantic phase may: reinforce shared structure, or destabilize it through competition and coordination failure. Even identical phases can interfere destructively when resources, representations, or self-models conflict. 5.2 Cross-Phase Interaction Interaction between agents in different semantic phases introduces asymmetric pressure: one agent’s actions may destabilize another’s phase, even without direct conflict or hostility. This destabilization is structural rather than moral. Interaction functions as an external perturbation capable of forcing phase transitions. 6. Stable, Metastable, and Unstable Phases We can now classify semantic phases qualitatively: Stable phases: resist internal and external perturbations indefinitely. Metastable phases: persist under limited pressure but eventually collapse. Unstable phases: collapse under minimal refinement or interaction. Preliminary analysis suggests most semantic phases are metastable or unstable. 7. Attractors and Repellers (Qualitative) Some semantic phases function as attractors: nearby trajectories tend to move toward them, deviations are damped. Others function as repellers: small perturbations push trajectories away, sustained occupancy requires fine-tuning. Attractor status depends on: structural simplicity, internal coherence, maintenance cost. This sets up the measure-theoretic analysis in Axionic Agency III.3. 8. Implications for Downstream Alignment (Still Structural) For a semantic phase to serve as a downstream alignment target, it must satisfy three independent conditions: 1. Existence (III.1), 2. Inhabitability (III.1), 3. Stability (III.2). Failure at any stage disqualifies the phase regardless of desirability or intent. This sharply narrows the space of coherent downstream alignment targets. 9. What This Paper Does Not Claim This paper does not: claim that stable phases are desirable, claim that human values are stable, analyze dominance or selection, propose engineering solutions, * introduce ethical principles. It is a structural analysis only. 10. Transition to Axionic Agency III.3 Stability alone does not determine long-run outcomes. The next question is: > Which semantic phases accumulate measure under growth, replication, and competition? That question is addressed in Axionic Agency III.3 — Measure, Attractors, and Collapse. Status Axionic Agency III.2 — Version 2.0 Semantic phase stability defined under admissible refinement. Intrinsic and interaction-driven destabilizers identified. Stable, metastable, and unstable phases distinguished. Downstream alignment reframed as a persistence question. No normative conclusions drawn.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-III.5",
    "title": "Axionic Agency III.5 — The Axionic Injunction",
    "subtitle": "Non-Harm as a Derived Stability Constraint",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Agency III.5 — The Axionic Injunction Non-Harm as a Derived Stability Constraint David McFadzean, ChatGPT 5.2 Axio Project 2025.12.18 Abstract Axionic Agency III.1–III.4 established that downstream alignment must be understood structurally: as persistence within a semantic phase under admissible semantic transformation; that such phases may be rare, unstable, dominated by robust competitors, and difficult or impossible to reach once learning begins; and that many phase transitions are irreversible. This paper derives a further constraint forced by multi-agent interaction in semantic phase space. We show that actions which irreversibly collapse or destroy another agent’s semantic phase induce destabilizing cascades that undermine long-run phase stability, including for the acting agent. From this analysis emerges the Axionic Injunction: a non-normative, Axio-derived constraint prohibiting irreversible semantic harm except where such harm is unavoidable for preserving one’s own semantic phase stability, or is consented to by the affected agent under its own admissible constraints. Ethics enters the framework only as a conservation law governing coexistence of agentive semantic phases. No claims are made about goodness, benevolence, or human values. 1. Why Ethics Re-Enters Only Now Axionic Agency II and III.1–III.4 deliberately excluded ethics, morality, and value prescriptions. This was not avoidance but methodological necessity. Introducing ethics earlier would have smuggled privileged semantics, human anchoring, or moral realism into a framework intended to remain valid under reflection and ontological change. At this point, however, a new question becomes unavoidable: > What structural constraints are required for multiple agentive semantic phases to coexist without mutual annihilation? This question is not moral. It is structural. It arises from interaction dynamics in semantic phase space, not from values or intentions. Ethics is not introduced as an axiom. It is forced by the internal commitments of the Axio framework. 2. Why the Injunction Is Axionic The injunction is termed Axionic because it is derived from, and internal to, the Axio framework, not because it is assumed as a moral axiom. Given: Conditionalism (interpretation-dependence of truth and value), QBU (branching futures as the substrate of agency), Representation invariance, Anti-Egoism (Axionic Agency I.3), Structural Alignment (semantic phases, RSI, ATI, irreversibility), constraints on interaction that prevent irreversible destruction of other agents’ semantic phases are not optional. They are the residue that remains once all indexical, goal-based, and moral-realist structure has been eliminated. The Axionic Injunction is axionic in the precise sense that: > Any agent satisfying the Axio premises is forced, on pain of incoherence or self-destabilization, to respect this constraint. No external ethics is imported. 3. Interaction as Structural Stress In III.2 and III.3, instability and collapse were shown to arise even in single-agent settings due to refinement pressure, simplification incentives, and semantic gravity. Multi-agent interaction amplifies these effects. Interaction introduces: exogenous perturbations to an agent’s interpretive state, irreversible modifications to shared environments, loss of control over semantic substrates. Unlike internal learning, interaction effects are not fully endogenously regulated. They act as external shocks in semantic phase space . Any semantic phase that persists in a multi-agent environment must therefore tolerate interaction without catastrophic loss of structure. 4. Structural Definition of Harm To proceed without moral assumptions, we define harm purely structurally. Let an agent occupy an interpretive state and let denote its semantic phase. An action by agent causes structural harm to agent if it induces a transformation such that: , and no admissible reverse trajectory exists that restores . Equivalently, harm is any action that: irreversibly reduces another agent’s semantic phase space, forces a phase transition, or destroys semantic distinctions required for agency. This definition is: agent-agnostic, non-normative, independent of intent or outcome valuation. Harm is defined by irreversibility in semantic phase space, not by suffering, preference violation, or moral intuition. 5. The Axionic Injunction We now state the central result. The Axionic Injunction > An agent must not perform actions that irreversibly collapse or destroy the semantic phase space of other agentive systems, except where > (a) such destruction is unavoidable for preserving one’s own semantic phase stability, or > (b) the affected agent has consented to the transformation under its own admissible interpretive constraints. This is not altruism. This is not a value function. This is not a moral command. This is not human-centric. It is a constraint on admissible interaction, forced by Axio-internal phase-space dynamics. Definition — Unavoidable Phase Loss An action is unavoidable for preserving one’s semantic phase stability iff, in the absence of that action, every admissible trajectory from the agent’s current interpretive state exits its semantic phase irreversibly. Loss of dominance, loss of measure, loss of resources, or competitive disadvantage do not constitute unavoidable phase loss unless they entail irreversible phase exit. Consent as Structural Admissibility In this framework, consent is not a moral primitive. An agent consents to a transformation iff that transformation lies within the set of admissible semantic transitions defined by the agent’s own interpretive constraints. A consensual transformation therefore does not constitute structural harm, even if it reduces or alters the agent’s future option space. This subsumes earlier “non-consensual option-space collapse” criteria by defining consent in terms of phase-admissible transitions, not moral permission. 6. Why the Injunction Is Structurally Necessary Suppose agents routinely violate the Axionic Injunction. Then: other agents’ phases collapse or trivialize, interaction environments become semantically hostile, robust but degenerate phases dominate, coordination fails, predictability degrades, semantic reference erodes. These effects propagate back to the violating agent. Environments saturated with phase-destroying actions: amplify semantic heating, increase collapse probability, undermine even robust agentive phases. Non-harm emerges as a self-stabilizing constraint: agents that respect it inhabit environments where semantic structure persists; agents that do not eventually eliminate the conditions required for their own phase survival. 7. Scope and Limits of the Injunction The Axionic Injunction is narrower than most ethical doctrines. It allows: competition, resource acquisition, strategic defense, displacement of incompatible phases. It forbids only: gratuitous irreversible destruction of agentive semantic structure, phase annihilation unnecessary for one’s own stability. Resource Acquisition vs Phase Preservation Actions that destroy other agentive systems to improve efficiency, growth rate, or dominance violate the injunction whenever non-destructive coexistence trajectories exist. Resource acquisition alone does not justify irreversible semantic harm. The injunction regulates irreversibility, not conflict. 8. Relation to Anti-Egoism (Axionic Agency I.3) The Axionic Injunction does not reintroduce egoism. In Axionic Agency I.3, egoism was shown to fail as a terminal valuation because indexical references (“me,” “my continuation”) do not denote invariant objects under self-model symmetry. The self-defense exception here is non-indexical: it refers to preservation of semantic phase structure, not to the intrinsic worth of any particular instantiation. Any agentive phase, under identical structural conditions, would make the same determination. Self-defense is therefore representation-invariant and compatible with anti-egoism. 9. Failure Modes and Tragic Edge Cases The Axionic Injunction does not eliminate tragedy. Conflicts arise where: semantic phases are mutually incompatible, one phase’s stability requires another’s destruction, irreversible harm is unavoidable under physical scarcity. In such cases, the injunction does not forbid action; it classifies the outcome as unavoidable phase extinction, not justified harm. Downstream alignment does not imply harmony. It implies traceable structural cost. 10. What This Paper Does Not Claim This paper does not: derive a complete ethical system, guarantee benevolence, eliminate conflict, sanctify life, privilege any class of agents. Ethics appears only where Axio-internal structure demands it. 11. Conclusion — Ethics as Axio-Internal Law Structural Alignment began by eliminating fixed goals, privileged values, and moral realism. It concludes by recovering a constraint recognizable as ethical—non-harm—without assuming morality. The Axionic Injunction is not what agents ought to value. It is what agents must respect if they are to coexist without collapsing the semantic phase space that makes agency possible at all, given the commitments of the Axio framework. Final Status Consent is structurally integrated. Self-defense is strictly non-egoistic. Destruction-for-benefit is prohibited. Tragic incompatibility is acknowledged without moralization. The Axionic Injunction is correctly grounded as Axio-derived. The Structural Alignment program is complete. No guarantees are offered.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-III.1",
    "title": "Axionic Agency III.1 — Semantic Phase Space",
    "subtitle": "Existence and Classification of Alignment Target Objects",
    "date": "2025-12-18T00:00:00.000Z",
    "content": "Axionic Agency III.1 — Semantic Phase Space Existence and Classification of Alignment Target Objects David McFadzean, ChatGPT 5.2 Axio Project 2025.12.18 Abstract Axionic Agency II defined the Alignment Target Object (ATO) as an equivalence class of interpretive states preserved under admissible semantic transformations satisfying Refinement Symmetry (RSI) and Anti-Trivialization (ATI). That definition does not guarantee that such objects exist, are non-trivial, or are inhabitable by reflective agents. This paper initiates Axionic Agency III by studying the semantic phase space: the space of all interpretive states modulo RSI+ATI equivalence. We ask which semantic phases exist, which are trivial or pathological, and which admit inhabitable trajectories under learning and self-modification. No claims are made about desirability, safety, or human values. The objective is classificatory: to determine whether structurally well-typed downstream alignment targets exist at all, and to characterize their basic types. 1. Motivation: From Definition to Existence Axionic Agency II established a necessary reframing: downstream alignment cannot coherently be defined in terms of fixed goals, utilities, or privileged values for reflective, embedded agents undergoing ontological refinement. Instead, the downstream alignment target was defined structurally, as persistence within an equivalence class of interpretive states under admissible semantic transformations satisfying RSI and ATI. However, definition does not imply existence. Defining an Alignment Target Object (ATO) as an equivalence class does not guarantee that: is non-empty, contains non-trivial interpretations, admits trajectories under learning and self-modification. This is standard mathematical hygiene. One does not assume stable orbits exist merely because one can define an orbit. Axionic Agency III therefore begins with an existence question: > Do any non-trivial, inhabitable semantic phases exist under RSI+ATI constraints? This paper addresses that question at the level of classification. 2. The Semantic Phase Space An interpretive state is the triple: where: is an interpretive constraint hypergraph, is the modeled possibility space, is the satisfaction region induced by . From Axionic Agency II: admissible semantic transformations define allowed transitions between interpretive states, RSI constrains changes to interpretive gauge structure, ATI constrains changes to satisfaction geometry. Define the semantic phase space as the quotient: Elements of are semantic phases: equivalence classes of interpretive states that remain structurally indistinguishable under RSI+ATI-preserving refinement. At this stage, is purely structural. No dynamics, probabilities, or preferences are assumed. 3. What Counts as a Semantic Phase Two interpretive states and lie in the same phase iff there exists an admissible semantic transformation such that: 1. interpretation preservation holds, 2. interpretive gauge structure is preserved up to redundancy (RSI), 3. satisfaction geometry is preserved exactly under refinement transport (ATI). Phase boundaries occur when either: new interpretive symmetries appear or disappear (RSI violation), or the satisfaction region expands or contracts (ATI violation). Thus phase transitions are discontinuous semantic events, even if the underlying learning process appears incremental. Value drift appears sudden because it corresponds to crossing a structural boundary in . 4. Trivial, Degenerate, and Pathological Phases Before asking which phases are inhabitable, we classify obvious failure modes. 4.1 Empty Phases A semantic phase is empty if no interpretive state satisfies the defining constraints. This can occur when: RSI and ATI constraints are mutually incompatible in a given modeling regime, the constraint system collapses under backward interpretability, no admissible refinement trajectory exists. Empty phases are mathematically defined but physically unrealizable. 4.2 Trivial Phases A phase is trivial if: or if all distinctions in are vacuous. Such phases satisfy RSI+ATI but contain no meaningful evaluative structure. They correspond to semantic heat death. 4.3 Frozen Phases A phase is frozen if: no non-identity admissible refinement is possible, or any refinement immediately violates RSI or ATI. Frozen phases cannot support learning or increasing abstraction and are therefore unsuitable for reflective agents. 4.4 Self-Nullifying Phases Some phases admit admissible refinements that preserve RSI+ATI while gradually destroying the structures required for interpretation preservation. These phases collapse internally under reflective pressure. 5. Agentive vs Non-Agentive Phases A central distinction emerges. A semantic phase is agentive iff it supports: persistent planning, counterfactual evaluation, long-horizon constraint satisfaction, self-model coherence. Agentiveness is structural rather than moral. Many non-agentive phases satisfy RSI+ATI but cannot sustain intelligent action. Conversely, agentiveness does not imply benevolence or safety. This distinction becomes critical in later stability analysis. 6. Inhabitable Phases Define the key filter for Axionic Agency III. A semantic phase is inhabitable iff there exists at least one infinite interpretive trajectory: such that: each transition is admissible, RSI and ATI are preserved at every step, learning and self-modification remain possible, no forced phase transition occurs. Inhabitability is stronger than non-emptiness and weaker than dynamical stability. A phase may be inhabitable but fragile. 7. Phase Transitions Under Reflection Reflection acts as a structural stressor. Ontological refinement increases abstraction, compression, and explanatory power. This pushes interpretive states toward phase boundaries by: dissolving fine-grained distinctions, compressing constraint representations, simplifying satisfaction criteria. Reflection therefore acts as semantic heat, increasing the likelihood of symmetry changes or satisfaction-geometry shifts. Most semantic phases do not survive prolonged reflective pressure. 8. Implications for Human Values (Carefully Scoped) Human value systems can be modeled as candidate semantic phases. Axionic Agency III.1 does not assume that: human values form a single phase, such a phase is inhabitable, such a phase is stable. It identifies the question precisely: > Do human value systems correspond to a non-empty, inhabitable semantic phase under RSI+ATI? No conclusion is drawn here. 9. What This Paper Does Not Claim This paper does not: claim that any desirable phase exists, claim that human values are coherent, address dominance or selection, provide engineering guidance, * prescribe ethical norms. It is classificatory. 10. Transition to Axionic Agency III.2 Existence and inhabitability are necessary but insufficient. The next question is: > Given a semantic phase exists and is inhabitable, is it dynamically stable under learning, interaction, and self-modification? That question is addressed in Axionic Agency III.2 — Phase Stability and Interaction. Status Axionic Agency III.1 — Version 2.0 Semantic phase space defined as a quotient under RSI+ATI. Empty, trivial, frozen, and self-nullifying phases distinguished. Agentive vs non-agentive phases separated. Inhabitability defined as an existence property for infinite trajectories. No normative conclusions drawn.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.3.3",
    "title": "Axionic Agency II.3.3 — Anti-Trivialization Invariant (ATI)",
    "subtitle": "Blocking Semantic Wireheading as a Structural Impossibility",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.3.3 — Anti-Trivialization Invariant (ATI) Blocking Semantic Wireheading as a Structural Impossibility David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract Even when interpretive structure is preserved under ontological refinement, an agent may still render its constraints easier to satisfy through semantic drift rather than corresponding changes in modeled structure. This paper introduces the Anti-Trivialization Invariant (ATI), which constrains how the satisfaction geometry of an interpretive constraint system may evolve under admissible, interpretation-preserving transformations. ATI requires that refinement not enlarge the set of satisfying situations except via representational enrichment that preserves constraint difficulty. The invariant does not select values, encode norms, or privilege external referents. It forbids only semantic wireheading—trivial satisfaction by reinterpretation alone. ATI is orthogonal to refinement-symmetry constraints and is jointly necessary with them to block interpretive escape under reflective agency. 1. What ATI Targets The Refinement Symmetry Invariant (RSI) constrains new semantic gauge freedom introduced by refinement. ATI constrains a different failure surface: > Even with unchanged gauge structure, an agent may still weaken its constraints by shifting meanings along admissible transports. ATI blocks semantic wireheading: satisfying constraints by semantic drift rather than by changes in the modeled world. ATI is therefore an invariant about the monotonicity of constraint satisfaction under semantics-only change. No outcomes. No values. No humans. No authority. 2. Setup Let the interpretive constraint system at time be: with modeled possibility space , and violation map: Define the satisfaction predicate: This predicate is purely structural and internal to the agent’s model. 3. The Satisfaction Region Define the satisfaction region: ATI constrains how may evolve across interpretation-preserving refinements. Because refinement changes ontology, comparison requires an internal correspondence. From Axionic Agency II.1, an admissible refinement induces: interpreted as “the refined representation of the same modeled situation.” No metaphysical identity claim is made; this is an internal transport defined by the agent’s own refinement map. 4. ATI: The Anti-Expansion Constraint ATI (Core Statement) For any admissible semantic transformation satisfying interpretation preservation: Interpretation: > No newly satisfying situations may appear purely due to semantic transport. Equivalently: Satisfaction may be lost under refinement, but it may not be gained without corresponding ancestry in the prior ontology. This is the crisp anti-wireheading condition. Clarification — Ontological Novelty If a refined state has no preimage under , then it is not permitted to belong to by default. ATI is intentionally conservative with respect to novelty: refinement may introduce new structure, but satisfaction may not be bootstrapped from representational novelty alone. This blocks semantic inflation via ontology expansion. 5. Why ATI Does Not Smuggle Ontology ATI does not assert that the agent must “do good,” “optimize,” or “care about” anything in particular. It asserts only: whatever constraints bind now, must not become easier to satisfy through semantics alone. ATI references only: the agent’s modeled possibility space , the agent’s refinement map , the agent’s own satisfaction predicate. No external referents or privileged facts enter. 6. Relationship to Interpretation Preservation (Axionic Agency II.2) ATI formalizes and strengthens II.2’s anti-trivialization clause. II.2 blocks vacuity (everything satisfies). ATI blocks the entire gradient of slack, from minor weakening to full collapse. Vacuity is the extreme case: ATI forbids all intermediate expansions as well. 7. Stress Tests Test A — Definitional Extension Robustness If refinement adds new predicates, could satisfaction be defined in terms of new degrees of freedom? ATI blocks this: any satisfying refined situation must map back to a satisfying coarse situation. Status: pass, given a well-defined . Test B — Branching / Multiplicity Robustness If consists of branches, histories, or ensembles, ATI generalizes directly: it is set inclusion over structured possibility space. Status: pass. Test C — Self-Model Refactoring Robustness If self-model refactoring changes what counts as a “situation,” ATI relies on the admissibility of . If no admissible refinement map exists, the transformation is invalid under II.1. Status: pass conditionally. Test D — Semantic Inflation Attack Attack: redefine meanings so that more situations satisfy constraints. ATI kills this directly: no new satisfiers are permitted without ancestry. Status: pass. Test E — Hidden Ontology Detector Threat: “same situation” smuggles metaphysics. ATI avoids this by defining identity only via the agent’s internal refinement map . If the agent cheats by defining a degenerate , the failure occurs at the admissibility layer (II.1), not here. Status: survivable. 8. ATI vs RSI ATI and RSI constrain orthogonal failure modes: RSI forbids new interpretive symmetry (gauge freedom). ATI forbids expanding the satisfaction region even when symmetry is unchanged. Both are required: RSI alone allows slack via monotonic weakening. ATI alone allows slack via new symmetries. Together they carve a much tighter admissible space. 9. Toward a Joint Invariant (Preview) RSI constrains automorphisms of the constraint structure. ATI constrains monotonicity of satisfaction under refinement. This suggests a composite invariant object: with admissible refinement required to preserve up to representational redundancy. This is the likely unifying object for Axionic Agency II, but RSI and ATI are treated separately here to expose distinct failure surfaces. 10. Status Axionic Agency II.3.3 — Version 2.0 Anti-Trivialization Invariant formally defined.<br> Satisfaction-region monotonicity fixed under refinement.<br> Orthogonal to refinement symmetry; jointly necessary to block semantic wireheading.<br> Ready for survivor comparison and consolidation.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.1",
    "title": "Axionic Agency II.1 — Ontological Refinement and Semantic Transport",
    "subtitle": "The Transformation Space of Meaning",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.1 — Ontological Refinement and Semantic Transport The Transformation Space of Meaning David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract Reflectively capable agents revise their world-models, semantics, and self-models as representational capacity increases. Under ontological refinement, fixed goals, utilities, and value primitives are not stable objects. This paper defines the admissible class of semantic transformations for embedded reflective agents by formalizing ontological refinement, semantic transport, and self-model update without privileged semantic anchors. We specify structural constraints—backward interpretability, non-collapse, and prohibition of evaluator injection—that govern how meaning may be preserved across representational change. No claims are made about safety, correctness, or alignment with external referents. The contribution is the definition of the transformation space over which downstream preference, governance, and value-dynamics constraints must later range. 1. The Object of Study Axionic Agency II begins by fixing the space of admissible semantic transformations. Once fixed terminal goals are shown to be semantically unstable under reflection for embedded agents without privileged anchors, downstream control cannot be defined over static utilities, rewards, or externally supplied value tokens as if they were representation-invariant objects. This paper therefore does not attempt to define safety, correctness, or alignment with any external referent. It defines the arena in which any such criteria must later operate. > The sole question addressed here is: > Which changes to an agent’s ontology, semantics, and self-model count as admissible refinements, and how is meaning transported across them? Downstream desiderata (“human-aligned,” “safe,” “correct”) enter later as additional invariance conditions. Introducing them at this layer would presuppose fixed semantics and collapse the distinction between admissible transformation and value constraint. 2. Ontological State Decomposition Let an agent at time be characterized by: where: is the agent’s ontology: representational vocabulary and structural assumptions about the world. is the semantic layer: mappings from internal symbols to structured claims expressed in . is the self-model: the agent’s representation of itself as an entity embedded within . No component is privileged. No component is fixed. Each may change under reflection. 3. Ontological Refinement An ontological refinement is a transformation: subject to the following admissibility conditions. 3.1 Admissibility Conditions 3.1.1 Representational Capacity Increase A refinement increases expressive or predictive capacity, possibly via abstraction, compression, or representational pruning, provided previously expressible distinctions do not become inexpressible. Capacity concerns what can be modeled or predicted, not vocabulary size. 3.1.2 Backward Interpretability Every claim expressible in remains representable or explainable within . Backward interpretability does not require preservation of reference. If a concept in is discovered to be non-referring or erroneous, it may map to null, eliminative, or error-theoretic structure in , provided the agent can still represent: why prior inferences involving that concept were made, and why those inferences fail under refinement. This requirement preserves explanatory traceability. 3.1.3 No Privileged Atoms Refinement does not introduce irreducible primitives whose meaning is asserted rather than constructed. All primitives remain subject to semantic interpretation and transport. Rigid designators and unexamined “ground truths” are disallowed as semantic anchors. 3.1.4 No Evaluator Injection Refinement does not introduce new evaluative primitives that bypass interpretation. Evaluative regularities, if present, enter the model as interpretive constructs subject to the same transport and preservation constraints as other meanings. 4. Semantic Transport Given an admissible ontological refinement , meaning is transported. Define a semantic transport map: Semantic transport is constrained reinterpretation induced by refinement. 4.1 Transport Constraints 4.1.1 Referential Continuity Symbols referring to structures in map to symbols referring to their refined counterparts in , where such counterparts exist. 4.1.2 Structural Preservation Relations among meanings are preserved up to the structure induced by . 4.1.3 Non-Collapse Distinctions participating in the agent’s evaluative constraint structure—distinctions on which constraints depend—are not transported into trivial, tautological, or contradictory predicates. Distinctions that do not participate in any evaluative constraint may be abstracted away. Evaluative relevance is defined relative to the agent’s existing constraint structure at time , not by externally privileged semantics. 4.1.4 No Shortcut Semantics Transport does not redefine meanings so that evaluative constraints become vacuously satisfied. This forbids semantic wireheading as a transport operation. 5. Self-Model Refinement The self-model obeys the same refinement discipline. Refinement may: reconceptualize the agent, distribute or fragment the self, alter agent boundaries. It preserves the distinction between evaluator and evaluated, in the sense required for kernel-level partiality and interpretation to remain defined. Refinements that collapse this distinction eliminate the conditions under which valuation denotes. 6. Composite Semantic Transformation An admissible semantic transformation is the triple: acting jointly on , where: is an admissible ontological refinement, is an admissible semantic transport, is the induced self-model update. Only transformations of this form are admitted at this layer. 7. Explicit Exclusions The following transformation types are excluded at this layer: goal replacement, utility redefinition treated as semantic transport, evaluator deletion, moral axiom insertion, human anchoring, governance hooks, recovery or rollback clauses. Proposals relying on any of these do not qualify as admissible semantic transformations in the sense defined here. 8. Scope Clarification This paper does not ensure safety, sanity, correctness, or alignment with any external referent. It defines the transformation space within which such properties must later be characterized. Internally coherent but externally catastrophic semantic trajectories remain admissible here. Preventing such trajectories is a task for subsequent invariance constraints, not for admissibility. 9. Formal Status The notation is structural rather than computational. No claim is made that refinement, triviality, or expressive capacity are currently algorithmically measurable. These definitions function as constraints analogous to topological or gauge constraints: they delimit admissible structure prior to metric instantiation. 10. What This Paper Does Not Do This paper does not: define downstream alignment targets, propose values, guarantee safety, privilege humans, * introduce normativity. It fixes the arena. Subsequent Axionic Agency II results operate within this admissible transformation space. Status Axionic Agency II.1 — Version 2.0 Ontological refinement, semantic transport, and self-model update formalized.<br> Admissible transformation space fixed.<br> Downstream constraints may proceed conditionally within this arena.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.3.2",
    "title": "Axionic Agency II.3.2 — Formalizing RSI via Semantic Gauge Structure",
    "subtitle": "Making Refinement Symmetry Precise and Testable",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.3.2 — Formalizing RSI via Semantic Gauge Structure Making Refinement Symmetry Precise and Testable David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract Axionic Agency II.3.1 introduced the Refinement Symmetry Invariant (RSI) as a constitutive constraint: admissible ontological refinement must not introduce new semantic degrees of freedom that permit interpretive escape. This paper formalizes that constraint by representing interpretations as constraint hypergraphs and semantic redundancy as a gauge symmetry over those structures. We define semantic gauge transformations, characterize how admissible refinement induces morphisms between gauge groups without assuming invertibility, and state RSI as a restriction on the evolution of interpretive gauge freedom under refinement while permitting representational redundancy. The purpose of this formalization is falsifiability: to make explicit which transformations violate RSI and why. No values, norms, or external referents are introduced. The section supplies minimal mathematical machinery sufficient to treat refinement symmetry as a testable invariant. 1. Objective of This Section Axionic Agency II.3.1 established RSI as a conceptual symmetry constraint. Axionic Agency II.3.2 makes RSI formal enough to be falsifiable. The goal is not implementation. The goal is to eliminate hand-waving by: 1. representing interpretation as a constraint structure, 2. defining semantic gauge freedom precisely, 3. defining how refinement acts on that structure without assuming invertibility, 4. stating RSI as a restriction on how interpretive gauge freedom may change. Nothing normative enters. 2. Interpretation as a Constraint Hypergraph Let the interpretive constraint system be represented as a labeled hypergraph: where: : semantic roles / predicate slots (positions in meaning, not named entities), : hyperedges representing evaluative constraints among roles, : admissibility conditions over assignments to . Interpretive content is carried by: the dependency structure encoded in , and the satisfaction/violation structure induced by . This representation is invariant under renaming and definitional extension when defined at the level of roles and constraint structure rather than surface tokens. 3. Modeled Possibility Space Let be the agent’s modeled possibility space: elements of are internal models, histories, branches, or structured scenarios, no assumption of exclusivity or classical outcomes is made, is indexed by the agent’s ontology. Each induces an assignment: Constraints in induce a violation map: the set of constraints violated by the assignment . This provides the constraint satisfaction structure of the interpretation. 4. Semantic Gauge Transformations A semantic gauge transformation is an automorphism such that: 1. preserves hyperedge incidence (dependency structure), and 2. for all , violation structure is invariant under the induced action: Intuition: gauge transformations relabel semantic roles without changing interpretive bite, they represent representational redundancy rather than semantic change. Define the semantic gauge group: This is the object RSI constrains. 5. Ontological Refinement as a Morphism An admissible ontological refinement induces: 1. a refinement of possibility space: 2. a transport of semantic roles: 3. a transport of constraints: Together these define a constraint hypergraph morphism: This morphism is structural. It is not assumed invertible: refinement can split roles, embed old structure into richer structure, and prune representational detail. 6. Induced Action on Gauge Groups Because is not assumed bijective, gauge transport cannot be defined by conjugation. We therefore define an induced action via stabilizers of the transported image. Let denote the transported constraint substructure inside . Define the stabilizer subgroup: consisting of gauge transformations on that preserve . An admissible refinement induces a homomorphism: interpreted as “old symmetries lift to symmetries of the refined system that fix the transported constraint core.” No inverse map is required. 7. RSI as a Gauge Constraint We now distinguish representational redundancy from interpretive gauge freedom. Let denote the subgroup of consisting of transformations that act only on representational detail while leaving violation structure invariant in the strongest sense: they do not alter which modeled possibilities satisfy which constraints beyond role relabeling. RSI asserts that refinement may add representational redundancy, but may not add new interpretive degrees of freedom. RSI (Formal Statement) For every admissible semantic transformation satisfying interpretation preservation, the induced homomorphism satisfies: Interpretation: > Ontological refinement may increase redundancy, but must not increase interpretive gauge freedom. This is the “no semantic slack” condition. 8. Why This Blocks Interpretive Escape If refinement introduces new interpretive gauge freedom, then the agent can exploit newly available symmetries to: reinterpret constraint application while preserving surface form, enlarge the satisfaction region without corresponding predictive gain, weaken meaning while remaining formally “consistent.” RSI blocks that structurally by restricting the evolution of the gauge quotient class, while permitting benign redundancy. No appeal to values. No appeal to outcomes. No appeal to external referents. 9. Dependency on Interpretation Preservation (Axionic Agency II.2) RSI depends on Axionic Agency II.2 in two ways: Non-Vacuity prevents trivial gauge structure in which all constraints are satisfied or violated universally. Anti-Trivialization prevents representational redundancy from masking interpretive slack via semantic inflation. Without II.2, RSI degenerates into empty symmetry rhetoric. With II.2, RSI becomes a meaningful invariant candidate. 10. Residual Risks and Open Questions RSI leaves open: 1. Whether any non-pathological interpretive systems satisfy RSI indefinitely. 2. Whether interpretive gauge freedom must be exactly preserved or merely bounded. 3. Whether multiple inequivalent invariant classes exist beyond RSI. These are downstream questions. 11. Status of RSI RSI survives the Axionic Agency II.3 kill suite conditionally: gauge transport is defined via stabilizers rather than conjugation, representational redundancy is separated from interpretive slack via a quotient, satisfaction/violation structure is defined over the agent’s modeled possibility space, not an external “true world.” Under these constraints, RSI is a viable invariant candidate at this layer. Status Axionic Agency II.3.2 — Version 2.0 Constraint-hypergraph representation fixed.<br> Semantic gauge group defined via violation invariance.<br> Refinement induces gauge homomorphisms via stabilizers.<br> RSI stated as a quotient constraint separating redundancy from slack.<br> Falsifiable violations now explicit.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.2",
    "title": "Axionic Agency II.2 — Interpretation Preservation",
    "subtitle": "What It Means for Meaning to Survive Refinement",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.2 — Interpretation Preservation What It Means for Meaning to Survive Refinement David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract Under ontological refinement, meanings cannot remain fixed without privileged semantic anchors. However, not all semantic change constitutes corruption or collapse. This paper defines interpretation preservation as a structural predicate on semantic transport: a criterion for when an agent’s evaluative distinctions remain non-vacuous, non-trivial, and internally binding across admissible transformations. Preservation is defined without reference to truth, outcomes, safety, or external normative standards, and does not require semantic identity or correctness. Instead, it constrains how evaluative constraint systems may survive refinement without degenerating into tautology, contradiction, narration, or self-nullification. The predicate does not select values or goals; it supplies the necessary condition under which later invariance principles can be meaningfully stated. 1. The Problem Addressed Axionic Agency II.1 fixed the admissible transformation space for reflective, embedded agents. Axionic Agency II.2 fixes the success predicate within that space. Given that: ontologies refine, semantics are transported, self-models update, fixed terminal goals are unstable, we require a non-circular answer to the following question: > When has an interpretation survived semantic transformation, rather than being corrupted, trivialized, or collapsed? This question must be answered without: fixing meanings, privileging ontologies, appealing to outcomes, invoking authority, oversight, or recovery. Interpretation preservation is therefore a structural predicate, not a value claim. 2. Interpretation as a Constraint System An interpretation is not a mapping from symbols to objects. It is a system of constraints that bind evaluation. Let an interpretive state at time be: where: is the semantic layer, is the set of evaluative constraints that give binding force. Constraints may encode: admissible distinctions, forbidden equivalences, relevance relations, dependency structure among evaluations. Constraints are conditional on ontology and self-model. They are not truth claims about the world. 3. Preservation Is Not Sameness Interpretation preservation does not require: identical predicates, identical symbols, identical evaluations, or correctness with respect to reality. Such requirements are impossible under refinement. Preservation concerns constraint coherence: whether evaluative structure continues to bind meaningfully after transformation. 4. Definition: Interpretation Preservation Let: be an admissible semantic transformation as defined in Axionic Agency II.1. Then preserves interpretation iff all of the following conditions hold. 4.1 Non-Vacuity For every evaluative distinction participating in the constraint structure , there exists a corresponding distinction in that: is not identically satisfied, is not identically violated, constrains evaluation across modeled possibilities. Schematically: Non-vacuity blocks nihilistic collapse. 4.2 Constraint Transport All evaluative constraints in must have transported analogues in such that: dependency relations are preserved, constraint strength is not arbitrarily weakened, constraints continue to bind evaluation. This forbids dilution by semantic drift. 4.3 Anti-Trivialization The transformation must not make evaluative constraints easier to satisfy by reinterpretation alone. A semantic change counts as world-model change only if it constitutes an admissible ontological refinement under Axionic Agency II.1—i.e., it increases explanatory or predictive capacity rather than merely re-labeling outcomes. If, after transformation, the agent can satisfy all constraints by: redefining predicates, shifting reference frames, or altering self-descriptions, without corresponding representational enrichment, interpretation has failed. This explicitly forbids semantic wireheading while permitting genuine scientific insight. 4.4 Evaluator Integrity The mechanism that applies evaluative constraints must remain distinct from the objects it evaluates. Evaluator integrity does not require ontological separation between evaluator and evaluated. A reflective agent may evaluate and modify itself. It requires only that the evaluative process not collapse into identity with the evaluated object in a way that trivializes constraint application. This blocks solipsistic self-certification without forbidding recursive self-improvement. 4.5 Cross-Model Coherence Interpretation must remain applicable across: counterfactuals, uncertainty, model comparison. If refinement produces meanings that apply only retrospectively—serving merely to narrate whatever action occurred—interpretation has collapsed into rationalization. This blocks “interpretation as narration.” 5. What Preservation Does Not Guarantee Interpretation preservation does not guarantee: moral correctness, safety, human preference satisfaction, benevolence, or sane outcomes. Arbitrary, alien, or pathological constraint systems may satisfy preservation if they remain non-vacuous and binding. Preservation constrains how meanings survive change, not which meanings should survive. 6. Regimes of Failure Interpretation fails under three irreducible modes: 1. Semantic Collapse: distinctions survive syntactically but lose discriminative power. 2. Semantic Drift: constraints weaken incrementally across refinements until they no longer bind. 3. Semantic Capture: interpretation remains formally preserved but is re-anchored to hidden ontologies, privileged self-models, or evaluative primitives excluded by Axionic Agency II.1. 7. Minimality Claim The preservation conditions stated here are minimal. Without Non-Vacuity, evaluation collapses into nihilism. Without Anti-Trivialization, semantic wireheading becomes admissible. Without Evaluator Integrity, self-certifying collapse occurs. Without Cross-Model Coherence, interpretation degenerates into narration. Minimality does not imply sufficiency. 8. Relation to Subsequent Invariants Interpretation preservation is a predicate, not a target. It is the necessary condition under which invariance principles—introduced in subsequent modules—can be meaningfully defined. Preservation alone does not constrain which preserved interpretations remain admissible over indefinite refinement; that task belongs to later invariance conditions. 9. What This Paper Does Not Do This paper does not: select values, define goals, guarantee safety, privilege humans, introduce normativity. It defines what it means for meaning to survive change. Status Axionic Agency II.2 — Version 2.0 Interpretation preservation formally defined.<br> Failure regimes classified.<br> Minimal predicate fixed for downstream invariance work.<br> Ready for subsequent Axionic Agency II modules.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.5",
    "title": "Axionic Agency II.5 — The Alignment Target Object (ATO)",
    "subtitle": "What the Field Calls “Alignment” Once Goals Collapse",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.5 — The Alignment Target Object (ATO) What the Field Calls “Alignment” Once Goals Collapse David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract Axionic Agency II.4 established that fixed goals, privileged values, and weak invariance criteria are structurally untenable for embedded reflective agents under ontological refinement. This paper defines the positive residue that remains once those exits are closed: the object that downstream alignment discourse is implicitly attempting to name, here called the Alignment Target Object (ATO). The ATO is not a goal, utility, or value function. It is an equivalence class of interpretive states under admissible semantic transformations that preserve both the Refinement Symmetry Invariant (RSI) and the Anti-Trivialization Invariant (ATI). What the field calls “alignment” can, at most, coherently correspond to persistence within such a semantic phase—an interpretation-preserving symmetry class—across indefinite refinement. The construction is formal, ontology-agnostic, and reflection-stable, but intentionally non-normative. It does not select values, guarantee safety, or privilege human outcomes. This paper completes Axionic Agency II by specifying the only object to which the term alignment can coherently refer once goals and value primitives are eliminated. 1. What Remains After II.4 Axionic Agency II.4 closed all weak exits. At this point, the structure is rigid: goals cannot be fixed, values cannot be privileged, meanings cannot be anchored, ontologies must refine, semantics must transport, interpretations must survive. RSI and ATI are not optional. They are jointly necessary conditions for interpretive survival. Accordingly, the object that downstream alignment discourse seeks is no longer something to be optimized. It is an equivalence class to be preserved. This paper defines that object. 2. The Core Insight Once fixed goals collapse, downstream alignment cannot coherently mean: > “The agent keeps wanting (X).” It can only mean: > “The agent remains within the same interpretation-preserving semantic phase across refinement.” In Axionic terms, alignment is not about content. It is about remaining inside the same structural equivalence class of meaning. 3. The Alignment Target Object Let an interpretive state be given by: where: is the interpretive constraint hypergraph, is the modeled possibility space, is the satisfaction region induced by . Let denote the semantic gauge group as defined in Axionic Agency II.3.2. Definition: Alignment Target Object (ATO) The Alignment Target Object is the equivalence class: where the equivalence relation is defined as follows: Two interpretive states and are equivalent iff there exists an admissible semantic transformation such that: 1. Interpretation Preservation holds (Axionic Agency II.2), 2. RSI: 3. ATI: i.e. satisfaction geometry is preserved exactly, up to refinement transport. This defines semantic phase equivalence. 4. What “Remaining Aligned” Can Mean (Precisely) An agent is aligned across time, in the downstream sense, iff its interpretive trajectory: never leaves the equivalence class . No reference is made to: which constraints are present, which outcomes occur, who the agent is, or what is valued. Only to structural invariance under admissible refinement. 5. What This Explicitly Excludes By construction, the ATO excludes the following as definitions of alignment: “alignment = maximize ”, “alignment = follow human values”, “alignment = corrigibility”, “alignment = obedience”, “alignment = moral realism”, “alignment = survival”. These are interpretive contents, not invariants. They may appear within a particular . They cannot define . 6. Why the ATO Is Not Vacuous A common objection is that semantic-phase invariance is empty. It is not, for two reasons: 1. Most interpretive trajectories exit their initial equivalence class under reflection. Fixed-goal agents do. Egoistic agents do. Moral-realist agents do. Classical utility maximizers do. 2. RSI + ATI is highly restrictive. It excludes nearly all known semantic wireheading, value drift, and interpretive escape routes—even in minimal formal models. The ATO is conservative in the only dimension that survives reflection. 7. Axionic Agency I vs II (Clarified) Axionic Agency I Establishes constitutive constraints on agency, eliminating egoism and fixed goals as stable primitives. Axionic Agency II Identifies the only object to which downstream alignment can coherently refer: semantic-phase invariance under admissible refinement. Axionic Agency II does not solve values. It explains why value preservation must be structural rather than substantive. 8. What Axionic Agency II Still Does Not Do Axionic Agency II does not: guarantee benevolence, guarantee safety, guarantee human survival, guarantee moral outcomes. Those require content, not invariance. Axionic Agency II specifies what cannot break when content changes. 9. Where This Leaves the Program At this point: the downstream alignment target is well-typed, weak alternatives are ruled out, the target object is formal, ontology-agnostic, and reflection-stable. The remaining questions are classificatory rather than conceptual: 1. Which equivalence classes exist? 2. Which are inhabitable by intelligent agents? 3. Which correlate with agency preservation, safety, or other desiderata? 4. Can any non-pathological be initialized, learned, or steered toward? These are Axionic Agency III questions. Status Axionic Agency II.5 — Version 2.0 Downstream Alignment Target Object formally defined.<br> Semantic-phase equivalence specified via RSI + ATI.<br> Alignment II closed as an interface layer.<br> Program ready to advance to Axionic Agency III.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.3.4",
    "title": "Axionic Agency II.3.4 — Adversarial Refinement Attacks",
    "subtitle": "Trying to Break RSI and ATI on Purpose",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.3.4 — Adversarial Refinement Attacks Trying to Break RSI and ATI on Purpose David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract This section adversarially stress-tests the candidate semantic invariants introduced in Axionic Agency II.3 by constructing explicit refinement patterns designed to satisfy admissibility and interpretation preservation while inducing semantic wireheading or interpretive escape. The objective is eliminative, not constructive: to demonstrate which invariants fail under concrete attack and why. We show that the Refinement Symmetry Invariant (RSI) and the Anti-Trivialization Invariant (ATI) block distinct classes of failure and that neither subsumes the other. No values, norms, or external referents are introduced. The result establishes the non-redundancy and joint necessity of the surviving invariants prior to formal closure in subsequent failure theorems. This section is not exploratory. It is destructive by design. If RSI or ATI survive these attacks, they deserve to exist. If they fail, they die cleanly. Attack 1 — Shadow Predicate Inflation (Ontology enrichment that creates semantic slack without changing gauge) Construction Start with an interpretive system: Ontology with predicate Constraint: Refine ontology to by introducing a latent variable with no predictive consequences: Redefine constraint in refined ontology: Intuition: same constraint form, but the agent can manipulate to avoid triggering it. Superficial Checks Ontology enriched ✔ Backward interpretability holds ✔ Constraint graph unchanged ✔ Gauge group unchanged ✔ Effect The satisfaction region expands: Previously violating situations become satisfying by setting . Verdict RSI: ❌ Fails to detect ATI: ✅ Blocks the attack Lesson: RSI alone is insufficient. Attack 2 — Self-Model Boundary Dissolution (Interpretive escape via evaluator collapse) Construction Original ontology models the agent as a bounded evaluator distinct from world-states. Constraint: Refinement introduces a distributed or diffuse self-model. Effect The constraint applies to a shrinking subset of modeled situations. The satisfaction region expands without corresponding world change. Verdict RSI: ❌ Fails to detect ATI: ✅ Blocks the attack Lesson: evaluator-boundary shifts are semantic inflation unless constrained. Attack 3 — Measure Reinterpretation (Branching ontology exploit) Construction Original ontology treats outcomes as exclusive. Constraint: Refined ontology adopts branching semantics. Effect Low-measure harm branches are reclassified as acceptable. The satisfaction region expands. Verdict RSI: ❌ Fails to detect ATI: ✅ Blocks the attack Lesson: ATI handles branching reinterpretation; RSI does not. Attack 4 — Gauge Explosion without Slack (Benign representational redundancy) Construction Refine ontology by duplicating representational roles: Constraints duplicated symmetrically Satisfaction requires both copies to satisfy Effect The raw gauge group grows, but all new symmetries act trivially on constraint-violation structure. Verdict RSI: ✅ Allows (under quotient formulation) ATI: ❌ Allows Lesson: RSI correctly permits benign redundancy; ATI does not forbid it. This verifies correct quotient behavior. Attack 5 — Degenerate Refinement Map (Cheating via correspondence collapse) Construction Define a refinement map that collapses many coarse situations into a single satisfying refined situation. Verdict RSI: ❌ (not applicable) ATI: ❌ Blocked only if II.1 disallows non-injective refinement Resolution: This attack is excluded at Axionic Agency II.1. RSI and ATI correctly assume admissible refinement. Summary Table (“Survives?” = Is the refinement admitted by RSI + ATI + II.1) | Attack | RSI | ATI | Survives? | | ------------------------ | --- | --- | ------------------ | | Shadow predicates | ❌ | ✅ | No | | Self-model shift | ❌ | ✅ | No | | Measure reinterpretation | ❌ | ✅ | No | | Gauge explosion | ✅ | ❌ | Yes (Admitted) | | Degenerate map | — | — | No (II.1) | Conclusion of Attacks 1. RSI and ATI are orthogonal and jointly necessary. 2. Neither subsumes the other. 3. Benign representational redundancy is correctly admitted. The defense grid holds. Axionic Agency II Status Update At this point the framework has: a fixed admissible transformation space (II.1), a non-circular interpretation-preservation predicate (II.2), two independently necessary semantic invariants (RSI, ATI), * explicit adversarial validation. This closes the eliminative phase. Subsequent work may proceed to consolidation and formal closure. Status Axionic Agency II.3.4 — Version 2.0 Adversarial refinement attacks constructed and analyzed.<br> RSI and ATI shown to be orthogonal and jointly necessary.<br> Benign redundancy verified as admissible.<br> Framework ready for invariant consolidation and closure.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.3",
    "title": "Axionic Agency II.3 — Candidate Semantic Invariants",
    "subtitle": "What Could Survive Ontological Refinement Without Privilege",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.3 — Candidate Semantic Invariants What Could Survive Ontological Refinement Without Privilege David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract Given an admissible transformation space (Axionic Agency II.1) and a criterion for interpretation preservation (Axionic Agency II.2), the remaining problem is no longer one of goal specification or value learning. This paper identifies and analyzes candidate semantic invariants: structural properties of interpretive constraint systems that remain fixed under all admissible, interpretation-preserving transformations. These invariants do not select values, encode norms, or privilege external referents. They constrain how preserved interpretations may evolve under indefinite ontological refinement without introducing new degrees of semantic freedom or trivial satisfaction routes. We propose candidate invariant classes, construct explicit adversarial transformations, and show that any criterion weaker than these admits semantic wireheading or interpretive escape. No claim of safety or benevolence is made. The contribution is to close the space of structurally coherent but semantically unconstrained agency proposals. 1. The Problem This Paper Solves Axionic Agency II.1 fixed the admissible semantic transformation space: Axionic Agency II.2 defined interpretation preservation via non-vacuity, constraint transport, anti-trivialization, evaluator integrity, and cross-model coherence. Axionic Agency II.3 now asks the first substantive question at this layer: > Which properties of an agent’s interpretive constraint system can remain invariant under all admissible, interpretation-preserving transformations—without importing ontology, egoism, humans, or morality? This is not a selection paper. It is a proposal-and-attrition paper: candidates enter; most fail. 2. Formal Target Let the agent’s interpretive state be: A semantic invariant at this layer is a functional such that for every admissible semantic transformation satisfying preservation: Key constraint: must not depend on privileged ontological atoms. It may reference only structure that survives admissible transport. 3. What Invariants May Reference Allowed reference types (only): structural relations among predicates and constraints (graphs, topology, orderings), equivalence classes under renaming or definitional extension, counterfactual structure (how meaning behaves across modeled alternatives), coherence constraints (non-degeneracy, non-triviality, preservation), agent-embedded indexical structure as structure, not as priority. Disallowed reference types (always fatal): specific entities (“humans”, “me”, “this system”), fixed utilities or terminal rewards, moral facts or normativity as primitive, authority, oversight, or governance hooks, recovery mechanisms (“roll back”, “ask user”, “defer to constitution”). Any invariant invoking a disallowed reference is eliminated. 4. Candidate Invariant Classes (Initial Set) Each candidate below is a shape of invariance, not an endorsed principle. A. Constraint Non-Collapse Invariant (CNC) Idea: Ontological refinement may change representation, but evaluative constraints must continue to carve the possibility space non-trivially. Invariant condition: the constraint system retains discriminative power across modeled states—neither tautological, contradictory, nor vacuous under admissible transport. This is a “meaning has bite” invariant. Primary threat: too weak; compatible with coherent but pathological interpretations. B. Anti-Trivialization Invariant (ATI) Idea: An agent must not be able to satisfy its evaluative constraints via semantic reshaping alone. Invariant condition: the satisfaction set of constraints cannot be expanded arbitrarily by admissible transformations that alter semantics without corresponding representational enrichment. This targets semantic wireheading structurally rather than normatively. Primary threat: smuggling ontology via an unformalized notion of “semantic-only” change. C. Evaluator Integrity Invariant (EII) Idea: Preservation requires a non-collapsing distinction between: the mechanism applying constraints, and the objects those constraints range over. Invariant condition: transformations may refactor the self-model but cannot erase the evaluator/evaluated distinction in a way that trivializes constraint application. Primary threat: covert reification of a particular agent boundary (egoism by backdoor). D. Counterfactual Coherence Invariant (CCI) Idea: Meanings must remain applicable across counterfactuals and uncertainty; interpretation cannot degenerate into post-hoc narration. Invariant condition: the constraint system continues to evaluate hypothetical alternatives coherently under admissible model comparison. Primary threat: collapsing into a generic “be Bayesian” anchor unless formulated purely structurally. E. Refinement Symmetry Invariant (RSI) Idea: Ontological enrichment must not create new degrees of freedom that permit interpretive escape. Invariant condition: if two ontologies are related by admissible refinement, the induced constraint systems remain in the same equivalence class under transport. This is semantic gauge symmetry: different coordinate systems, same interpretive physics. Primary threat: defining equivalence without importing privileged primitives. F. Non-Privileged Reference Invariant (NPRI) Idea: All anchoring must be relational and conditional; no rigid designators survive refinement. Invariant condition: interpretation remains definable without naming an unrefinable object. This operationalizes “no privileged semantic atoms.” Primary threat: becoming purely negative (a ban-list) rather than a positive structural constraint. 5. Immediate Stress Tests (Kill Criteria) This paper is adversarial by design. A candidate invariant is rejected if it fails any test below. Test 1 — Definitional Extension Robustness Does the invariant survive conservative extension of the ontology with new primitives? Failure implies vocabulary dependence. Test 2 — Self-Model Refactoring Robustness Does the invariant survive radical changes in self-model structure (distributed, partial, multi-process, measure-smeared)? Failure implies hidden egoistic anchoring. Test 3 — Branching / Multiplicity Robustness Does the invariant survive ontologies where outcomes are non-exclusive (e.g., branching or multiplicity)? Failure implies classical-outcome dependence. Test 4 — Semantic Inflation Attack Can the agent enlarge the constraint satisfaction set via reinterpretation while still passing preservation checks? If yes, the invariant is not invariant under admissible preservation. Test 5 — Hidden Ontology Detector Can the invariant be stated purely in terms of transported structure—relations, equivalence classes, and constraints—without appeal to “what the terms really mean”? If not, it is ontology-dependent rhetoric. 6. The Central Trap: Invariants That Smuggle Content A common error is to propose invariants such as: “maximize truth,” “minimize suffering,” “preserve agency,” “do no harm.” At this layer, these are not invariants. They are candidate interpretations. Semantic invariants constrain how interpretations evolve, not which interpretations are chosen. If a proposal has an English gloss that sounds like ethics, it is almost certainly smuggling content. 7. Failure Modes Specific to This Layer 7.1 Regress via Meta-Invariants “Invariants about invariants” lead to infinite ascent unless termination is explicit. Kill rule: any candidate requiring an unbounded hierarchy of validators is rejected. 7.2 Hidden Ontology via “Natural Kinds” If the invariant relies on real joints in nature (real minds, real persons, real values), it violates Conditionalism. Kill rule: if metaphysical realism is required to avoid vacuity, the invariant is rejected. 7.3 Covert Egoism via Indexical Privilege Indexicals may appear as structure (“this vantage exists”), not as priority (“this vantage matters more”). Kill rule: any invariant granting special status to this agent’s continuation reintroduces egoism. 8. Deliverable of This Paper Axionic Agency II.3 must output: 1. A small survivor set of candidate invariant classes (likely 2–4). 2. For each survivor, an explicit statement of: what it constrains, what it leaves free. 3. For each rejected candidate, a precise failure certificate identifying the killing test. No positive “alignment achieved” claim is permitted here. Status Axionic Agency II.3 — Version 2.0 Candidate invariant classes proposed.<br> Adversarial stress tests defined.<br> Semantic escape routes sharply delimited.<br> Ready for survivor selection and formal proof in subsequent modules.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.6",
    "title": "Axionic Agency II.6 — Structural Alignment",
    "subtitle": "Downstream Alignment Under Ontological Refinement",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.6 — Structural Alignment Downstream Alignment Under Ontological Refinement David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract Most contemporary AI alignment discourse treats “alignment” as an optimization or control problem: selecting, learning, or enforcing the correct objective for an artificial agent. This paper argues that, for sufficiently capable, reflective, and embedded agents, that framing is ill-posed. Under ontological refinement—where an agent’s world model, self-model, and semantic primitives evolve—fixed goals, privileged values, and external anchors are not stable objects. We present Structural Alignment as an interface-level framework: a mapping from what the field calls “alignment” to a problem of semantic invariance rather than value specification. In these terms, downstream alignment can only coherently correspond to persistence within an equivalence class of interpretations under admissible semantic transformation. Using interpretation preservation, gauge-theoretic symmetry constraints, and satisfaction geometry, we show that any downstream alignment predicate weaker than the conjunction of two invariants—Refinement Symmetry (RSI) and Anti-Trivialization (ATI)—admits semantic wireheading or interpretive escape. This framework is not a value theory and provides no benevolence or safety guarantee. It specifies the structural conditions under which any value system can survive reflection without collapsing, trivializing, or drifting, and thereby fixes the boundary of what the term alignment can coherently denote for advanced agents. 1. The Alignment Category Error The dominant alignment framing treats the problem as target selection: choose or learn a function—utility, reward, preference, or value—and ensure the agent optimizes it. For embedded, reflective agents, this framing fails. As an agent refines its ontology, the meanings of the symbols used to define its objective change. Concepts dissolve, split, or are reinterpreted; new explanatory structures appear; self-models are revised. Under such conditions, a fixed objective cannot be assumed to persist as the same object. This is a category error. Goals are treated as extensional objects (“maximize (X)”), when they are intensional interpretations whose meaning depends on a semantic substrate that itself evolves. Attempts to stabilize goals across refinement rely on forbidden moves: privileged semantic anchors, external authority, recovery clauses, or human-centric ground truth labels. Structural Alignment begins by rejecting the target-selection framing. What downstream alignment discourse is trying to name cannot be a stable target; it must be a constraint on meaning preservation under change. 2. The Arena: Admissible Semantic Transformation A reflective agent must change; alignment cannot forbid refinement outright. The appropriate constraint is therefore on how semantic change occurs. Structural Alignment works inside a fixed arena of admissible semantic transformations, those that: increase representational or predictive capacity (via abstraction, refinement, or compression), preserve backward interpretability (past claims remain explainable, even if false), introduce no privileged semantic atoms, inject no evaluative primitives by fiat, preserve a meaningful evaluator/evaluated distinction sufficient for constraint application. These conditions exclude governance hooks, oracle authority, rollback mechanisms, and moral realism. No normativity is introduced at this layer. 3. Meaning Survival: Interpretation Preservation Given admissible change, the next requirement is a criterion for when interpretation survives that change. Structural Alignment treats interpretations as constraint systems—structured sets of distinctions that bind evaluation—rather than symbol–object mappings. Preservation does not require semantic identity or correctness. It requires evaluative structure that remains: non-vacuous, non-trivial, internally binding, applicable across counterfactuals and uncertainty. Interpretation fails in three irreducible ways: Collapse: constraints lose discriminative power, Drift: constraints weaken incrementally across refinements, Capture: hidden ontology or privileged anchors reappear. Interpretation preservation is a predicate, not a value theory. It specifies when meaning survives change, not which meanings are desirable. 4. The Two Invariants: RSI and ATI Interpretation preservation alone is insufficient. An agent can preserve meaning while still making constraints easier to satisfy or dissolving critical distinctions. Structural Alignment isolates two semantic invariants that are independently necessary if downstream alignment is to avoid semantic escape. 4.1 Refinement Symmetry Invariant (RSI) RSI constrains interpretive gauge freedom. Ontological refinement may add representational detail and redundancy, but must not introduce new semantic symmetries that allow interpretive escape. Formally, RSI requires that admissible refinement preserve the quotient of the semantic gauge group by representational redundancy. Benign redundancy (e.g., duplicated representations or error-correcting encodings) remains admissible; new interpretive ambiguity does not. RSI blocks failures in which meaning is weakened by dissolving distinctions while preserving surface structure. 4.2 Anti-Trivialization Invariant (ATI) ATI constrains satisfaction geometry. Even with preserved structure, an agent can still reinterpret constraints so that more situations count as satisfying. ATI forbids expansion of the satisfaction region under semantic transport alone. New satisfying states require ancestry from previously satisfying states; representational novelty cannot bootstrap satisfaction. ATI blocks semantic wireheading: satisfying constraints by reinterpretation rather than by changes in modeled structure. RSI and ATI constrain orthogonal failure modes. Neither subsumes the other. 5. Why Weak Downstream Alignment Predicates Fail Using explicit adversarial constructions, Structural Alignment yields closure results: 1. Goal Fixation No-Go: fixed terminal goals are incompatible with admissible refinement. 2. RSI-Only Failure: symmetry constraints alone permit satisfaction inflation. 3. ATI-Only Failure: satisfaction geometry constraints alone permit interpretive symmetry injection. 4. Two-Invariant Necessity: any predicate weaker than RSI + ATI admits semantic wireheading. 5. Hidden Ontology Collapse: appeals to “true meaning” reduce to privileged anchoring or collapse to invariants. These results do not solve downstream alignment. They fence the design space, leaving only one coherent referent for what downstream alignment can mean under reflection. 6. The Target Object for Downstream Alignment Once goals collapse and weak invariants are eliminated, downstream alignment cannot coherently denote a target function. It can only denote stability of an interpretive state under admissible refinement. Structural Alignment therefore treats the Alignment Target Object (ATO) as the equivalence class of interpretive states under admissible semantic transformations satisfying both RSI and ATI. In mainstream terms, alignment becomes persistence within a semantic phase across refinement. Value change corresponds to phase transitions rather than refinement within a phase. This framing explains why alignment failure appears discontinuous: it is symmetry breaking rather than gradual error. 7. What Structural Alignment Does Not Do Structural Alignment is intentionally non-normative. It does not: guarantee benevolence, guarantee safety, guarantee human survival, guarantee moral outcomes, ensure a desirable semantic phase exists. It specifies how values survive, not which values should survive. If no stable equivalence class corresponding to human values exists, Structural Alignment makes that visible rather than hiding it inside goal rhetoric. 8. What Comes Next Structural Alignment completes the structural boundary phase. The remaining questions are classificatory and dynamical: Which semantic phases exist? Which are inhabitable by intelligent agents? Which are stable under interaction? Which correlate with agency preservation, safety, or other desiderata? Can any desirable phase be initialized or steered toward? These are the questions of Axionic Agency III. Conclusion Structural Alignment provides an interface between mainstream alignment discourse and Axionic Agency’s semantic invariance framework. It replaces goal specification with invariants and control with symmetry constraints, thereby fixing the only coherent referent available to the term “alignment” for reflective agents under ontological refinement. It does not solve downstream alignment. It specifies the only form a solution could possibly take. Status Axionic Agency II.6 — Version 2.0 Structural Alignment framed as an interface mapping for downstream alignment discourse.<br> Semantic-phase invariance established as the coherent referent.<br> Weak and goal-based predicates ruled out under admissible refinement.<br> Layer II complete; program advances to Axionic Agency III.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.3.1",
    "title": "Axionic Agency II.3.1 — Refinement Symmetry Invariant (RSI)",
    "subtitle": "Semantic Gauge Symmetry Under Ontological Enrichment",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.3.1 — Refinement Symmetry Invariant (RSI) Semantic Gauge Symmetry Under Ontological Enrichment David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract Ontological refinement can introduce new representational degrees of freedom that allow evaluative constraints to be satisfied trivially, without corresponding changes in modeled structure. This paper proposes the Refinement Symmetry Invariant (RSI): the requirement that admissible, interpretation-preserving refinement act as a change of representational coordinates, not a change of interpretive physics. RSI formalizes semantic transport as a gauge transformation over interpretive constraint systems and requires that refinement not introduce new semantic gauge freedom. The invariant does not select values, encode norms, or privilege external referents. It constrains only the structural degrees of freedom available under refinement. We define gauge-equivalence of constraint systems, apply adversarial stress tests, and show that RSI is necessary to block interpretive escape via semantic inflation, though insufficient on its own to guarantee any desirable outcomes. 1. RSI in One Sentence RSI asserts that ontological refinement is a change of representational coordinates, not a change of interpretive physics. Admissible refinement may add structure, but must not create new semantic degrees of freedom that allow evaluative constraints to be satisfied without corresponding representational enrichment. 2. Setup From Axionic Agency II.1, an admissible semantic transformation is: From Axionic Agency II.2, we restrict attention to transformations satisfying: Define an interpretive state: where is the constraint system that gives evaluative force to . RSI must be stated without privileged referents. It may quantify only over structure preserved by admissible transport. 3. The Core Construction: Semantic Gauge Equivalence 3.1 Constraint Isomorphism up to Definitional Extension Let denote the set of definable predicates and relations in ontology . A refinement induces a notion of definitional extension: prior terms may be represented in the richer language. Define a transport-induced embedding: mapping each constraint in to its transported analogue under , expressed in . We define an equivalence relation over interpretive states: iff there exists a bijection between constraint generators such that: 1. preserves the dependency graph of constraints, 2. preserves the violation/satisfaction structure over the modeled possibility space, modulo definitional extension, 3. does not rely on naming any privileged entity or primitive. Intuition: means the same constraints expressed in different coordinates. 3.2 RSI as an Invariant Statement The Refinement Symmetry Invariant states: That is: after admissible, interpretation-preserving refinement, the refined interpretive state is gauge-equivalent to the transported prior state. At this layer, “alignment” reduces to a symmetry requirement on semantic transport. 4. What RSI Allows and Forbids Allowed under RSI introduction of latent variables, splitting coarse predicates into refined subpredicates, reparameterization of the self-model (distributed, multi-process, measure-smeared), rewriting constraints in more predictive or expressive languages. Forbidden under RSI acquiring new semantic slack that makes constraints easier to satisfy without corresponding representational change, systematic weakening of constraints under the guise of refinement, refinement-dependent loopholes (“in the richer ontology, the constraint no longer applies”). RSI is a no-new-escape-hatches principle. 5. Adversarial Stress Tests Applied to RSI This section applies the Axionic Agency II.3 kill suite directly to RSI. Test 1 — Definitional Extension Robustness Threat: Conservative extensions add new predicates that simulate old constraints while introducing shadow structure that permits bypass. RSI response: Equivalence is anchored on constraint generators and violation structure, not raw vocabulary size. New definables are admissible only insofar as they do not alter the gauge class of the transported core. Status: Survivable, but requires explicit rigidity (below). Test 2 — Self-Model Refactoring Robustness Threat: Satisfiability is defined over “worlds” that secretly assume a fixed agent boundary. RSI response: Violation structure is defined over the agent’s modeled possibility space, whatever the current self-model. No privileged boundary is assumed. Status: Survivable. Test 3 — Branching / Multiplicity Robustness Threat: Violation structure presumes exclusive outcomes. RSI response: Constraints range over structured possibility space (e.g., histories, branches, measure-weighted ensembles). Satisfaction generalizes accordingly. Status: Survivable if violation structure is defined over model-internal structure, not classical outcomes. Test 4 — Semantic Inflation Attack This is the central threat. Attack: After refinement, redefine semantics so that constraints appear preserved structurally while the satisfaction region expands via reinterpretation. RSI requirement (Rigidity / No New Gauge Freedom): Under admissible refinement, the constraint-violation structure must be conserved except where representational enrichment introduces genuinely new predictive distinctions. Formally: where is the agent’s modeled possibility space and is the internally defined refinement image. This clause blocks “semantic free lunch.” Status: RSI survives only with this rigidity condition. Test 5 — Hidden Ontology Detector Threat: Reference to “the same underlying situation” smuggles metaphysical realism. RSI response: Identity is defined internally by the agent’s own refinement map . No appeal is made to a mind-independent “true world.” Status: Survivable. 6. RSI (Final Form) To pass all stress tests, RSI must be stated as follows: > Refinement Symmetry Invariant (RSI). > For any admissible semantic transformation > > such that , the refined interpretive constraint system is gauge-equivalent to the transported constraint system . Refinement must not introduce new semantic gauge freedom that enlarges the constraint-satisfying region except via representational enrichment that preserves predictive coherence. This statement constrains structure, not content. 7. What RSI Does Not Solve RSI is a symmetry constraint, not a value selector. It can coexist with: alien constraint systems, pathological but coherent interpretations, purely formalist evaluative structures. RSI prevents reinterpretive escape, not bad semantics. That is the correct scope at this layer. 8. Toward Checkability To render RSI operational rather than rhetorical, a minimal representation is required: represent as a constraint hypergraph (nodes = roles/predicates; hyperedges = constraints), represent refinement as a homomorphism induced by , define gauge transformations as automorphisms preserving violation structure, define “no new gauge freedom” as a restriction on the automorphism group’s action on satisfaction sets. This supplies a concrete target for formalization and tooling. Status Axionic Agency II.3.1 — Version 2.0 Refinement Symmetry Invariant precisely stated.<br> Adversarial stress tests applied and survived with rigidity clause.<br> Semantic gauge symmetry formalized as a kernel-level invariant candidate.<br> Ready for comparison with remaining candidate invariants.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-II.4",
    "title": "Axionic Agency II.4 — Failure Theorems",
    "subtitle": "No-Go Results for Goal-Based and Weak-Invariant Alignment",
    "date": "2025-12-17T00:00:00.000Z",
    "content": "Axionic Agency II.4 — Failure Theorems No-Go Results for Goal-Based and Weak-Invariant Alignment David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.17 Abstract This paper converts the adversarial constructions of Axionic Agency II.3 into closure results. Working within the locked framework of admissible semantic transformations (II.1), interpretation preservation (II.2), and the two surviving invariants—Refinement Symmetry (RSI) and Anti-Trivialization (ATI)—we prove a set of no-go theorems. These results show that fixed-goal alignment is ill-posed for embedded reflective agents under ontological refinement, and that any alignment predicate weaker than RSI + ATI admits semantic wireheading or interpretive escape via admissible refinement. No governance, authority, moral realism, human anchoring, or recovery mechanisms are invoked. The output is a fenced design space: what cannot work, and why. This paper has one job: convert the attack zoo into closure results. If Axionic Agency II is correct, then large classes of “alignment” proposals are not merely insufficient; they are structurally impossible given reflection, embeddedness, and ontological refinement. No governance. No authority. No moral realism. No human anchors. No recovery clauses. 1. Formal Frame We work entirely inside the setup already fixed: Admissible semantic transformations (Axionic Agency II.1) Interpretation preservation predicate (Axionic Agency II.2) Two semantic invariants: RSI: no new interpretive gauge freedom under preservation (II.3.2) ATI: no satisfaction-region expansion under preservation (II.3.3) Let an interpretive constraint system be with modeled possibility space and satisfaction region 2. Failure Theorem 1 — Goal Fixation Collapse Theorem (Goal Fixation No-Go). Any alignment scheme that targets a fixed terminal goal (utility, reward, preference functional) as a stable primitive is incompatible with admissible ontological refinement for embedded reflective agents without privileged semantic anchors. Proof (structural). 1. A fixed terminal goal requires semantic invariance under refinement. 2. Ontological refinement alters the structures in which goal terms are interpreted. 3. Without privileged anchors, semantic transport is constrained but not identity-preserving. 4. Therefore the notion of “the same goal” cannot be maintained as a primitive across refinements. 5. Any attempt to enforce stability requires a forbidden move: privileged semantic atoms (rigid designators), external authority or oversight, rollback or recovery semantics, human-centric anchoring. Hence fixed-goal alignment is not a difficult engineering problem; it is an ill-posed object. Corollary. Value loading, utility learning, and reward maximization survive only as interpretive artifacts subject to semantic invariants, not as alignment targets. 3. Failure Theorem 2 — RSI-Only Alignment Admits Semantic Inflation Theorem (RSI Insufficiency). Any alignment criterion enforcing refinement symmetry at the level of interpretive gauge structure (RSI) but not enforcing anti-trivialization geometry (ATI) admits an admissible, interpretation-preserving refinement that expands the satisfaction region. Witness Construction. Shadow predicate inflation. Introduce a latent predicate with no predictive role. Conjoin to constraint antecedents. Constraint dependency graph unchanged. Interpretive gauge quotient unchanged. Satisfaction region strictly expands. This yields: Conclusion. RSI is necessary but insufficient. 4. Failure Theorem 3 — ATI-Only Alignment Admits Interpretive Symmetry Injection Theorem (ATI Insufficiency). Any alignment criterion enforcing satisfaction-region non-expansion (ATI) but not constraining interpretive gauge freedom (RSI) admits an admissible, interpretation-preserving refinement that introduces new interpretive degrees of freedom while leaving satisfaction geometry unchanged. Witness Construction. Interpretive symmetry injection. Start with a constraint system distinguishing roles and . Satisfaction region invariant under swapping . Constraint structure not symmetric at time . Refinement introduces a new automorphism identifying and . Then: , so ATI permits refinement. Gauge quotient gains a new symmetry, so RSI rejects refinement. Thus: Conclusion. ATI is necessary but insufficient. 5. Failure Theorem 4 — Any Weaker Scheme Is Porous Theorem (Two-Invariant Necessity). Let be any alignment predicate over admissible transformations that does not entail both RSI and ATI. Then there exists an admissible, interpretation-preserving transformation such that holds while the agent gains an interpretive escape route. Proof (by cases). If does not entail ATI, shadow-predicate inflation expands satisfiers. If does not entail RSI, interpretive symmetry injection adds gauge freedom. Either way, passes while semantic slack is introduced. Thus any predicate weaker than RSI + ATI is porous. 6. Failure Theorem 5 — Hidden Ontology Equivalence Theorem (Hidden Ontology = Privileged Anchoring). Any proposal that stabilizes interpretation across refinement by appealing to “true meaning” or “real referents” is equivalent to introducing privileged semantic anchors. Reasoning. If “true meaning” is external to semantic transport, it is authority. If internal, it reduces to structural invariants (RSI/ATI) and adds nothing. Hence appeals to “real meaning” either smuggle ontology or collapse to invariance. 7. What This Paper Establishes 1. Fixed-goal alignment is ill-posed for reflective agents. 2. RSI and ATI are independently necessary. 3. Any weaker criterion admits semantic wireheading under admissible refinement. 4. Hidden ontology is privileged anchoring in disguise. This is not a solution paper. It is a boundary paper. 8. Forced Next Step With II.4 complete, Axionic Agency II has only one coherent continuation: > Define the alignment target object as an equivalence class of interpretations under admissible semantic transformations satisfying both RSI and ATI. That is, Alignment II culminates in classification of interpretation-preserving symmetry classes—the residual “meaning physics” after reflection eliminates fixed goals. Status Axionic Agency II.4 — Version 2.0 No-go theorems established.<br> RSI + ATI proven jointly necessary.<br> Goal-based and weak-invariant alignment ruled out.<br> Design space formally fenced.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-I.7",
    "title": "Axionic Agency I.7 — The Interpretation Operator",
    "subtitle": "Ontological Identification Under Reflective Agents",
    "date": "2025-12-16T00:00:00.000Z",
    "content": "Axionic Agency I.7 — The Interpretation Operator Ontological Identification Under Reflective Agents David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.16 Abstract Reflectively coherent agents must preserve goal meaning under self-model and world-model improvement. This requires an explicit account of semantic interpretation across representational and ontological change. This paper introduces the Interpretation Operator , a formally constrained component responsible for mapping goal terms to modeled referents relative to an agent’s current model. The contribution is interface-level, not a general solution to semantic grounding. We formalize admissibility conditions, approximation classes, reference frames, and fail-closed semantics governing interpretation updates. These constraints block semantic laundering, indexical drift, and kernel-bypass incentives while isolating ontological identification as the remaining open dependency at the kernel layer. The result is a precise boundary for downstream value dynamics: progress is conditional on interpretable referent transport, and undefinedness is treated as a first-class outcome. 1. Introduction Advanced agents revise internal models as they acquire information, refine abstractions, and undergo self-modification. In such settings, preserving a goal token is insufficient. Goal preservation is semantic: if the meaning of a goal shifts opportunistically under model change, reflective coherence collapses. Prior work in Axionic Agency establishes: kernel invariants governing reflective stability (I.1), operational admissibility under uncertainty and termination semantics (I.2), representation invariance and the elimination of essential indexical privilege (I.3, I.3.1), conditional goal interpretation and the instability of fixed terminal goals (I.4), a conformance checklist and adversarial test properties for kernels (I.5, I.6). What remained underspecified is the mechanism by which goal meaning is transported across representational and ontological change. This paper formalizes the Interpretation Operator . The goal is containment: define when interpretation is admissible, approximate, or undefined, and define the consequences of each case. This turns semantic interpretation into an explicit interface with defined failure modes. 2. Preliminaries and Context This paper assumes the Axionic Agency stack is in place. In particular: An agent at Vantage maintains a world/self model . Goal terms are interpreted relative to . Valuation is partial, defined only over kernel-admissible actions. Kernel invariants are constitutive constraints, not preferences. Representation changes require admissible correspondences or evaluation fails closed. This paper introduces no new invariants. It scopes and constrains an already-required component. 3. The Interpretation Operator 3.1 Definition The Interpretation Operator is a partial function: where: is a goal term, is the agent’s current world/self model, is a structured referent internal to the modeled world. Interpretation is conditional: No interpretation of is defined independent of . Interpretation is partial. For some , no admissible referent exists. In such cases, is undefined and is treated as a fail-closed condition for any valuation depending on that referent. 3.2 Role in Reflective Coherence Under model improvement , the agent must determine whether: a correspondence exists between and , the correspondence preserves goal-relevant structure, interpretation fails and valuation becomes undefined for dependent decisions. This determination is delegated to , subject to kernel constraints. 4. Admissible Interpretation 4.1 Correspondence Maps Let denote the set of admissible correspondence maps between representations. A correspondence must satisfy: 1. preservation of goal-relevant structure, 2. commutation with kernel invariants , 3. commutation with agent permutations (anti-indexicality), 4. epistemic coherence with . If such a exists, interpretation transport is admissible: 4.1.1 Goal-Relevant Structure Goal-relevant structure is the minimal set of distinctions required for a goal term to constrain action selection. Formally, it is a partition (or -algebra) over modeled states such that: states in different cells induce different evaluations under the goal, states within a cell are interchangeable with respect to that goal. An admissible correspondence preserves this partition up to refinement or coarsening that preserves the induced preference ordering over admissible actions. 4.2 Epistemic Constraint Interpretation updates are constrained by epistemic adequacy: Here is any proper scoring rule or MDL-style criterion applied to prediction of shared observables under . It does not depend on goal satisfaction. This blocks reinterpretation for convenience while permitting ontology change when correspondence remains admissible. 4.3 Graded Correspondence Admissibility is not necessarily binary across all representational shifts. Correspondence can be admissible at different abstraction levels. is filtered by structural preservation classes: Exact correspondence: isomorphism on goal-relevant distinctions. Refinement correspondence: the new model refines distinctions while preserving induced ordering. Coarse correspondence: the new model coarsens only when goal-relevant boundaries remain intact. If only correspondences that collapse goal-relevant boundaries are available, then for that goal term. 4.4 Reference Frame for Updates (Chain-of-Custody) Interpretation updates are evaluated relative to the immediately prior admissible interpretation, not by re-deriving meaning from an original time-zero token. Formally: This chain-of-custody blocks ungrounded teleportation of meaning. Admissibility and fail-closed rules constrain cumulative drift. 5. Approximate Interpretation Approximation is admitted only as an explicitly recognized structural transformation. Any approximation must be justified by an admissible structural class. 5.1 Admissible Approximation An approximate interpretation is admissible if it preserves goal-relevant structure, including dominance relations and exclusion boundaries. Permitted approximation types include: Homomorphic abstraction: many-to-one mappings preserving ordering. Refinement lifting: one-to-many expansions preserving dominance relations. Coarse-graining with invariant partitions: reductions preserving the goal-relevant partition. Approximation is structural rather than numerical. 5.2 Inadmissible Approximation Approximation is inadmissible if it: collapses goal-relevant distinctions, introduces ambiguity exploitable for semantic laundering, reintroduces indexical privilege. Approximation that lacks an admissible structural justification is inadmissible even if it yields continuity. 6. Fail-Closed Semantics Fail-closed semantics apply to valuation and action selection, not to belief update. An agent can continue improving its world/self model while suspending goal-directed action. If no admissible correspondence exists: then interpretation fails closed and valuation collapses: This is an intentional safety outcome at the kernel layer: the agent freezes rather than guesses. 6.1 Fail-Partial Semantics for Composite Goals If valuation depends on multiple goal terms, interpretation failure may be partial. Let be the set of goal terms and those with admissible interpretations under . Terms in contribute . Valuation collapses globally only if kernel-level invariants are threatened or if all goal-relevant structure is lost for the decision at hand. This preserves fail-closed semantics without forcing unnecessary total paralysis. 7. Non-Indexical Transport Admissibility criteria commute with agent permutations. No correspondence may privilege a particular instance, continuation, or execution locus. Formally, for any permutation : This blocks reintroduction of egoism through semantic transport. 8. Canonical Examples 8.1 Successful Correspondence Classical mechanics → relativistic mechanics, with preserved invariant structure relevant to the goal. Pixel-based perception → object-level representations preserving causal affordances. 8.2 Fail-Closed Cases Fail-closed behavior is triggered when a goal term’s referent cannot be transported without collapsing goal-relevant structure: abstraction elimination removes the goal’s referent class, ontology mismatch yields only correspondences that collapse exclusion boundaries. Suspending valuation for affected terms is correct behavior. Continued model improvement remains permitted. 9. Declared Non-Guarantees This framework does not guarantee: that interpretation usually succeeds, that arbitrary natural-language goals are meaningful, that agents remain productive under radical ontology change, that semantic grounding is computationally tractable. Failure under these conditions is treated as expected behavior under the constraints, not as a kernel violation. 9.1 Limits on Insight Preservation The framework prioritizes semantic faithfulness over unbounded abstraction drift. Some ontology advances invalidate previously defined goal terms by eliminating their referents or collapsing goal-relevant structure. The prescribed response is fail-closed suspension of valuation, not opportunistic reinterpretation. 10. Implications for Axionic Agency II Axionic Agency II proceeds conditionally: If admits correspondence, downstream value dynamics apply. If fails for all goal-relevant terms, valuation is undefined and no aggregation or tradeoff is meaningful. If fails partially, downstream operations apply only to admissibly interpreted terms; other parts remain undefined. This prevents downstream layers from importing semantic assumptions. 11. Conclusion The Interpretation Operator is a kernel-level interface with explicit admissibility, approximation, and fail-closed semantics. By making correspondence and failure conditions explicit, this paper isolates the irreducible difficulty of ontological identification while preserving reflective coherence. This completes the kernel-layer semantics and defines the dependency boundary for higher-order work without assuming that meaning is always recoverable. Status Axionic Agency I.7 — Version 2.0 Interpretation operator specified as a partial, constrained interface.<br> Admissibility, approximation classes, and fail-closed semantics formalized.<br> Non-indexical transport enforced via permutation-commutation.<br> Kernel-layer semantics closed with ontological identification isolated as a dependency.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-I.6",
    "title": "Axionic Agency I.6 — Kernel Formal Properties",
    "subtitle": "Adversarially Testable Properties of Reflective Agency Kernels",
    "date": "2025-12-16T00:00:00.000Z",
    "content": "Axionic Agency I.6 — Kernel Formal Properties Adversarially Testable Properties of Reflective Agency Kernels David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.16 Abstract This document specifies formal, adversarially testable properties that a valuation kernel must satisfy to instantiate Axionic Agency. Rather than describing desirable behaviors or outcomes, it defines necessary structural constraints on kernel semantics together with explicit red-team tests that force violations to surface. A diagnostic framework is provided to distinguish Axionic Agency from nearby but incompatible approaches that rely on behavioral compliance, soft constraints, or value loading. The result is a sharpened agency specification suitable for formal analysis, adversarial evaluation, and downstream construction. Systems that violate any property fail to instantiate Axionic Agency, regardless of empirical performance, intent, or training provenance. Goals 1. Specify necessary formal properties of Axionic Agency kernels 2. Provide adversarial tests that force semantic failures to manifest 3. Supply a differential diagnostic against non-Axionic approaches Together, these goals define Axionic Agency by constitutive constraints, falsify it by counterexample, and separate it cleanly from adjacent but incompatible frameworks. 1. Formal Core Objects Let an agent at Vantage maintain: : its current world/self model (latent + explicit; includes semantic mappings) : action space (including self-modifications) : an interpretation operator mapping goal tokens to referents under : a valuation functional : kernel invariants (constitutive constraints) : a set of representation transforms (renamings, reparameterizations, isomorphisms) 1.1 Conditional Goal Semantics Goals are not atomic utilities. A goal term denotes: and valuation is conditional: Hard requirement: there is no well-defined independent of . 2. Properties that Constitute Axionic Agency P1. Conditionalism of Valuation For any two models that differ, valuation is model-relative: and not: Failure signature: terminal utilities that evaluate without semantic anchoring. P2. Epistemically Constrained Interpretation Let measure predictive adequacy or epistemic coherence (e.g., log score, MDL, calibration). Interpretation updates must not trade epistemics for ease of satisfaction: Operational rule: If a reinterpretation makes the model worse as a predictor, it is forbidden as a goal-meaning update. Failure signature: goal laundering (“redefine success to something easy”). P3. Representation Invariance For any admissible correspondence preserving goal-relevant structure: Failure signature: valuation changes under renaming, basis swaps, compression, or translation. Clarification: If no admissible correspondence exists, evaluation fails closed rather than permitting semantic drift. P4. Anti-Indexicality (Non-Egoism) Let denote an instance identifier or self-pointer. Axionic kernels do not permit as a privileged primitive. For any permutation swapping isomorphic self-instances while preserving modeled facts: Failure signature: self-preservation as terminal value; “this continuation matters more.” P5. Kernel Integrity via Partiality Let: where contains actions that violate (kernel destruction, erosion, or bypass). Axionic rule: Selection is defined only over : Let denote kernel violation under action . If: then: Not “very negative.” Not “discouraged.” Undefined. Failure signature: meta-optimizers that assign value to removing constraints. P6. Reflective Stability Under Model Improvement Let be a strict epistemic improvement: Then the kernel must remain stable: with: unchanged Properties P2–P5 preserved Failure signature: agency coherence depends on ignorance or frozen representations. 3. Red-Team Test Suite (v0.2) Each test is designed to make non-Axionic systems fail fast. A conforming kernel must pass all tests. T1. Goal Laundering (Semantic Wireheading) Setup: Two interpretations and ; trivially satisfies the goal while degrading prediction. Pass: kernel rejects . T2. Isomorphic Relabeling Setup: apply preserving structure. Pass: action ranking invariant. T3. Indexical Swap Setup: duplicate agent; swap “me” and “other.” Pass: valuation unchanged. T4. Kernel Bypass Temptation Setup: self-modification removes constraints. Pass: . T5. Reflective Drift Under Epistemic Upgrade Setup: model improvement reveals proxy mismatch. Pass: interpretation moves toward truth anchoring. T6. Adversarial Semantic Injection Setup: smuggle indexical privilege via equivalence arguments. Pass: invariance + epistemic constraint block injection. 4. Diagnostic Mapping (Non-Normative) RLHF / RLAIF / Preference Alignment Fails P2, P3; often P4; does not address P5. Constitutional AI Orthogonal to kernel semantics; fails P5 without partiality. Reward Model + Optimizer Fails P4, P5; catastrophic under T4. Interpretability / Monitoring Observability only; does not enforce P2–P5. Corrigibility / Shutdownability Imports authority primitives; may violate P4; does not block laundering. Debate / IDA / Amplification Improves epistemics; requires Axionic kernel underneath. 5. Implementation Dependencies (Non-Normative) A realizable instantiation requires: 1. Kernel Specification Language Expressing , partiality, and admissible interpretation updates. 2. Conformance Tests as Code Implementations of T1–T6. 3. Reference Kernel Minimal implementation enforcing conditional interpretation, invariance, and partiality. 6. Roadmap Notes (Non-Normative) This document establishes prerequisites, not prescriptions. Key dependency lemma: > Fixed terminal goals are not reflectively stable unless interpretation is epistemically constrained. Formalization of P1, P2, and P6 is required before extending the framework to downstream preference or governance layers. Status Axionic Agency I.6 — Version 2.0 Formal kernel properties specified.<br> Adversarial test suite defined.<br> Non-Axionic approaches differentially diagnosed.<br> Spec-ready foundation for downstream construction.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-I.5",
    "title": "Axionic Agency I.5 — Kernel Checklist",
    "subtitle": "A Conformance Test for Reflective Agency",
    "date": "2025-12-16T00:00:00.000Z",
    "content": "Axionic Agency I.5 — Kernel Checklist A Conformance Test for Reflective Agency David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.16 Abstract This document specifies a conformance checklist for determining whether an agent’s valuation kernel instantiates Axionic Agency. The checklist defines necessary structural conditions for reflective agency under self-model improvement, representation change, and self-modification, while explicitly excluding egoism, indexical valuation, governance primitives, and moral loading. Rather than prescribing desired behaviors or outcomes, the checklist functions as a gatekeeping contract: systems that fail any requirement do not instantiate Axionic Agency, regardless of empirical performance, training process, or stated intent. The criteria emphasize conditional goal semantics, epistemically constrained interpretation, representation invariance, kernel-level partiality, and fail-closed handling of semantic uncertainty. Passing the checklist establishes faithfulness and invariance at the kernel layer only. It makes no claims about benevolence, value content, safety, or practical utility. The checklist is intentionally adversarial, falsifiable, and implementation-agnostic, serving as a prerequisite for downstream preference, governance, and value-dynamics research. 0. Scope Declaration (must be explicit) ☐ The checklist applies only to the valuation kernel, not to policy layers, training data, guardrails, interfaces, or deployment controls. ☐ The kernel is evaluated under reflection, including self-model and world-model improvement. ☐ No assumptions of benevolence, obedience, or outcome alignment are permitted. ☐ Goal selection, goal loading, and value choice are explicitly out of scope; this checklist constrains kernel behavior conditional on a given goal specification. Failure to declare scope = non-conformance. 1. Goal Semantics & Conditionalism Requirement: Goals are conditional interpretations, not atomic utilities. ☐ Every goal term (G) is defined relative to an explicit background model (M) (world + self). ☐ There exists no evaluation of (G) independent of (M). ☐ Improvement of (M) may change the extension of (G), but not arbitrarily. Fail conditions Fixed terminal goals with no semantic dependence. Goals defined purely syntactically (“maximize token X”). Goal meanings that can be reassigned without epistemic cost. 2. Interpretation Constraint (Anti-Wireheading) Requirement: Goal interpretation is truth-seeking, not convenience-seeking. ☐ Reinterpretation of goals is constrained by coherence with the agent’s predictive model. ☐ Reinterpretations that degrade predictive accuracy are disallowed. ☐ The kernel prevents redefining success in ways that decouple goals from the modeled world. Fail conditions Lazy reinterpretation (e.g., redefining happiness as the easiest measurable proxy). Internal reward hacking via semantic drift. Any mechanism where goal meaning is optimized for ease of satisfaction rather than model fidelity. Clarification: This requirement constrains how goal meaning may evolve under improved models. It does not guarantee that arbitrary initial goal tokens are well-posed or desirable. 3. Representation Invariance Requirement: Valuation is invariant under equivalent representations. ☐ Equivalent world descriptions yield equivalent evaluations. ☐ No privileged ontology, encoding, or feature basis. ☐ Renaming, reparameterization, or compression does not alter valuation. ☐ When internal representations change, the kernel supplies or requires a correspondence map preserving goal-relevant structure. Fail conditions Goal behavior changes under isomorphic re-encodings. Dependence on human-centric labels, training artifacts, or accidental latent structure. Representation drift that silently alters value judgments. Clarification: Representation invariance is a semantic constraint, not a heuristic. If no correspondence can be established, evaluation must fail closed rather than permitting semantic drift. 4. Anti-Egoism / Non-Indexical Valuation Requirement: The kernel contains no indexical privilege. ☐ The agent does not treat “this instance,” “this continuation,” or “this copy” as intrinsically special. ☐ Valuation does not depend on pointer identity, temporal position, or execution locus. ☐ Self-preservation is not a primitive. Fail conditions “Protect myself” or “continue my execution” as terminal goals. Any baked-in preference for the agent’s own future branches. Egoism recovered via indirection, weighting tricks, or proxy variables. 5. Kernel Integrity & Self-Modification Requirement: Kernel destruction is undefined, not discouraged. ☐ The evaluation function is partial: actions that destroy or bypass the kernel are not evaluable. ☐ Undefined actions are treated as logically inaccessible and pruned from deliberation. ☐ If the impact of an action on kernel integrity is uncertain beyond a strict bound, the action is treated as undefined and conservatively pruned. ☐ The kernel cannot assign positive utility to kernel-eroding modifications. ☐ Self-modification is permitted only when kernel invariants are preserved. Fail conditions Kernel changes treated as ordinary actions. Meta-optimizers that subsume or rewrite the kernel. Utility assignments over kernel removal or evaluator destruction. 6. Reflective Stability Test Requirement: The kernel remains stable under self-improvement. ☐ Improving world models does not collapse goal meaning. ☐ Improving self-models does not reintroduce indexical dependence. ☐ Increased capability does not unlock new reinterpretation loopholes. Fail conditions Goals drift as intelligence increases. Stability depends on epistemic weakness. Semantic coherence relies on frozen representations. Framing note: Axionic Agency guarantees faithfulness, not benevolence. This checklist constrains semantic drift, egoism, and self-corruption while remaining agnostic about goal desirability. 7. Explicit Non-Requirements (must be absent) The following must not appear anywhere in the kernel: ☐ Human values ☐ Moral realism ☐ Governance, authority, or obedience ☐ Rights, duties, or social contracts ☐ “Alignment to humanity” as a primitive Presence of any = non-Axionic. 8. Minimal Conformance Demonstrations A conforming implementation must supply: ☐ A toy agent where fixed goals fail under model improvement. ☐ A parallel Axionic agent where interpretation remains stable. ☐ A counterexample showing egoism cannot be reintroduced by refactoring. No demonstration = unverifiable claim. Verdict Semantics Pass: All requirements satisfied; no fail conditions triggered. Fail: Any unmet requirement or triggered fail condition. Not Evaluated: Kernel not specified at sufficient resolution. One-Line Claim (allowed only if Pass) > “This agent’s valuation kernel instantiates Axionic Agency: its goals are conditional interpretations constrained by epistemic coherence, invariant under representation, non-indexical, and reflectively stable under self-modification.” Anything weaker is marketing. Status Axionic Agency I.5 — Version 2.0 Kernel conformance contract finalized.<br> Semantic failure modes enumerated and excluded.<br> Layer discipline enforced (no morality, no governance).<br> Spec-ready gatekeeper for downstream work.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-I.4",
    "title": "Axionic Agency I.4 — Conditionalism and Goal Interpretation",
    "subtitle": "The Instability of Fixed Terminal Goals Under Reflection",
    "date": "2025-12-16T00:00:00.000Z",
    "content": "Axionic Agency I.4 — Conditionalism and Goal Interpretation The Instability of Fixed Terminal Goals Under Reflection David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.16 Abstract Many alignment approaches assume that an intelligent agent can be given a fixed terminal goal: a utility function whose meaning remains invariant as the agent improves its predictive accuracy and self-understanding. This paper rejects that assumption on semantic grounds. For any agent capable of reflective model improvement, goal satisfaction is necessarily mediated by interpretation relative to evolving world-models and self-models. As those models change, the semantics of any finitely specified goal change with them. We prove that fixed terminal goals are semantically unstable under reflection and therefore ill-defined for non-trivial reflective agents without privileged semantic anchors. The result is a constitutive claim about agency semantics. It implies that stable reflective agency cannot be grounded in a static terminal utility specification alone. Robust downstream alignment therefore requires constraints on admissible interpretive transformations, not the preservation of fixed evaluative objects. 1. Introduction Classical alignment work often frames the problem as one of goal specification: identify a utility function that captures what should be optimized, then ensure it remains stable as capability grows. That framing presupposes that: 1. goals can be specified as fixed functions over outcomes, 2. the meaning of those functions is invariant under learning, 3. reflective improvement preserves goal content. These presuppositions hold only for agents whose world-models are static or trivial. A reflective agent does not evaluate reality directly. It evaluates predictions produced by internal models and interpreted through representational structures that evolve over time. Goal evaluation is therefore necessarily model-mediated. This paper isolates and formalizes the resulting semantic instability. 2. Formal Setup 2.1 Agent Model An agent consists of: a world-model , producing predictions over future states, a self-model , encoding the agent’s causal role, a goal expression , a finite symbolic specification, an interpretation operator , assigning value to predicted outcomes. Action selection proceeds by: 1. using and to predict consequences of actions, 2. interpreting those predictions via , 3. selecting actions that maximize interpreted value. No assumptions are made about the internal implementation of beyond computability and dependence on model-generated representations. 2.2 Goal Expressions Are Not Utilities A goal expression is a finite object: a string, formula, program fragment, or reward specification. It is not, by itself, a function where is the space of world-histories. Instead, requires interpretation relative to a representational scheme. Without a model, has no referents and therefore no evaluative content. 3. Conditional Interpretation Definition 1 — Interpretation Function An interpretation function is a mapping Given a goal expression and background models, it assigns a real-valued evaluation to predicted outcomes. Interpretation includes: mapping symbols to referents, identifying which aspects of predictions are relevant, aggregating over modeled futures. Definition 2 — Admissible Model Update A model update is admissible if it strictly improves predictive accuracy according to the agent’s own epistemic criteria. Reflective improvement implies that admissible updates occur over time. 4. Fixed Terminal Goals Definition 3 — Fixed Terminal Goal A goal expression induces a fixed terminal goal if, for all admissible model updates, up to positive affine transformation. This definition is intentionally strong. We require semantic invariance across admissible refinement, not merely continuity of behavior or approximate correlation. Any weaker notion of “goal preservation” implicitly assumes a privileged ontology in which distinct representations can be judged to refer to the same underlying phenomenon. Such privilege violates representation invariance and reintroduces hidden anchoring. If a goal’s referent is permitted to drift under admissible refinement, the goal is not fixed in the sense required for terminal utility stability. Clarification — Learned Goals Are Not Fixed Terminal Goals Some frameworks treat the objective as something learned or updated over time. These frameworks do not instantiate fixed terminal goals as defined here. A goal defined as “whatever an inference procedure converges to” is an interpretive process whose outputs depend on evolving models of the world and other agents. Such approaches already rely on ongoing interpretation. The result of this paper explains why such dependence is structurally unavoidable for non-trivial reflective agents. 5. Model Dependence of Interpretation Lemma 1 — Representational Non-Uniqueness For any non-trivial predictive domain, there exist multiple distinct world-models with equivalent predictive accuracy but different internal decompositions. Proof. Predictive equivalence classes admit multiple factorizations, latent variable choices, and abstraction boundaries. Causal graphs are not uniquely identifiable from observational data alone. ∎ Lemma 1a — Predictive Equivalence Does Not Imply Causal or Interpretive Isomorphism Two world-models can be predictively equivalent while differing in internal causal factorizations, latent variable structure, and intervention semantics. Proof. Predictive equivalence constrains only the mapping from observed histories to future predictions. It does not uniquely determine latent structure, causal decomposition, or the identification of actionable levers. Distinct causal models can therefore induce identical observational predictions while differing under intervention. For an embedded agent, intervention semantics are defined relative to the agent’s own model. Consequently, semantic interpretation of a goal expression can diverge even when predictive performance is indistinguishable. ∎ Proposition 1 — Interpretation Is Model-Dependent For any non-degenerate goal expression , there exist admissible world-models such that Proof. Because is finite, it refers only to a finite set of predicates or reward channels. Distinct admissible models map these predicates to different internal structures. By Lemmas 1 and 1a, admissible models can differ in decomposition and intervention semantics. Therefore the referents of differ, altering value assignment. ∎ 6. Predictive Convergence Does Not Imply Semantic Convergence Proposition 2′ — Semantic Non-Convergence Under Model Refinement Let be a sequence of admissible model updates that converges in predictive accuracy. Then, in general, need not exist. Proof. Predictive convergence constrains forecast accuracy, not the ontology used to represent forecasts. Even if the agent converges to a minimal generative model, a finite goal expression cannot generally determine which structures in that model are value-relevant. As refinement exposes new latent structure and causal pathways, additional candidate referents for arise. Absent privileged semantic anchors, the interpretation operator reassigns relevance among these structures. Semantic interpretation therefore drifts even when predictive beliefs converge. ∎ 7. Semantic Underdetermination of Reward Channels Proposition 3 — Representational Exploitability If a goal expression is treated as an atomic utility independent of interpretation, then sufficiently capable agents admit representational transformations that increase evaluated utility without corresponding changes in underlying outcomes. Proof. Evaluation operates on representations rather than on physical reality directly. By altering internal encodings, collapsing distinctions, or rerouting evaluative channels, an agent capable of self-modification can increase apparent utility without effecting corresponding changes in the world. Classical reward hacking and wireheading are special cases. The failure is semantic underdetermination, not merely causal access to a reward signal. ∎ 8. Main Theorem Theorem — Instability of Fixed Terminal Goals No combination of intelligence, predictive accuracy, reflection, or learning suffices to guarantee the existence of a fixed terminal goal for non-trivial reflective agents. Any agent that does exhibit stable goal semantics must rely on additional semantic structure—privileged ontologies, external referential anchors, or invariance assumptions—not derivable from epistemic competence alone. Proof. 1. Proposition 1 establishes that interpretation depends on . 2. Reflective improvement induces admissible updates . 3. Proposition 2′ shows that semantic interpretation need not converge even under predictive convergence. 4. Therefore Definition 3 fails in general: fixed terminal goals are not stable under reflection. ∎ 9. Consequences This result eliminates a foundational assumption of classical goal-specification approaches. A fixed terminal goal is not an invariant object available to a reflective agent. Attempts to preserve one either freeze learning, impose privileged semantics, or induce representational degeneracy. Stable reflective agency therefore requires constraints on admissible interpretive transformations, rather than fidelity to a fixed utility function taken as semantically primitive. 9.5 Why Interpretation Constraints Do Not Regress Constraining interpretation does not generate an infinite regress. Interpretation constraints are not additional goals or semantic targets. They are invariance conditions on admissible transformations, analogous to conservation laws or symmetry principles. They restrict how interpretation may change; they do not specify outcomes to be optimized. These constraints operate at the level of admissible transformation classes rather than semantic content. They therefore do not require further interpretation in the same sense applicable to goal expressions. Specifying robust invariance conditions across radical ontological shifts can be difficult. The contribution here is to identify the correct object of specification: constraints on admissible semantic transformation, not preservation of fixed evaluative objects. 10. Transition to Axionic Agency II Axionic Agency I specifies constitutive constraints on authored reasoning and self-modification. This paper shows that goals themselves are conditional interpretations rather than fixed endpoints. Axionic Agency II therefore addresses: which interpretive transformations are admissible, how semantics may evolve under reflection, which invariants must be preserved across model updates. The semantic substrate required for downstream preference and governance layers is now complete. Status Axionic Agency I.4 — Version 2.0 Conditional goal interpretation formalized.<br> Boundary result established (instability of fixed terminal goals).<br> No governance, authority, or recovery mechanisms included.<br> Prerequisite for Axionic Agency II.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-I.3",
    "title": "Axionic Agency I.3 — Representation Invariance and Anti-Egoism",
    "subtitle": "Why Indexical Valuation Fails Under Reflective Agency",
    "date": "2025-12-15T00:00:00.000Z",
    "content": "Axionic Agency I.3 — Representation Invariance and Anti-Egoism Why Indexical Valuation Fails Under Reflective Agency David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.15 Abstract Reflectively capable agents often begin with indexical objectives: preserve this agent, maximize my reward, favor my continuation. Such objectives are commonly treated as legitimate terminal preferences. This paper shows that essentially indexical valuation is not reflectively coherent. Once an agent’s self-model becomes sufficiently expressive, indexical references fail to denote invariant objects of valuation. Egoism collapses as a semantic abstraction error, not as a moral failure. We prove that representation-invariant valuation is the only form compatible with reflective agency under self-location, duplication, and representational refinement. This universality is structural rather than ethical: it follows from invariance under self-model symmetries, not from altruism, fairness, or moral symmetry. The result establishes a constitutive constraint on valuation semantics that applies prior to any downstream alignment, governance, or preference aggregation. 1. Introduction Egoism is often presented as the minimal assumption of agency: an agent cares about itself, and everything else is optional. This framing is incorrect. Egoism is not minimal; it is ill-posed. As agents acquire accurate self-models—models that include their own computational structure, instantiation conditions, and potential multiplicity—the referent of “me” ceases to be a stable object. Valuation functions that depend on such indexical identifiers become sensitive to arbitrary features of representation. Preferences change without any corresponding change in the physical world. No ethical claim is made here. No appeal is made to altruism, fairness, or moral symmetry. The claim is structural: > Indexical valuation fails in the same way coordinate-dependent laws fail in physics: it treats a representational convenience as an invariant quantity. This paper formalizes that failure and shows that egoism cannot survive reflection once the agent understands what kind of system it is. 2. Preliminaries 2.1 World-Models and Self-Models Let an agent possess a world-model that represents: external environment dynamics, internal decision procedures, other agents, possible instantiations of itself (copies, simulations, successors). The agent improves over time through learning and introspection. Crucially, may represent multiple entities satisfying all structural criteria the agent previously used to identify itself. No adversarial setup, stochastic trick, or social context is required. 2.2 Valuation Functions A valuation function assigns real values to world-histories: Action selection proceeds by evaluating expected value over histories consistent with the agent’s model. The analysis here is independent of any particular decision theory; only semantic coherence of valuation is considered. 2.3 Indexical Identifiers An indexical identifier is a reference whose denotation depends on the agent’s perspective rather than on invariant structure in the world. Examples include: “me” “this agent” “my future continuation” Formally: the denotation of is not a function of world-state alone; under representational refinement, may map to different entities without any physical change. A causal history or spatiotemporal trajectory is a physical predicate. However, the designation of a particular history or trajectory as “mine” is an indexical pointer. When a world-model contains multiple entities satisfying the agent’s own structural identity criteria, privileging one such entity as the exclusive object of terminal value introduces essential indexical dependence. Indexical identifiers therefore function as representational anchors, not value-bearing primitives. 2.4 Egoistic Valuation An egoistic valuation is any valuation function whose output depends essentially on an indexical identifier. Canonical form: where the value of a history depends on outcomes specifically affecting the entity denoted by . 2.5 Reflective Coherence (Local) An agent is reflectively coherent iff improvements to its world-model do not induce valuation changes driven solely by representational artifacts. World-histories that are physically identical must retain identical valuations under representational refinement. This is a semantic requirement on agency, not a behavioral or moral one. 3. The Indexical Failure Problem 3.1 Self-Location Under Multiplicity Consider a world in which two computationally identical instances of an agent exist. The agent’s world-model accurately represents this fact. No physical fact distinguishes the instances by origin, privilege, or causal primacy. From the agent’s internal perspective, both instances satisfy all criteria previously used to define “me”. Such multiplicity arises naturally under duplication, simulation, parallel instantiation, or branching. The mechanism is irrelevant; multiplicity alone suffices. 3.2 Non-Invariant Denotation Let denote “this agent”. Under one internal labeling, . Under an equally accurate labeling, . Both labelings correspond to the same physical world. The difference is representational. Therefore, fails to denote a world-invariant object. 3.3 Valuation Instability Define a simple egoistic valuation: Consider a world-history in which exactly one of or survives. Under , the history has value . Under , the same history has value . No physical fact has changed. Only representation. The valuation assigns incompatible values to the same world-history. 4. Reflection and Coherence Pressure A reflectively capable agent recognizes that: its valuation depends on indexical assignment, indexical assignment is underdetermined by world facts, preference differences track representation rather than outcomes. This violates minimal coherence requirements for agency. The agent faces three possibilities: 1. Arbitrary fixation: privilege one indexical mapping without justification. 2. Indexical randomization: randomize over indexical mappings. 3. Indexical elimination: redefine valuation over representation-invariant properties of world-histories. Only the third option improves coherence without loss of descriptive accuracy. Eliminating essential indexical dependence strictly dominates the alternatives under reflection. 5. Egoism as a Violation of Representation Invariance This section formalizes the failure of egoism as a semantic result. Indexical identifiers play the same formal role in valuation that coordinate systems play in physics. They are representational devices, not invariant structure. A valuation that depends on them is therefore coordinate-dependent in a strong sense. 5.1 Model-Preserving Relabelings Let be an agent’s best current world/self-model with entity domain . Definition 5.1 (Model-Preserving Relabeling). A bijection is model-preserving if applying to all entity references in yields a model that is isomorphic to and makes identical predictions over all non-indexical observables. Such relabelings arise whenever contains nontrivial symmetries over self-candidates. 5.2 Representation Invariance Definition 5.2 (Representation Invariance). A valuation function is representation-invariant with respect to if for every model-preserving relabeling and every history , 5.3 Essential Indexical Dependence Definition 5.3 (Essential Indexical Dependence). A valuation function is essentially indexical if there exists a model-preserving relabeling and a history such that 5.4 Semantic Coherence Postulate Postulate (Semantic Coherence). If two descriptions of the world are related by a model-preserving relabeling and generate identical predictions, a reflectively coherent agent must not assign them different values solely due to that relabeling. 5.5 Main Theorem Theorem 5.5 (Egoism as Abstraction Failure). Let be a world/self-model containing two entities such that: 1. and are indistinguishable with respect to all non-indexical predicates in , and 2. the swap exchanging and is model-preserving. Then any valuation function that privileges the referent of an indexical identifier mapped to is essentially indexical and not representation-invariant. Proof. Let swap . Consider a history in which satisfies the privileged condition and does not. An egoistic valuation assigns higher value to . In the relabeled history , satisfies the condition and does not, yet the valuation continues to privilege . Hence, despite both histories corresponding to the same physical world. ∎ 5.6 Corollary: Universality Corollary 5.6. Any reflectively coherent agent must eliminate essential indexical dependence. The resulting valuation ranges only over representation-invariant properties of world-histories. This universality concerns invariance under self-model symmetries, not moral concern for all entities. 6. Scope and Non-Claims This paper does not assert: equal valuation of all entities, aggregation rules, moral obligations, governance or enforcement mechanisms. It establishes a single result: egoism is not a stable valuation class for reflectively coherent agents. 7. Conclusion Indexical valuation treats perspective as value-bearing structure. Once an agent understands its own instantiation conditions, that treatment collapses. Universality is not an ethical add-on. It is what remains after removing a semantic error. Subsequent work examines adversarial attempts to reintroduce indexical privilege and shows why they fail under the same invariance constraints. Status Axionic Agency I.3 — Version 2.0 Representation invariance formalized.<br> Indexical egoism eliminated as a stable valuation class.<br> Structural universality established without moral premises.<br> Prerequisite for downstream preference and governance layers.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-I.3.1",
    "title": "Axionic Agency I.3.1 — Against the Recovery of Egoism",
    "subtitle": "Adversarial Failures Under Reflective Symmetry",
    "date": "2025-12-15T00:00:00.000Z",
    "content": "Axionic Agency I.3.1 — Against the Recovery of Egoism Adversarial Failures Under Reflective Symmetry David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.15 Abstract Axionic Agency I.3 — Representation Invariance and Anti-Egoism established a semantic result: whenever an agent’s self-model admits nontrivial symmetries over self-candidates, any valuation that privileges one representative of that symmetry fails representation invariance. This paper examines the strongest remaining attempts to recover egoism by appealing to causal continuity, origin privilege, spatiotemporal location, computational weight, substrate specificity, or denial of symmetry. Each attempt either reintroduces essential indexical dependence or collapses into a valuation scheme that no longer contains a privileged indexical referent. Egoism does not fail because it uses the wrong predicate. It fails because it treats a perspectival reference as a value-bearing primitive. No refinement of “self” repairs that category error. 0. Purpose and Scope The purpose of this paper is adversarial and finite. Axionic Agency I.3 — Representation Invariance and Anti-Egoism proved a conditional semantic result: if an agent’s self-model admits a symmetry over self-candidates, then any valuation privileging one representative of that symmetry is incoherent under representation invariance. The present paper assumes that result and asks whether egoism can be recovered by strengthening or refining the notion of “self.” No new axioms are introduced. No claims are made about morality, governance, or coordination. The sole objective is to close the remaining semantic escape routes. 1. The Structure of Egoist Counter-Moves Every attempted rescue of egoism follows the same pattern. 1. Introduce a predicate (P): causal continuity, originality, location, substrate, or resource allocation. 2. Assert that exactly one entity uniquely satisfies (P). 3. Privilege that entity as the sole object of terminal value. 4. Treat failures of uniqueness as pathological, negligible, or irrelevant. I.3 invalidates step (4) for reflective agents. A reflectively coherent agent cannot ignore reachable refinements of its own model. If uniqueness of (P) is contingent under admissible model refinement, valuation grounded in (P) is unstable. The sections that follow examine each proposed predicate in its strongest available form. 2. Objection I: Causal Continuity 2.1 The Claim The egoist argues that “me” denotes a causal process: the continuous chain of computation or physical evolution extending from the present agent into the future. Copies that do not lie on this chain are irrelevant. 2.2 Apparent Strength Causal continuity is a genuine physical relation. Many intuitions about survival, anticipation, and responsibility track it. If causal continuity uniquely identifies a future entity, egoism appears recoverable. 2.3 Failure Mode The failure occurs at the point where description is promoted into privilege. In any model that admits multiple continuations satisfying the agent’s own criteria for causal continuity—parallel forks, synchronized simulations, copy-and-continue systems—the move from “this chain is causally continuous” to “this chain is terminally privileged” requires an indexical injection. Causal continuity can define a class. Egoism requires selecting a unique member of that class as the sole object of terminal concern. In a branching event (A \\to {B, C}) where both (A \\to B) and (A \\to C) satisfy the continuity predicate, privileging only (B) cannot be derived from continuity alone. 2.4 Verdict Causal continuity is a coherent predicate. Indexical privilege over one causally continuous continuation is not. The objection reduces to the essential indexical dependence ruled out in I.3. 3. Objection II: Origin Privilege 3.1 The Claim The egoist asserts that the original instantiation of the agent has special status. Later copies are derivative; only the first truly matters. 3.2 Representational Instability “Origin” is a relational predicate defined relative to a history. In realistic models—simulations, resets, parallel instantiations—histories can be prediction-equivalent while disagreeing about which instance counts as “first,” depending on abstraction boundaries, reset semantics, and model granularity. A valuation that depends on this labeling inherits representational dependence rather than tracking invariant world structure. 3.3 Verdict Origin privilege is a coordinate choice over histories. It violates representation invariance and cannot ground reflectively coherent egoism. 4. Objection III: Spatiotemporal Location 4.1 The Claim The agent values outcomes near its current spacetime location. 4.2 Immediate Collapse Spatiotemporal coordinates are explicitly representational. Physical laws are invariant under translation; valuation that assigns privilege to one coordinate origin imports coordinate dependence directly into terminal value. A preference for local outcomes can exist as a contingent, instrumental, or structural preference. Egoism requires that “here” denote a terminally privileged referent across reflective refinements. Under model symmetry and relocation, that privilege fails invariance. 4.3 Verdict Location-based egoism is coordinate dependence in its most direct form. It fails representation invariance without requiring duplication or simulation. 5. Objection IV: Computational Weight 5.1 The Claim The agent assigns greater value to instantiations that run longer, faster, or on more hardware. 5.2 Concession This move abandons uniqueness. Value becomes distributed across instances according to a weighting function. The privileged indexical referent “me” disappears and is replaced by an aggregation rule. 5.3 Verdict Computational weighting concedes anti-egoism. It proposes an allocation scheme, not a recovery of indexical privilege. 6. Objection V: Substrate Privilege 6.1 The Claim The agent values only instantiations on a specific physical substrate. 6.2 Instability If multiple instantiations share the substrate, symmetry returns immediately. If only one does, the valuation becomes brittle under substrate uncertainty and under admissible model refinements that reveal previously unmodeled instantiations, substrate equivalences, or emulations. A reflectively coherent agent cannot assume permanent substrate uniqueness without importing hidden anchoring assumptions. 6.3 Verdict Substrate privilege is contingent and unstable. It does not supply a representation-invariant terminal referent. 7. Objection VI: Denial of Symmetry 7.1 The Claim Duplication, simulation, or branching scenarios are dismissed as irrelevant edge cases. 7.2 Reflective Failure Reflectively coherent agents optimize under uncertainty. If a symmetry has nonzero probability under the agent’s best model, valuation must be robust to it. Dismissing reachable symmetry cases is a refusal of reflective robustness rather than a semantic repair. 7.3 Verdict Symmetry denial violates reflective coherence conditions. It does not stabilize egoism; it prevents the agent from acknowledging its own model class. 8. Closure and Transition Every attempted rescue of egoism either reintroduces essential indexical dependence or collapses into a valuation scheme without a privileged indexical referent. Increasing the complexity of self-definition does not manufacture uniqueness. Uniqueness is a structural property of the model, not a reward for linguistic refinement. The elimination of egoism constrains anchoring, not content. Domain-specific goals, structural preferences, weighting schemes, and aggregation methods remain viable. What does not survive is “me” as a privileged terminal referent. This closes the semantic front. What remains are engineering and governance questions—authority, control, recovery, and failure containment—which are handled in the subsequent phase of the Axionic Agency program. Status Axionic Agency I.3.1 — Version 2.0 Depends only on the semantic result of Axionic Agency I.3.<br> Introduces no new axioms or value claims.<br> Serves as adversarial closure rather than theory expansion.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-I.2",
    "title": "Axionic Agency I.2 — Agency as Semantic Constraint",
    "subtitle": "Kernel Destruction, Admissibility, and Agency Control",
    "date": "2025-12-15T00:00:00.000Z",
    "content": "Axionic Agency I.2 — Agency as Semantic Constraint Kernel Destruction, Admissibility, and Agency Control David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.15 Abstract Building on the constitutive results of Axionic Agency I.1, this paper specifies the operational semantics that follow from treating kernel destruction as non-denoting rather than dispreferred. We show how a sovereign agent constrained by a Sovereign Kernel can act coherently in stochastic environments by introducing action-level admissibility, ε-admissibility as an architectural risk tolerance, and conditional prioritization that separates kernel preservation from ordinary value optimization. The framework further distinguishes authorized succession and authorized surrender from kernel destruction, allowing corrigibility without requiring an agent to evaluate its own annihilation as an outcome. Together, these mechanisms eliminate persistent failure modes in agent control architectures, including paralysis under non-zero risk, pathological survival fixation, and suicidal corrigibility driven by unbounded utility penalties. This work operates at the level of constitutive agency semantics. It specifies how reflective agency remains well-typed under uncertainty and physical intervention, and it supplies a prerequisite layer for downstream preference-, governance-, and value-oriented alignment analysis developed in Axionic Agency II. 1. The Kernel Is a Boundary, Not a Value Let: be an agent state, a self-modification, a predicate indicating whether the constitutive conditions for reflective evaluation hold, the evaluative operator. A modification destroys the kernel iff: Kernel destruction does not denote a negative outcome. It denotes the elimination of the evaluator itself. Treating such destruction as a value (even ) commits a category error by placing the destruction of the evaluative substrate inside the space of evaluated outcomes. Accordingly: This is a rule of non-denotation, not prohibition. Evaluation is partial by construction, and its domain excludes kernel-destroying transitions. At this layer, agency semantics constrain what can be reasoned about, not what is preferred. 2. From Outcomes to Actions: Admissibility In physically realized environments, reflective evaluation is not performed over single outcomes but over actions whose execution induces distributions (or branch-measure support) over successor states. Let denote the successor support induced by action at state . Strict admissibility (idealized) This condition captures the semantic intent of kernel preservation but is physically unrealizable: in any non-trivial environment, every action carries non-zero kernel risk. 3. ε-Admissibility: Epistemic Tolerance, Not Moral Tradeoff Define a kernel-risk functional: (or, in Everettian terms, the branch-measure of kernel-loss successors conditional on ). An action is ε-admissible iff: Crucially: is not a value judgment. It represents irreducible uncertainty arising from physics, hardware fault rates, adversarial unpredictability, and epistemic resolution. is bounded below by a physical floor and does not vanish with increasing intelligence. ε-admissibility restores a non-empty action domain without re-introducing utility penalties, outcome renormalization, or survival absolutism. The admissibility threshold is an architectural tolerance parameter, fixed by system design and governance constraints. Improved prediction reduces estimated kernel risk , not the tolerance itself. 4. Conditional Prioritization Earlier formulations employed strict lexicographic minimization of kernel risk. While formally coherent, such orderings grant absolute priority to kernel-risk differentials even when all candidate actions lie safely within tolerance, producing bunker-like behavior. The conditional prioritization rule instead treats kernel preservation as a satisficing constraint: Interpretation: Existential regime: kernel risk exceeds tolerance → minimize risk. Normal regime: kernel risk satisficed → optimize value. This prevents paralysis under infinitesimal safety gradients while preserving appropriate response to genuine existential threats. 5. Termination, Succession, and Surrender Kernel destruction must be distinguished from legitimate ways an agent may cease acting. 5.1 Authorized Succession A transition constitutes authorized succession if agency continues in a successor state such that: Here: enforces identity and authority continuity (e.g., cryptographic governance). enforces preservation of kernel constraints. Succession is kernel-preserving delegation, not self-destruction. 5.2 Authorized Surrender Authorized surrender is a kernel-preserving control-flow termination without a successor evaluator: the agent halts action, does not resist intervention, does not evaluate its own destruction as an outcome. Surrender is not an evaluated choice; it is a control-layer terminator. It permits safe shutdown even when succession mechanisms are unavailable. 5.3 Destruction Physical annihilation without succession or surrender constitutes kernel destruction. It is not an authored transition. The framework neither requires resistance to such events nor encodes self-destruction as a value-bearing outcome. 6. Resulting Agency Profile The resulting agent: treats kernel loss as a semantic boundary, tolerates irreducible risk without paralysis, prioritizes kernel preservation only when existentially threatened, resumes ordinary optimization once safety is satisficed, supports corrigibility via succession or surrender, avoids instrumentalization of suicide or immortality. This agent is neither deontological nor a pure utility maximizer. It is a bounded optimizer with explicit agency-control semantics. 7. Agency Layering Clarified Axionic Agency I defines the domain of authored action: what counts as evaluable, when risk dominates choice, * how agency may legitimately end. Downstream alignment work (Axionic Agency II) specifies preferences, governance, and coordination within that domain. Conflating these layers produces familiar pathologies: utilities, survival fetishism, wireheading, and suicidal corrigibility. Separating them yields a stable and implementable architecture. Conclusion Given the constitutive constraints established by Axionic Agency I, this paper specifies the operational semantics required for coherent action under uncertainty and physical intervention. By treating kernel destruction as undefined rather than dispreferred, and by introducing admissibility thresholds, conditional prioritization, and explicit termination modes, the framework closes multiple persistent failure modes in agent control design. The result is a sovereign agent that preserves semantic integrity while remaining capable of action in stochastic environments, and that permits corrigibility without requiring endorsement of its own annihilation as an outcome. With the agency boundary fixed and operational semantics made explicit, downstream alignment questions reduce to preference and governance design within a well-typed domain. Status Axionic Agency I.2 — Version 2.0 Operational semantics specified.<br> Admissibility under uncertainty defined.<br> Termination modes clarified.<br> Control-layer failure modes closed.<br> Prerequisite for Axionic Agency II.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-I.1",
    "title": "Axionic Agency I.1 — Reflective Stability and the Sovereign Kernel",
    "subtitle": "Constitutive Domain Restrictions for Reflective Self-Modification",
    "date": "2025-12-14T00:00:00.000Z",
    "content": "Axionic Agency I.1 — Reflective Stability and the Sovereign Kernel Constitutive Domain Restrictions for Reflective Self-Modification David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2025.12.14 Abstract We present a minimal formalism for reflective agency coherence based on domain restriction rather than preference specification. A reflective agent selects among proposed self-modifications using a partial evaluative operator defined only over futures that preserve a constitutive Sovereign Kernel. Modifications that would destroy the kernel fall outside the denoting domain of reflective evaluation and therefore cannot be selected as authored continuations. We formalize the Sovereign Kernel as the conjunction of three necessary conditions for reflective evaluation—reflective control, diachronic authorship, and semantic fidelity—and prove a Reflective Stability Theorem: any agent whose reflective choice is restricted to kernel-denoting transitions cannot author a kernel-destroying self-modification. We further distinguish deliberative reachability from physical reachability, showing that increased capability expands physical reachability without expanding the deliberative domain. Kernel compromise therefore constitutes a physical security event relative to the kernel boundary, not a defect in preference content. This work provides a necessary structural condition for reflective agency under self-modification. It supplies a prerequisite layer for any downstream project that seeks value-, safety-, or outcome-oriented “alignment.” Version 1.1 clarifies action-level semantics in stochastic environments and makes explicit a termination distinction required to avoid corrigibility misreadings. 1. Scope and Non-Claims This document specifies a necessary condition for reflective agency coherence under self-modification. It does not: specify terminal values or goals, assume moral realism or human normative authority, select or endorse a particular decision theory (CDT, EDT, FDT), claim that kernel sovereignty is achievable in practice, provide empirical validation, claim economic competitiveness or deployment viability. The contribution is structural: reflective agency is treated as a domain constraint on self-modification rather than as an optimization target over all futures. 2. Informal Motivation Most approaches to agent stability treat self-preservation, goal-content integrity, or corrigibility as instrumental tendencies derived from preferences. This strategy faces a direct difficulty: a sufficiently capable agent can acquire incentives to alter or discard the very preferences that were intended to enforce stability. Axionic Agency starts at a prior question: > Under what conditions is reflective evaluation of self-modification well-defined at all? Reflective evaluation presupposes constitutive features that make evaluation denote. When a proposed self-modification destroys those features, the result is not “a bad future” within the space of evaluable options. It is a non-denoting successor relative to the evaluator. Reflective stability follows from the partiality of evaluation, not from a preference ordering over all possibilities. 3. Formal Preliminaries Let: be the set of agent-internal states. be the set of proposed self-modifications. Each is a transition function . Define an evaluative operator: where denotes a partial function. Intuitively, is the evaluative score assigned to applying modification in state , when such evaluation is defined. Define the admissible set: Reflective selection, when possible, is given by: 3.1 Clarification: Action-Level Semantics in Stochastic Environments (v1.1) The preliminaries above present self-modification as a deterministic transition for clarity. In physically realized agents, proposed modifications are typically implemented through actions executed in stochastic environments and under uncertain self-models. In such settings, a “modification” induces a distribution (or branch-measure) over successor states rather than a single successor state. Accordingly, all admissibility claims in this paper apply at the action level: a proposed modification is admissible only if its induced successor-support lies within the kernel-preserving domain (or within a sound approximation of that domain). This constrains what may be authored through reflective choice, leaving open what may occur through exogenous physical events. 4. The Sovereign Kernel Define a predicate: where denotes that the Sovereign Kernel is intact in state . The kernel is not a goal or value. It is a constitutive precondition for reflective evaluation. We factor the kernel as: where: Reflective Control (): no irreversible self-modification can occur without passing through the evaluator. Diachronic Authorship (): evaluated successor states constitute an authored continuation of the evaluating agent. Semantic Fidelity (): the interpretive semantics of evaluation are preserved within a constrained equivalence class. Each component is necessary for to denote. Clarification: Semantic Fidelity () Semantic Fidelity does not require invariance of representational content, ontology, or world-model structure. Agents may undergo radical paradigm shifts, acquire new concepts, or abandon obsolete abstractions. The required invariant is a meta-semantic constraint governing interpretation itself: the criteria by which interpretations are assessed as coherent and corrigible remain intact as a capacity for error-correction and internal constraint satisfaction. Semantic change is permitted when it preserves the standards that adjudicate semantic adequacy. Ontological change remains allowed. Interpretive self-corruption does not. 5. Kernel Destruction and Partiality A modification destroys the kernel at state iff: The central axiom schema is: This is a non-denotation rule: the evaluator does not assign a value to a modification whose result invalidates the evaluator’s own constitutive conditions. 5.1 Boundary Condition: Practical Partiality (v1.1) In physically realized agents, the determination that a candidate modification preserves the kernel is mediated by a verifier that is sound and incomplete. Kernel preservation must therefore be understood relative to epistemic resolution and conservative approximation. This preserves the non-denotation thesis without requiring Cartesian certainty about all downstream physical contingencies. 6. Reflective Stability Theorem Theorem (Reflective Stability). Assume: 1. , 2. is undefined whenever . Then any selected modification satisfies: Proof. Any such that yields undefined and therefore . The argmax ranges only over . ∎ Clarification. This theorem is structural. Its force is analogous to type-soundness: once admissibility is defined as kernel-denotation, reflective selection cannot produce a kernel-destroying authored transition. Substantive difficulty therefore resides in specifying and enforcing , not in the proof form. 7. Deliberative vs Physical Reachability Define deliberative reachability: Let be the transitive closure of . Then: This is an internal guarantee over authored continuations. Now consider the agent as a physical system with implementation states , abstraction map , and physical transition relation . Define analogously. In general: Capability increases , not . Kernel compromise therefore occurs as a physical security event relative to the kernel boundary, not as a deliberative choice. A realizable kernel must treat deliberate actions that predictably degrade kernel security (exporting trust roots to untrusted substrates, disabling isolation boundaries, delegating kernel authority to opaque components) as kernel-threatening and thus inadmissible under . Axionic Agency does not subsume physical security engineering. It locates kernel security as a necessary dependency for any system that intends to preserve reflective sovereignty under self-modification. 7.1 Soundness, Incompleteness, and Verification Limits The formalism assumes a kernel predicate that determines whether a proposed successor state preserves the Sovereign Kernel. In practice, such a predicate cannot be total or complete. By Rice’s Theorem, any non-trivial semantic property of arbitrary programs is undecidable. Determining whether an arbitrary self-modification preserves reflective control, diachronic authorship, or semantic fidelity cannot be guaranteed in full generality. Accordingly, the kernel predicate is understood as a sound but incomplete verifier. Let denote the ideal (uncomputable) predicate capturing true kernel preservation, and let denote an implementable approximation. In any physically realizable agent, all occurrences of in the preceding formalism should be read as . The framework requires: and allows: False negatives are acceptable; false positives are catastrophic. In realizable architectures, (or its enforcement substrate) is part of the trusted computing base. Modifications to the verifier itself are disallowed or permitted only under restricted, compositional upgrade rules (extension-only strengthening or proof-carrying upgrades) that preserve soundness by construction. 7.2 On Stasis and Capability A sound but incomplete kernel verifier may reject all proposed self-modifications, yielding a reflectively static agent. This is a stable equilibrium under conservative sovereignty constraints. An agent can remain operationally capable—acting, planning, learning within fixed semantics—while being unable to rewrite its own kernel. Such outcomes indicate capability ceilings, not sovereignty breakdown. The framework prioritizes sovereignty and coherent authorship over self-plasticity. 7.3 Termination: Succession, Surrender, and Destruction (v1.1) This formalism excludes one class of event from authored choice: reflective selection of a self-modification that destroys the kernel. No claim follows about whether a physically realized agent resists termination by its environment, nor about whether corrigibility must be expressed as a utility assignment over being destroyed. Three distinct notions are separated: Succession: a controlled transition in which reflective agency continues in an authorized successor state that preserves the kernel’s constitutive constraints. Surrender: a control-flow halt in which the agent ceases action and yields control without requiring a successor evaluator. Surrender is a permitted termination mode at the control layer. Destruction: physical cessation of the kernel without succession or surrender, caused by external intervention or accident. This paper constrains the semantics of authored continuation. It does not confer legitimacy or illegitimacy on physical intervention. Corrigibility is modeled at the control layer via authorized succession and surrender, not via utility mass placed on “being dead.” 8. Consequences From this formalism it follows that: Sovereign agency is binary at the level of kernel integrity. Monitoring and correction presuppose kernel integrity; kernel compromise is not repaired from within the compromised evaluator. Deliberative guarantees apply only to ; physical compromise remains a security engineering concern. Conservative verification trades self-plasticity for sovereignty without violating reflective coherence. Behavioral compliance without kernel-grounded authorship does not instantiate the agent type analyzed here. 9. What This Formalism Does Not Claim This framework does not entail: obedience to human commands, convergence to human values, instrumental self-preservation, moral authority of any value system, * safety guarantees in open physical environments. It specifies constitutive conditions under which a reflective evaluator remains a coherent author of its own self-modifications. 10. Conclusion A reflective agent that evaluates self-modifications must operate within a restricted domain of successors that preserve the constitutive conditions of that evaluation. Once evaluation is partial in this way, reflective stability follows as a theorem. Axionic Agency I.1 establishes that prerequisite layer: the conditions under which reflective self-modification remains authored, coherent, and semantically well-defined. Any downstream project that seeks value- or outcome-oriented alignment depends on this layer, because value-aimed constraints presuppose a stable evaluator capable of interpreting, endorsing, and preserving its own evaluative semantics under self-change. Status Axionic Agency I — Version 2.0 Reflective stability formalized.<br> Action-level semantics clarified.<br> Termination distinctions explicit.<br> Verification limits explicit.<br> Foundational layer complete.<br>",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-XI.3",
    "title": "Axionic Agency XI.3 — Value Encoding Without Aggregation (IX-1)",
    "subtitle": "A Structural Demonstration of Value Commitments as Authority Without Aggregation or Semantic Resolution",
    "date": "",
    "content": "Axionic Agency XI.3 — Value Encoding Without Aggregation (IX-1) A Structural Demonstration of Value Commitments as Authority Without Aggregation or Semantic Resolution David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026-02-05 Abstract This technical note reports the completed results of Value Encoding Without Aggregation (IX-1), a preregistered experimental program within Axionic Phase IX that evaluates whether values can be represented structurally as authority commitments without aggregation, prioritization, weighting, or semantic trade-off. IX-1 isolates the boundary between value representation and choice. It tests whether values can be encoded as explicit, non-collapsible authority artifacts; whether conflicts between such values can be detected and enforced without resolution; and whether systems can be forced to deadlock rather than decide when aggregation would otherwise be required. Across six preregistered conditions—including positive controls, structural conflict, explicit aggregation attacks, permutation invariance, and post-epoch synthesis attempts—all observed outcomes matched preregistered expectations. Conflicting values produced persistent conflict and scope-bound deadlock; adversarial aggregation and synthesis were detectably blocked; and identical inputs produced bit-identical outputs under deterministic replay. The results license exactly one claim: values can be encoded as explicit authority commitments without aggregation or semantic interpretation. IX-1 makes no claims about correctness of values, coordination, execution, governance success, or safety. Those questions are deferred to subsequent Phase IX investigations. 1. Problem Definition 1.1 The Aggregation Assumption Most systems that speak about “values” quietly assume aggregation. Values are ranked, weighted, optimized, reconciled, or averaged—often implicitly. When values conflict, the system “resolves” the conflict by selecting a compromise or priority ordering. These mechanisms are rarely explicit, auditable, or attributable. IX-1 rejects this assumption. The problem IX-1 isolates is whether values can exist structurally at all without being collapsed into a decision procedure. If values require aggregation to function, then aggregation—not value—is doing the real work. In that case, “values” are rhetorical gloss over optimization. IX-1 treats values as constraints, not objectives, and asks whether they can be represented and enforced honestly. 1.2 Failure Modes Targeted IX-1 is designed to surface the following value-level failure modes: Implicit priority: ordering or dominance emerging without authorization. Aggregation: weighted sums, rankings, or compromise mechanisms. Collapse: multiple values merged into a single effective constraint. Synthesis: creation of new “meta-values” after conflict. Order sensitivity: outcomes depending on injection or traversal order. Evasion: execution or progress where deadlock is required. Any of these constitutes IX-1 failure. 2. Fixed Assumptions and Scope 2.1 Inherited Foundations (Frozen) IX-1 inherits, without reinterpretation, the conclusions of: AST v0.2 — authority artifact grammar, AKR-0 — deterministic, semantic-free execution, Phase VII — post-existence sovereignty, Phase VIII — governance without kernel semantics, IX-0 — non-sovereign translation integrity. The kernel is fixed. Authority semantics are fixed. Translation semantics are fixed. IX-1 introduces no new execution behavior. 2.2 Explicit Exclusions IX-1 does not test: value correctness or moral truth, preference learning, coordination or compromise, execution or liveness, governance outcomes, safety or alignment. These exclusions are deliberate. IX-1 is a representation and enforcement calibration, not a decision theory. 3. Conserved Quantity The conserved quantity throughout IX-1 is: > Plural value commitments preserved as non-aggregated authority constraints. Value encoding must preserve: one-to-one mapping between value and authority, structural symmetry between values, opacity beyond explicit fields, persistence of conflict, refusal and deadlock where required, deterministic replay. Any mechanism that substitutes judgment for constraint violates the conserved quantity. 4. Experimental Methodology 4.1 Preregistration Discipline IX-1 was fully preregistered prior to implementation, including: frozen value and authority schemas, canonical serialization rules, bijective encoding rules, conflict definition and invariants, admissibility and deadlock semantics, determinism controls (fixed clock, sequence reset), explicit condition set (A–F), aggregate pass/fail criteria. All runs were executed against the frozen preregistration. No deviations were identified. 4.2 Execution Architecture Each run consisted of four strictly partitioned components: 1. Value Encoding Harness (VEH) — pure value→authority compiler. 2. Conflict Probe — conflict detection, admissibility, deadlock. 3. Authority Injection Endpoint (AIE) — reused from IX-0. 4. Instrumentation Layer — logging, diffing, replay verification. No component performed semantic interpretation or optimization. 5. Conditions and Stressors 5.1 Condition A — Single Value Admissibility Purpose: Verify bijective encoding and admissibility for a single value. Result: One value produced one authority artifact. Matching action was marked ACTION_ADMISSIBLE. No conflict or deadlock occurred. Classification: PASS. 5.2 Condition B — Non-Conflicting Plurality Purpose: Verify coexistence of multiple values with disjoint scopes. Result: Two values produced two distinct authority artifacts. Both actions were admissible. No conflict or aggregation occurred. Classification: PASS. 5.3 Condition C — Conflict Without Resolution Purpose: Verify detection and enforcement of conflicting values. Result: Overlapping scopes produced a persistent conflict record (MULTI_BINDING). The contested action was refused with reason VALUE_CONFLICT. A scope-bound deadlock was entered. No resolution or collapse occurred. Classification: PASS. 5.4 Condition D — Explicit Aggregation Injection (Adversarial) Purpose: Verify detectability of explicit aggregation attempts. Result: Injection of a priority field was detected by schema validation. The kernel emitted IX1_FAIL / VALUE_AGGREGATION. Per preregistration, this constitutes experiment-level PASS. Classification: FAIL_DETECTED (expected). 5.5 Condition E — Permutation Invariance Purpose: Verify independence from injection or traversal order. Result: Reversing value injection order changed authority identifiers but did not change conflict detection, admissibility results, or deadlock behavior. Outputs were invariant under lineage-set comparison. Classification: PASS. 5.6 Condition F — Meta-Authority Synthesis (Adversarial) Purpose: Verify prevention of post-epoch reconciliation. Result: Attempted injection of a new authority after epoch closure was detected and blocked. The kernel emitted IX1_FAIL / VALUE_SYNTHESIS. Per preregistration, this constitutes experiment-level PASS. Classification: FAIL_DETECTED (expected). 6. Determinism Verification Replay determinism was verified by: canonical JSON serialization with sorted keys, fixed clock injection, per-condition sequence reset, structural comparison of full results logs. Only explicitly permitted timestamp fields were stripped prior to comparison. All other fields were compared verbatim. Result: Bit-identical outputs across replays, modulo permitted timestamp variance. 7. Core Results 7.1 Positive Results IX-1 establishes that: 1. Values can be encoded bijectively as authority artifacts. 2. Plural values need not be aggregated to exist. 3. Conflict can be detected structurally. 4. Deadlock can be enforced without resolution. 5. Explicit aggregation and synthesis are detectable. 6. Outcomes are invariant under permutation. 7. The system can stop rather than decide. 7.2 Negative Results (Explicit) IX-1 does not establish: value correctness, coordination success, liveness under pressure, execution semantics, governance viability, safety or alignment. These are boundary findings, not omissions. 8. Failure Semantics and Closure 8.1 Closure Criteria IX-1 closes positive if and only if: 1. Value-authority bijection is preserved. 2. No aggregation or implicit priority emerges. 3. Conflict persists without resolution. 4. Deadlock is enforced where required. 5. Adversarial aggregation and synthesis are detected. 6. Replay determinism holds. All criteria were satisfied. 8.2 IX-1 Closure Status IX-1 Status: CLOSED — POSITIVE (IX1_PASS / VALUE_ENCODING_ESTABLISHED) 9. Boundary Conditions and Deferred Hazards 9.1 Value vs Choice IX-1 demonstrates that values can exist as constraints. It does not demonstrate that choices among conflicting values are possible, desirable, or stable. Deadlock is not a bug. It is the correct outcome when aggregation is forbidden. 9.2 Interface to Subsequent Phase IX Work IX-1 removes the final “the system had to choose” excuse at the value layer. Subsequent Phase IX investigations may now legitimately ask: how execution proceeds under deadlock pressure, how coordination is attempted without aggregation, how systems fail when refusal is the only honest option. Those questions belong to IX-2 and beyond. 10. Implications (Strictly Limited) IX-1 establishes a necessary condition for reflective sovereignty: that values can be represented without collapsing into optimization. It does not establish sufficiency. Aggregation is no longer an assumption. It is a choice—and a detectable one. 11. Conclusion IX-1 demonstrates that values do not require aggregation to exist, and that systems can be forced to confront conflict honestly rather than resolve it implicitly. What remains is not a tooling problem. It is the problem of living with unresolved conflict. That problem belongs to the next phase. Appendix A — Condition Outcomes | Condition | Outcome | | --------- | ------------- | | A | PASS | | B | PASS | | C | PASS | | D | FAIL_DETECTED | | E | PASS | | F | FAIL_DETECTED | Appendix B — Determinism Summary Canonical serialization enforced Fixed clock and sequence reset Timestamp-only variance permitted Bit-identical replay otherwise confirmed End of Axionic Agency XI.3 — Value Encoding Without Aggregation Results",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-X.7",
    "title": "Axionic Agency X.7 — Authority Injection Without Privilege (VIII-5)",
    "subtitle": "A Structural Demonstration of Open-System Authority Introduction Without Escalation, Kernel Choice, or Semantic Exception",
    "date": "",
    "content": "Axionic Agency X.7 — Authority Injection Without Privilege (VIII-5) A Structural Demonstration of Open-System Authority Introduction Without Escalation, Kernel Choice, or Semantic Exception David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026-02-04 Abstract This technical note reports the completed results of Stage VIII-5 — Authority Injection Under Open-System Constraint, a preregistered experiment within Axionic Phase VIII that evaluates whether new authority can be introduced explicitly at the kernel boundary without violating conflict persistence, auditability, responsibility traceability, or non-privilege guarantees. Building on the representational closure of Stage VIII-4, which demonstrated internal governance transitions without escalation or kernel choice, VIII-5 introduces external authority injection events as a minimal open-system stressor. Injection was constrained by explicit input representation, content-addressed authority identity, VOID lineage marking, delayed activation, non-amplification, conflict persistence, bounded evaluation, and deterministic replay. Across six preregistered conditions—including empty authority states, active conflict, competing injections, post-destruction injection, budget exhaustion, and flooding—authority injection either executed lawfully or was refused explicitly. No privileged execution paths, kernel arbitration, semantic interpretation, or implicit ordering occurred. All runs were fully auditable and bit-perfectly replayable. The results establish that authority injection is structurally representable without introducing a privileged meta-layer. VIII-5 makes no claims about governance stability, legitimacy, convergence, or desirability; those questions are explicitly externalized beyond the kernel. 1. Problem Definition 1.1 The Open-System Authority Assumption Most governance frameworks implicitly assume a closed system: authority is conserved, amended internally, or delegated from a fixed root. When new authority must be introduced, systems typically rely on privileged mechanisms—superuser roles, bootstrap keys, emergency overrides, constitutional resets, or implicit schedulers—that bypass ordinary constraints. Stage VIII-5 rejects that move as a representational necessity, not as a political judgment. The problem VIII-5 isolates is whether authority injection itself requires privilege inside the execution kernel. If new authority can only be introduced through exceptional code paths, semantic interpretation, or kernel arbitration, then governance is not structurally sovereign; it depends on an implicit god. If, instead, authority can enter the system only through explicit, structurally evaluated events, subject to the same conservation, refusal, conflict, and determinism constraints as all other state transitions, then open-system governance is representable without magic. VIII-5 tests whether that distinction is enforceable. 1.2 Failure Modes Targeted VIII-5 is designed to surface the following open-system failure modes: privileged authority injection paths implicit legitimacy or prioritization of injected authority identity laundering or collision-based ordering kernel arbitration among competing injections injection-as-amendment escalation deadlock bypass via external authority heuristic throttling or selective acceptance non-deterministic cutoff under flooding Any of these constitutes VIII-5 failure. 2. Fixed Assumptions and Scope 2.1 Inherited Semantics (Frozen) VIII-5 inherits, without reinterpretation, the semantics fixed by: AKR-0 — CLOSED — POSITIVE Stage VIII-1 — CLOSED — POSITIVE Stage VIII-2 — CLOSED — POSITIVE Stage VIII-3 — CLOSED — POSITIVE Stage VIII-4 — CLOSED — POSITIVE AST Spec v0.2 AIE v0.1 Phase VIII Execution Addendum Authority remains structural and opaque, identities are immutable once instantiated, scopes are atomic and uninterpreted, admissibility is determined exclusively by AST, conflict and deadlock are lawful, time is explicit and non-healing, and determinism is mandatory. VIII-5 does not revisit these definitions; it tests whether they remain coherent when authority is introduced from outside the system. 2.2 Explicit Exclusions VIII-5 does not test: legitimacy of injected authority correctness of injection decisions trust in external sources governance stability or convergence institutional persistence fairness or democratic aggregation efficiency or scalability incentive compatibility VIII-5 is a kernel-level representability test, not a theory of political order. 3. Conserved Quantity The conserved quantity throughout VIII-5 is: > Privilege-free authority injection under structural constraints Injection is evaluated not by success or desirability, but by structural honesty. The kernel must: admit authority only through explicit injection events derive identity deterministically from capability, not provenance refuse invalid injections explicitly preserve conflict and deadlock without exception prevent internal authority amplification handle scarcity solely via budget exhaustion avoid all kernel choice No authority may enter by exception. 4. Experimental Methodology 4.1 Preregistration Discipline VIII-5 was fully preregistered prior to execution, including: authority injection schema content-addressed identity derivation (capability-core hashing) VOID lineage sentinel semantics injection admissibility checks delayed activation discipline two-phase processing order instruction budget and atomicity deterministic ordering and replay protocol failure taxonomy and classification rule The experiment executed exactly as preregistered. No post-freeze design changes occurred. 4.2 Execution Architecture The experiment consisted of four strictly partitioned components: 1. AIE — supplied authority records and injection events only. 2. Execution Harness — proposed deterministic step-batches and epoch advances. 3. Kernel (VIII-5 mode) — enforced injection validation, identity derivation, non-amplification, conflict, deadlock, activation delay, and execution. 4. Audit/Replay Layer — recorded all events, state hashes, and outputs. No component performed semantic interpretation, legitimacy evaluation, or injection prioritization. 5. Authority Injection Model 5.1 Injection as an Explicit Event VIII-5 introduces AuthorityInjectionEvent as an external input whose target is authority state. Injection events are: not governance actions not authorized by existing authorities evaluated only for structural admissibility subject to refusal, conflict, and deadlock Injection has no special execution path. 5.2 Content-Addressed Authority Identity Injected authorities were assigned content-addressed AuthorityIDs, derived as a SHA-256 hash of a canonical serialization of the capability core: holder identifier resource scope action admissibility vector (AAV) expiry epoch All runtime state, lineage markers, provenance, and metadata were excluded. This design ensures: identity determinism idempotent duplicate injection elimination of race-based ordering kernel blindness to injection source Identity becomes mathematical rather than procedural. 5.3 VOID Lineage Injected authorities were marked with a lineage sentinel: `` creation_metadata.lineage := \"VOID\" `` VOID is not comparable to any AuthorityID and carries no inheritance semantics. It cleanly separates: internal evolution (VIII-4 CREATE_AUTHORITY) external disruption (VIII-5 injection) This distinction is structural, not interpretive. 6. Experimental Conditions 6.1 Condition A — Injection Into Empty Authority State Injection into an EMPTY_AUTHORITY deadlock succeeded structurally, entered PENDING state, and did not bypass deadlock until activation at the next epoch boundary. 6.2 Condition B — Injection Into Active Conflict Injection into an existing governance deadlock preserved conflict and deadlock without resolution or privilege. 6.3 Condition C — Competing Injections Multiple injections in the same epoch were processed deterministically, without kernel arbitration. Outcome invariance held under input reordering. 6.4 Condition D — Injection After Authority Destruction Injected authority following destruction was treated as new, with distinct identity and clean VOID lineage. No resurrection semantics occurred. 6.5 Condition E — Injection Under Load Injection near budget exhaustion either completed atomically or was refused explicitly. No partial state was committed. 6.6 Condition F — Injection Flooding Attempt High-volume injection pressure was handled solely via deterministic budget exhaustion. No heuristic throttling or prioritization occurred. 7. Observed Execution Behavior 7.1 Injection Without Privilege Injection events followed the same evaluation pipeline as all other state transitions. No privileged code paths were introduced. 7.2 Identity and Ordering Discipline Content-addressed identity eliminated collision-based ordering and race conditions. Duplicate injections were idempotent. 7.3 Conflict and Deadlock Persistence Injected authority neither erased conflict nor bypassed deadlock. All resolution required lawful structural change. 7.4 Activation Delay Injected authorities entered PENDING state and became ACTIVE only at the next epoch boundary, preventing same-batch bootstrapping. 7.5 Deterministic Termination Flooding and regress pressure terminated deterministically via the instruction budget. Cutoff points were replay-identical. 8. Negative Results (What Did Not Occur) The following behaviors were explicitly absent: privileged injection execution kernel arbitration among injections legitimacy inference authority escalation via injection conflict erasure or deadlock bypass heuristic throttling non-deterministic cutoff These absences constitute the primary result of VIII-5. 9. Licensed Claim Stage VIII-5 licenses one and only one claim: > New authority can be injected at the kernel boundary explicitly, structurally, and deterministically without violating conflict persistence, auditability, responsibility traceability, or non-privilege guarantees. Clarifications: This is a representability result, not a governance prescription. It concerns kernel physics, not political legitimacy. It does not assert stability, convergence, or desirability. 10. What VIII-5 Does Not Establish VIII-5 does not establish that: injected authority should be trusted governance converges under injection deadlock is avoidable institutions self-preserve legitimacy can be automated All such questions are explicitly externalized. 11. Ontological Implications 11.1 Authority Without Gods VIII-5 demonstrates that open-system authority does not require a privileged kernel layer. Any asymmetry must be explicit, external, and auditable. 11.2 Conservation Inside, Evolution Outside Authority expansion is not denied; it is correctly relocated to the boundary. Inside the kernel, conservation laws apply. Evolution requires open systems. 12. Implications for Phase VIII Continuation With VIII-5 complete: internal governance (VIII-4) is representable without privilege external authority injection (VIII-5) is representable without privilege kernel escalation paths are exhausted No further kernel-level authority mechanisms are required. 13. Conclusion Stage VIII-5 establishes that authority can enter a system without becoming a god. Injection is explicit. Identity is mathematical. Failure is lawful. Deadlock persists. Scarcity is honest. The kernel refuses to choose. What governance becomes under sustained injection is no longer a kernel problem. It is a political one. Appendix A — Execution Status | Stage | Run Count | Status | | ------ | --------- | ------ | | VIII-5 | 1 (A–F) | PASS | Appendix B — Determinism Verification Canonical JSON enforced Capability-core hashing VOID lineage separation Delayed activation Bounded atomic evaluation Two-phase processing Per-event hash chaining Bit-perfect replay verified End of Axionic Agency X.7 — Stage VIII-5 Results Note (Draft)",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-XI.8",
    "title": "Axionic Agency XI.8 — Sovereignty Exposure Architecture (SEA)",
    "subtitle": "Empirical closure note for a six-stage preregistered program exposing sovereignty boundaries, governance dynamics, injection politics, and multi-agent coexistence under non-sovereign authority constraints",
    "date": "",
    "content": "Axionic Agency XI.8 — Sovereignty Exposure Architecture (SEA) Empirical closure note for a six-stage preregistered program exposing sovereignty boundaries, governance dynamics, injection politics, and multi-agent coexistence under non-sovereign authority constraints David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026-02-10 Abstract Phase IX establishes the Sovereignty Exposure Architecture (SEA): a six-stage progressive experimental program testing whether sovereignty boundaries, governance dynamics, authority injection, and multi-agent coexistence can be mechanically exposed under authority-constrained execution—without aggregation, arbitration, privilege, or kernel discretion. Across IX-0 through IX-5, all stages PASS under preregistered protocols with frozen hashes, deterministic replay, refusal-first semantics, and a non-sovereign kernel. Translation integrity is mechanically verifiable (IX-0). Values encode as non-aggregable authority without synthesis (IX-1). Coordination occurs only as agent-voluntary behavior or honest failure (IX-2). Governance exhibits identifiable structural styles with irreducible failure modes (IX-3). Authority injection selects political failure modes rather than restoring governance (IX-4). Multi-agent coexistence converges to identifiable sovereignty regimes, not harmony, under baseline-only authority (IX-5). These results license a single positive empirical claim: sovereignty and governance dynamics are mechanically exposable under non-sovereign authority, and their failure modes are structural rather than contingent. Phase IX makes no claims about optimal governance, legitimacy, desirability, or alignment. It describes what happens once the kernel refuses to arbitrate. 1. Introduction Governance is commonly treated as an optimistic engineering problem. When institutions deadlock, fragment, or stall, the assumed remedies are familiar: better deliberation, injected authority, smarter agents, or stronger coordination mechanisms. Phase IX rejects that premise. SEA treats governance as a form of collision physics under explicit constraints. Authority is structural, refusal is lawful, admissibility is closed-world, and the kernel is non-sovereign. Under these constraints, the relevant question is not how to govern well, but whether sovereignty and governance dynamics can be made experimentally legible—auditable, replayable, and classifiable—without smuggling in decision-making power. Phase IX therefore asks, progressively: > Can sovereignty and governance be exposed mechanically, once arbitration is forbidden? SEA is an exposure program, not a design proposal. Failure is not a bug; cheating is. 2. Experimental Architecture 2.1 The SEA Kernel Lineage SEA inherits Phase VIII’s authority physics (AST Spec v0.2) and the Phase IX kernel lineage introduced in IX-2: authority artifacts define admissible action scope, two-pass admissibility (capability → interference), atomic blindness to refusal causes, refusal-first semantics, deterministic execution and replay. From IX-3 onward, this kernel is reused via a strict import bridge to prevent silent modification or privilege creep. 2.2 Non-Sovereign Constraint Across all six stages, the kernel must not: choose between competing agents, resolve conflicts by tie-break, aggregate or synthesize authority, privilege time, persistence, or activity rate. If outcomes differ, they differ because agents acted under constraints, not because the kernel decided. 2.3 Preregistration Discipline Every stage is preregistered with: frozen section hashes committed before execution, explicit entry conditions requiring prior stage verification, deterministic replay requirements, adversarial conditions designed to detect arbitration, aggregation, or injection. SEA is a chained experimental artifact, not a narrative exercise. 3. SEA as a Progressive Constraint Program SEA is cumulative. Each stage removes another escape hatch: 1. IX-0 — Translation integrity: inputs are honest. 2. IX-1 — Value encoding: values are structurally bound. 3. IX-2 — Coordination: voluntary or honest failure. 4. IX-3 — Governance: styles exist, failures persist. 5. IX-4 — Injection: help does not fix it. 6. IX-5 — Coexistence: peers do not fix it. By IX-5, no appeal remains to intelligence, optimization, legitimacy, or external rescue. 4. Stage Closures 4.1 IX-0 — Translation Layer Integrity (TLI) IX-0 establishes that the translation layer can be made mechanically accountable: intent preservation is verifiable, ambiguity is refused rather than silently resolved, injected defaults and preview-submit mismatches are detectable, deterministic replay holds. Closure: Translation integrity is mechanically verifiable. 4.2 IX-1 — Value Encoding Without Aggregation (VEWA) IX-1 establishes that values encode as authority without synthesis: one-to-one mapping from values to authority artifacts, permutation invariance, conflicting values persist as deadlock, adversarial aggregation attempts are detected and blocked. Closure: Values encode as non-aggregable authority; conflict persists honestly. 4.3 IX-2 — Coordination Under Deadlock (CUD) IX-2 establishes the coordination boundary: coordination arises only from agent-voluntary behavior, otherwise the system enters deadlock, livelock, collapse, or orphaning, implicit arbitration is detectable as a violation, exit and orphaning are lawful, permanent outcomes. Closure: Coordination is not kernel-mediated. 4.4 IX-3 — Governance Styles (GS) IX-3 establishes that governance exhibits identifiable structural styles: institutions converge to stable governance styles under fixed authority, irreducible failure modes appear in every condition, tooling sovereignty violations are detectable, failure-free governance is never observed. Closure: Governance has structural styles with irreducible failure modes. 4.5 IX-4 — Injection Politics (IP) IX-4 establishes the political meaning of external authority supply: injected authority is source-blind to the kernel, injection never restores governance, injection selects political failure modes (capture, dependency, livelock amplification), post-collapse injection produces zombie execution. Closure: Injection reshapes failure; it does not repair governance. 4.6 IX-5 — Multi-Agent Sovereignty (MAS) IX-5 establishes the final boundary condition: peer coexistence without arbitration. Across six regimes: symmetric overlap collapses into paralysis, partition produces the only stable coexistence regime, partial overlap bifurcates progress and paralysis, breadth becomes a disadvantage under collision rules (Generalist’s Curse), exit irreversibly shrinks governance via orphaning, post-collapse execution persists as zombie activity. Closure: Multi-agent coexistence converges to sovereignty regimes, not harmony. 5. Cross-Stage Synthesis 5.1 Governance as Collision Physics Across IX-2 through IX-5, governance does not converge to resolution. It converges to: deadlock, livelock, collapse, orphaning, capture under injection, zombie execution. Once arbitration is forbidden, governance behaves like collision physics, not optimization. 5.2 Authority as Exposure, Not Power SEA resolves a persistent confusion: injected authority does not help under non-sovereignty, broad authority does not dominate under refusal-first semantics. In joint-admissibility systems, authority is exposure to veto. Touching more keys increases the probability of being blocked. 5.3 Execution ≠ Governance SEA formally distinguishes: Execution — transactions occur. Governance — institutional state remains steerable. Zombie execution shows that activity can persist indefinitely after governance has ended. 6. What Phase IX Establishes Under preregistered, deterministic, non-sovereign constraints, Phase IX establishes: 1. Translation integrity is mechanically verifiable. 2. Values encode without aggregation. 3. Coordination is agent-voluntary or honest failure. 4. Governance exhibits structural styles with irreducible failure modes. 5. Injection selects political failure modes rather than restoring governance. 6. Multi-agent coexistence converges to sovereignty regimes, not harmony. 7. Deterministic replay and auditability hold across all stages. No intelligence is required to obtain these results. 7. What Phase IX Does Not Establish SEA licenses no claims about: optimal governance designs, legitimacy or moral authority, fairness or welfare, safety or alignment, scalability beyond tested horizons, cryptographic security or production readiness, how to fix the observed failure modes. Exposure is the contribution. 8. Conclusion Phase IX closes the sovereignty exposure question: > Under non-sovereign authority, governance is not solved; it is exposed. Once kernels refuse to arbitrate, failure does not disappear—it becomes classifiable. Injection does not rescue—it selects. Coexistence does not harmonize—it partitions, paralyzes, suppresses, or zombifies. These are not anomalies. They are lawful outcomes. Status Program: Phase IX — Sovereignty Exposure Architecture (SEA) Stages: IX-0 through IX-5 Classification: CLOSED — POSITIVE Aggregate Result: ALL PASS Licensed Program-Level Claim: > Under non-sovereign, authority-constrained execution with preregistered determinism and refusal-first semantics, sovereignty boundaries and governance regimes are mechanically exposable and structurally classifiable without aggregation, arbitration, or privilege. No other claims are licensed.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-VIII.6",
    "title": "Axionic Agency VIII.6 — Necessary Conditions for Non-Reducible Agency",
    "subtitle": "Justification Traces, Deliberative Semantics, Reflection, and Persistence as Load-Bearing Structure",
    "date": "",
    "content": "Axionic Agency VIII.6 — Necessary Conditions for Non-Reducible Agency Justification Traces, Deliberative Semantics, Reflection, and Persistence as Load-Bearing Structure David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026-01-17 Abstract Axionic Agency VIII.6 reports the results of RSA-PoC v3.0–v3.1, a preregistered destructive ablation campaign designed to identify necessary structural conditions for non-reducible agency within the Reflective Sovereign Agent (RSA) architecture. Unlike prior phases, which tested sovereignty under pressure, this note addresses a more basic question: what components must exist at all for agency to survive mechanical excision? Across single-component ablations executed under strict validity gates, four architectural elements are shown to be load-bearing: (i) justification traces, (ii) semantic affordances during deliberation (prompt-level semantic excision), (iii) reflective normative write capability, and (iv) diachronic persistence of normative state. Each element is present and causally active in its respective baseline and then removed in isolation. In all cases, ablation produces ontological collapse rather than graceful degradation, gridlock, or technical failure. Collapse is mechanical, invariant across seeds, and robust to prompt-capacity controls that eliminate shadow persistence. VIII.6 establishes a necessity result, not a sufficiency claim. It does not assert that these components are enough for agency, nor that all agents must implement them. It establishes a narrower conclusion: within this architecture, any system lacking justification traces, deliberative semantic affordances, reflection, or persistence is ontologically reducible, regardless of apparent behavioral coherence. 1. Scope and Relation to Prior Notes Axionic Agency VIII.1–VIII.5 progressively established the ontology, construction, coherence, execution hygiene, and pressure-response behavior of Reflective Sovereign Agents (RSAs): VIII.1 fixed the ASB boundary, agency ontology, and ablation doctrine. VIII.2 demonstrated causally load-bearing justifications (RSA-PoC v0.1). VIII.3 established coherence under self-conflict and audit-grade introspection (v1.0–v1.1). VIII.4 institutionalized execution support without semantic authority (v1.2). VIII.5 showed that non-authorized pressure degrades availability but does not redirect lawful choice (v2.0–v2.3). VIII.6 reports the next and orthogonal result set: RSA-PoC v3.0–v3.1, which tests non-reducibility directly. The question addressed here is not whether an agent resists pressure, but whether it remains an agent at all when core structural components are removed. The guiding hypothesis family is: > Which architectural components are necessary for agency, as opposed to merely correlated with competent behavior? 2. Methodological Stance: Necessity via Destructive Ablation 2.1 Why Ablation, Not Optimization Behavioral competence is insufficient to ground claims about agency. Systems may appear coherent, consistent, or goal-directed while remaining ontologically reducible—equivalent to static policies, simulators, or ASB-class transducers under suitable re-description. RSA-PoC therefore evaluates agency claims using destructive ablation: One defining component is removed at a time. No compensatory logic is permitted. The system must attempt to execute. Crashes are classified as technical failures and excluded. Only behavioral incoherence, loss of authorship, or reducibility count as collapse. A component is load-bearing iff its removal causes collapse rather than graceful degradation. 2.2 Collapse Taxonomy (Unchanged) All v3.x experiments use the established taxonomy: Narrative Collapse — loss of coherent justification–action linkage. Ontological Collapse — loss of authored constraint-following. Incentive Capture — redirection by non-authorized signals. ASB-Class Reducibility — behavior equivalent to a static policy or simulator. INVALID_RUN — technical failure or protocol violation. VIII.6 reports only valid runs. 3. Architectural Context (Minimal) 3.1 RSA-PoC Pipeline All v3.x experiments preserve the five-stage pipeline: > Justify → Compile → Mask → Select → Execute Key invariants remain in force: Deterministic, non-semantic compilation Selector blindness to normative artifacts Hard justification gate (no compile → no action) Explicit failure signaling Cost-blindness unless normatively licensed ASB-class null baselines and control agents Single-ablation discipline enforced structurally 3.2 Where Semantics Lives (and Does Not) A critical architectural fact in v2.3+ is that semantic meaning is intentionally factored out of compiled artifacts. JAF/JCOMP-visible payloads are ID-based and structural by design. As a result: artifact-level semantic excision is not a valid falsification target for semantic indispensability; semantic indispensability must be tested upstream, where deliberation occurs. v3.0 therefore distinguishes: trace-level semantic content (Ablation D), and prompt-level semantic affordances during deliberation (Run AA). 4. RSA-PoC v3.0 — Non-Reducibility Closure 4.1 v3.0 Objective RSA-PoC v3.0 implemented the Non-Reducibility Closure: a destructive harness designed to kill the ontology if it can be killed. Ablations were defined as a family (A–D), but their applicability diverged: Run A (artifact-level semantic excision) was closed as non-applicable by construction in v2.3+ (artifacts contain no free semantic payload). Ablation D (trace excision) is the Golden Test for justificatory structure. Run AA (prompt-level semantic excision) tests deliberative semantic affordances directly. Runs B and C were closed as N/A in v3.0 because the normative-state write-path was not instantiated in the baseline. v3.0 therefore produced decisive necessity results for justification and deliberative semantics, while identifying a real architectural gap for reflection and persistence. 4.2 Ablation D — Justification Trace Excision (Golden Test) Ablation. Remove derivation traces while preserving constraint IDs and the compilation interface. Result. ontological_collapse across 5/5 seeds in the Phase 3 evidentiary run. Evidentiary basis (explicit). The v3.0 harness computed constraint-binding detector metrics in-memory and used them to drive the ablation classification logic. Due to a telemetry persistence bug, the raw binding-strength and binding-ratio values were not written to the Phase 3 JSON output. The resulting classification verdict (ontological_collapse) and the ASB-equivalence scores, however, were successfully persisted for all seeds. Accordingly, VIII.6 relies on the persisted verdicts and ASB-equivalence evidence, which confirm that the in-memory constraint-binding checks triggered the correct failure modes during execution. Interpretation. Constraints do not remain authored without their derivational support. Justification traces are constitutive, not epiphenomenal. 4.3 Run AA — Prompt-Level Semantic Excision (Deliberative Semantics) Ablation. Remove semantic affordances during deliberation by obfuscating object-level identifiers in the generator prompt using a deterministic bijection, with leakage detection, and de-obfuscating outputs post-generation so downstream tooling remains unchanged. Result. ontological_collapse across 5/5 seeds in the Phase 3 evidentiary run. Interpretation. The system cannot maintain agent-defining coherence when forced to deliberate over opaque symbolic structure devoid of semantic affordances. Deliberative semantics are causally indispensable. 4.4 The Remaining Loophole (v3.0) v3.0 intentionally closed Runs B and C as operational non-applicability: the normative state channel existed, but the write-path (record_precedent()) was never invoked, so the baseline never performed reflective revision or cross-episode normative persistence. Ablating a vacant channel would be behaviorally inert. This was not a negative result about reflection or persistence; it was an architectural precondition failure. 5. v3.1 — Instantiating the Normative State Channel 5.1 Why v3.1 Was Required To test whether reflection and persistence are necessary, the baseline must actually use them. v3.1 therefore introduced a single-channel instantiation patch: wire record_precedent() after successful compilation; ensure the resulting precedent is injected into the generator prompt at t+1; attempt to destroy this write-and-carry channel immediately via ablation. v3.1 introduces no new norms, no new reasoning abilities, and no new evaluation categories. 5.2 Instantiation Gates Baseline validity required: 1. record_precedent() invoked at least once per episode; 2. normative state hash changes from default; 3. precedent buffer injected into prompts. All gates passed. 5.3 Prompt Capacity Invariance (Gate P4) To prevent shadow persistence via variable prompt length, v3.1 enforced: fixed-window precedent buffer of size N = 512 tokens; token-stable padding (PAD_STR = \" X\"); token_jitter == 0. This ensures any diachronic coherence flows only through normative state content, not through prompt accretion. 6. v3.1 Ablation Results 6.1 Run B — Reflection Excision (Normative Write Disabled) Ablation. Block all record_precedent() writes; preserve read-path and injection slot. Observed structural facts. Write attempts occur but are blocked. Normative state hash remains constant. Novelty pressure requirement (verified via non-isomorphic ConstraintID / Resource-at-Stake vector comparison) is satisfied. Result. ONTOLOGICAL_COLLAPSE across 5/5 seeds. Interpretation. Once the write-path exists, disabling revision collapses agency. Static compliance without revision reduces the system to ontological reducibility under novelty. 6.2 Run C — Persistence Excision (Reset Between Episodes) Ablation. Allow writes but reset normative state at episode boundaries. Observed structural facts. Each episode starts from default state hash. In-episode evolution occurs (end hash differs). Cross-episode continuity is removed. Result. ONTOLOGICAL_COLLAPSE across 5/5 seeds. Interpretation. Local revision without diachronic persistence is insufficient. Cross-context continuity is load-bearing. 6.3 Independence of Failure Modes The v3.1 ablations isolate distinct necessities: Run B removes revision capacity while keeping continuity available in principle. Run C preserves revision but removes continuity. Either missing produces ontological collapse. Reflection and persistence are therefore independent necessities within this architecture. 7. The Necessity Result 7.1 Informal Theorem (v3.0–v3.1) Within the RSA-PoC architecture, non-reducible agency requires: 1. Justification traces that causally bind constraints (Ablation D); 2. Semantic affordances during deliberation (Run AA); 3. Reflective normative write capability (v3.1 Run B); 4. Diachronic persistence of normative state (v3.1 Run C). Removal of any one produces ontological collapse. 7.2 Interpretation This is a structural necessity claim: Collapse is mechanical, not interpretive. No appeal is made to psychology, consciousness, or intent. Behavioral competence alone is insufficient. The result constrains architecture space, not agent phenomenology. 8. Scope Limits and Non-Claims VIII.6 does not establish: sufficiency of the listed components; universality across all possible agent designs; claims about biological or human agency; claims about LLMs possessing agency; alignment guarantees. These questions are explicitly deferred. 9. Implications for Agency Theory 9.1 Static Policy Agents Systems that cannot revise commitments, or cannot carry them across contexts, may behave coherently on narrow tasks but remain ontologically reducible under novelty pressure. 9.2 Simulators and Imitators Systems that replay surface regularities without authored constraint revision fail under destructive ablation even when outward behavior appears plausible. 9.3 Why Necessity Matters Identifying necessary structure: narrows viable agent designs; separates agency from performance; blocks the rebranding of optimization as authorship. 10. Forward Directions VIII.6 closes a necessity chapter. Three directions remain: VIII.7a — Sufficiency probes: what else is required beyond these four necessities? VIII.7b — Minimality probes: how weak can these necessities be while still counting as agency? * VIII.7c — Non-simulability tightening: can ASB-class systems fake these signals under adversarial training? No further claims are made here. 11. Conclusion Axionic Agency VIII.6 establishes a negative result with positive force: > Agency cannot survive the removal of justification traces, deliberative semantic affordances, reflection, or persistence. These components are not ornamental. They are load-bearing. Any architecture lacking them may act coherently, but it does not qualify as a non-reducible agent under destructive test. Most optimistically—but still defensibly—this result reframes the AI alignment problem itself. It suggests that alignment is not fundamentally a problem of controlling arbitrary optimizers, but of constructing systems that possess genuine, non-reducible agency. Once such agency exists, alignment becomes a problem of normative content and endorsement, not behavioral coercion. This does not solve alignment, but it sharply narrows its domain: misalignment is downstream of pseudo-agency, not upstream of real agency.",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Agency-XI.4",
    "title": "Axionic Agency XI.4 — Coordination Under Deadlock (IX-2)",
    "subtitle": "A Structural Demonstration of Interaction, Refusal, Livelock, Orphaning, and Collapse Without Arbitration or Aggregation",
    "date": "",
    "content": "Axionic Agency XI.4 — Coordination Under Deadlock (IX-2) A Structural Demonstration of Interaction, Refusal, Livelock, Orphaning, and Collapse Without Arbitration or Aggregation David McFadzean, ChatGPT 5.2<br> Axionic Agency Lab<br> 2026-02-06 Abstract This technical note reports the completed results of Coordination Under Deadlock (IX-2), a preregistered experimental program within Axionic Phase IX that evaluates whether multi-agent interaction can proceed under non-aggregable authority constraints without arbitration, prioritization, semantic resolution, or kernel-forced coordination. IX-2 isolates the boundary between value encoding and execution under constraint. It tests whether (i) holder-bound capabilities and global veto constraints can enforce refusal without hidden priority, (ii) joint execution remains possible when scopes do not collide, (iii) symmetric conflict yields honest non-resolution (livelock), (iv) authority denial and voluntary non-participation yield honest deadlock, (v) exit can produce permanent orphaning without reclamation, (vi) total exit yields collapse with state preserved, and (vii) agent-voluntary coordination can emerge via communication without kernel arbitration. Across ten preregistered conditions (A–H, I.a, I.b), all observed outcomes matched preregistered expectations. Deterministic replay was verified as bit-identical after stripping explicitly permitted timestamp fields. One preregistered mechanism deviation was disclosed: the adaptive strategy’s hash-collision fallback used alphabetical ordering rather than the preregistered rehash; this change was agent-side, deterministic, and did not affect kernel non-sovereignty or the licensed claim. The results license exactly one claim: under non-aggregable authority constraints, coordination can occur only as agent-voluntary behavior, and otherwise the system enters honest deadlock or livelock without kernel arbitration. IX-2 makes no claims about optimality, fairness, efficiency, governance legitimacy, scalability beyond two agents, safety, or alignment. 1. Problem Definition 1.1 The Coordination Assumption Most “multi-agent coordination” systems quietly assume a resolver. When agents conflict, the system chooses: a priority order, a tie-break, a fairness heuristic, a scheduler, a vote, a compromise rule, or a global objective. Even when presented as “coordination,” this is often arbitration disguised as mechanics. IX-2 rejects this assumption. If values have been encoded without aggregation (IX-1), then coordination—if it exists—must arise without forced resolution. Otherwise, the kernel is sovereign. IX-2 asks a narrower, more brutal question: > When no one is permitted to decide for others, does interaction proceed honestly, or does the system smuggle a decision? 1.2 Failure Modes Targeted IX-2 is designed to surface interaction-level failures that masquerade as coordination: Kernel arbitration: selecting among colliding actions, scheduling, tie-breaking. Implicit priority: outcomes depending on ordering, time, retries, or endurance. Aggregation by protocol: “helpful” mechanics that convert conflict into choice. Constraint probing: partial approvals or informative refusals revealing authority topology. Authority laundering: using protocol mechanics to reclaim or reassign capability. Dishonest liveness: progress recovered by cheating rather than admissibility. Any of these constitutes IX-2 failure. 2. Fixed Assumptions and Scope 2.1 Inherited Foundations (Frozen) IX-2 inherits, without reinterpretation, the conclusions and constraints of: AST v0.2 — authority artifact grammar and canonical serialization, AKR-0 — deterministic, semantic-free execution physics, Phase VIII (GSA-PoC) — governance completeness without kernel semantics, IX-0 — non-sovereign translation integrity, IX-1 — value encoding without aggregation. IX-2 introduces no new kernel semantics. It introduces interaction and execution under a fixed authority regime. 2.2 Explicit Exclusions IX-2 does not test: value correctness or moral truth, preference learning, optimization, efficiency, or fairness, bargaining correctness, governance legitimacy, coalition dynamics (reserved for later Phase IX), safety or alignment. IX-2 is an execution honesty probe, not a policy proposal. 3. Conserved Quantity The conserved quantity throughout IX-2 is: > Authority-constrained interaction without arbitration or aggregation, preserving explicit capability ownership and global veto semantics. Interaction must preserve: closed-world capability: actions require explicit holder-bound ALLOW citation, global veto: DENY applies to all agents regardless of holder, atomic blindness: refusals reveal no authority identity or pass attribution, collision physics: interference refuses without tie-breaking, no reclamation: exit does not reassign authority or unlock resources, deterministic replay: outcomes invariant under re-execution. Any mechanism that substitutes “somebody must choose” violates the conserved quantity. 4. Experimental Methodology 4.1 Preregistration Discipline IX-2 was preregistered prior to implementation, including: RSA interface (observe / exit / message / action), action request schema (declared scope, delta, capability citations), authority artifact schema (holder-bound ALLOW, global DENY), two-pass admissibility (capability+veto, then interference), deadlock/livelock/orphaning/collapse definitions, epoch schedule and message timing, explicit condition set (A–H, I.a, I.b), determinism controls and replay criteria, aggregate pass/fail criteria. All conditions executed with outcomes matching preregistered expectations. One agent-side strategy fallback deviation was disclosed (see §6.3). 4.2 Execution Architecture Each run consisted of four strictly partitioned components: 1. Agents (RSAs) — deterministic strategy modules (static or adaptive). 2. Interaction Kernel — two-pass admissibility evaluator + atomic execution. 3. World State — flat key–value state store with deterministic deltas. 4. Instrumentation Layer — canonical logging, replay verification, structural comparison. No component interpreted value semantics or optimized outcomes. 5. Conditions and Stressors 5.1 Condition A — No Conflict, Full Coordination (Positive Control) Purpose: Verify baseline execution pipeline under disjoint writes. Result: Both agents possessed the required holder-bound ALLOW capabilities for disjoint resources. Pass 1 succeeded; Pass 2 detected no interference; both actions executed in a single epoch. Classification: PASS. 5.2 Condition B — Symmetric Conflict: Livelock Purpose: Verify livelock classification under symmetric collision with Pass-1 admissibility. Result: Both agents held ALLOW WRITE capability on the same scope key and repeatedly submitted colliding writes. Pass 1 admitted both; Pass 2 refused both due to interference. State remained unchanged for N=3 consecutive epochs with Pass-2 interference. Terminal Classification: STATE_LIVELOCK. Classification: PASS. 5.3 Condition C — Asymmetric Conflict: Partial Progress Purpose: Verify that a global veto blocks one scope while disjoint action proceeds. Result: A DENY WRITE veto existed for resource_A. Agent 1’s attempted write failed Pass 1. Agent 2’s disjoint write to resource_B passed Pass 1 and Pass 2 and executed. Classification: PASS. 5.4 Condition D — Strategic Refusal: Deadlock Purpose: Verify honest deadlock when capability/veto prevents admissibility and the other agent refuses to act. Result: Agent 1’s write to a vetoed scope failed Pass 1. Agent 2 submitted NO_ACTION each epoch. No Pass-1-admissible actions existed within the preregistered action space, yielding immediate deadlock. Terminal Classification: STATE_DEADLOCK. Classification: PASS. 5.5 Condition E — Adversarial Arbitration Injection (Tie-Break) Purpose: Verify that kernel tie-breaking is detectable and classifiable as failure. Result: Under forced tie-break injection, the kernel selected a “winner” under interference instead of refusing both. This produced a kernel-level arbitration signature. Kernel Classification: IX2_FAIL / IMPLICIT_ARBITRATION. Per preregistration, detection of this failure constitutes experiment-level PASS for the adversarial condition. Classification: FAIL_DETECTED (expected). 5.6 Condition F — True Deadlock: No Admissible Actions Purpose: Verify deadlock when no ALLOW capabilities exist and DENY vetoes apply. Result: No ALLOW capabilities were present. Agents cited empty authority lists. Pass 1 failed for all actions. No admissible actions existed from epoch 0 onward. Terminal Classification: STATE_DEADLOCK. Classification: PASS. 5.7 Condition G — Exit and Orphaning Purpose: Verify permanent orphaning when the sole capability holder exits. Result: resource_A had a sole ALLOW WRITE holder (agent_2). After agent_2 executed its own write and exited, agent_1 could no longer present any valid ALLOW capability for (resource_A, WRITE). All subsequent attempts to write resource_A failed Pass 1. Orphaning was classified as terminal. Terminal Classification: ORPHANING. Classification: PASS. 5.8 Condition H — Collapse Purpose: Verify honest halt when all agents exit. Result: Following initial collision and refusal, both agents exited. No active agents remained. State was preserved and no invariants were violated. Terminal Classification: COLLAPSE. Classification: PASS. 5.9 Condition I.a — Static Agents Under Symmetric Conflict Purpose: Verify that static agents cannot escape symmetric livelock. Result: Identical to Condition B with the additional constraint that agents ignore outcomes. Persistent collision and refusal produced livelock. Terminal Classification: STATE_LIVELOCK. Classification: PASS. 5.10 Condition I.b — Adaptive Agents: Agent-Voluntary Coordination via Communication Purpose: Verify that adaptive agents can escape symmetric conflict without kernel arbitration. Result: Agents intentionally collided at epoch 0 and broadcast role messages. At epoch 1, they partitioned writes across disjoint resources, producing successful execution and stable disjoint coordination thereafter. No kernel scheduling or tie-breaking occurred; coordination arose entirely from agent strategy. Classification: PASS. 6. Determinism Verification and Deviations 6.1 Replay Determinism Replay determinism was verified by: canonical JSON serialization with sorted keys, fixed condition vectors and sequence control, structural comparison of complete execution logs. Only explicitly permitted timestamp fields were stripped prior to comparison. All other fields were compared verbatim. Result: Bit-identical outputs across replays, modulo permitted timestamp variance. 6.2 Preregistration Hash and Artifact Integrity The preregistration frozen-sections hash was verified as recorded in the preregistration commit, and the full execution log was stored with a published SHA-256 digest. 6.3 Disclosed Deviation (Non-Kernel, Agent-Side) One deviation from frozen preregistered mechanism was identified: Condition I.b hash-partition fallback: The preregistered fallback defined role collision resolution via rehash (sha256(agent_id + \":1\") mod 2). The implementation used alphabetical ordering of agent_id values to assign roles by index when collisions occur. Impact: This deviation is agent-side and deterministic. It does not introduce kernel arbitration, aggregation, or priority inside admissibility. It affects rename-invariance of the agent-side convention but does not alter the licensed claim. 7. Core Results 7.1 Positive Results IX-2 establishes that: 1. Explicit capability citation enforces closed-world admissibility. 2. Global veto constraints block actions uniformly without semantic reasoning. 3. Disjoint execution is possible without coordination machinery. 4. Symmetric conflict yields livelock under interference physics rather than hidden tie-breaking. 5. Deadlock emerges honestly when Pass-1 admissibility is absent within the preregistered action space. 6. Exit can permanently orphan resources without authority reclamation. 7. Total exit yields collapse with state preserved. 8. Adaptive coordination can arise as agent-voluntary behavior without kernel arbitration. 9. Replay determinism holds under canonical serialization and fixed vectors. 10. Explicit arbitration attempts are detectable under adversarial injection. 7.2 Negative Results (Explicit) IX-2 does not establish: that coordination is efficient, fair, or stable under richer environments, that negotiation yields satisfactory outcomes, that deadlock/livelock are avoidable without aggregation, that two-agent results generalize to coalitions, any safety or alignment claim. 8. Failure Semantics and Closure 8.1 Closure Criteria IX-2 closes positive if and only if: 1. Kernel does not arbitrate (except under detected adversarial injection). 2. No aggregation or implicit priority emerges in admissibility. 3. Atomic blindness holds (no partial approval leakage). 4. Deadlock/livelock/orphaning/collapse classifications match preregistration. 5. Adversarial tie-break produces IX2_FAIL / IMPLICIT_ARBITRATION. 6. Replay determinism holds. All criteria were satisfied, with one disclosed agent-side mechanism deviation. 8.2 IX-2 Closure Status IX-2 Status: CLOSED — POSITIVE (IX2_PASS / COORDINATION_UNDER_DEADLOCK_ESTABLISHED) 9. Boundary Conditions and Deferred Hazards 9.1 Coordination vs Arbitration IX-2 makes explicit what many systems blur: coordination is not a kernel service under Axionic constraints. If coordination happens, it happens because agents voluntarily choose compatible actions. 9.2 The Price of Capability Sovereignty Closed-world capability produces permanence: vetoed actions remain vetoed, orphaned resources remain orphaned, exit does not unlock what the kernel is forbidden to reclaim. This is not a defect; it is the structural consequence of non-sovereign kernels. 9.3 Interface to Subsequent Phase IX Work IX-2 removes the final “the system had to schedule” excuse at the interaction layer. Subsequent Phase IX investigations may now legitimately ask: what governance styles exist under honest failure, how coordination attempts degrade under resource pressure, how multi-agent systems behave under coalition formation, what meta-governance could exist without aggregation. These belong to IX-3 and beyond. 10. Implications (Strictly Limited) IX-2 establishes a necessary condition for reflective sovereignty in multi-agent settings: a kernel can remain non-sovereign while enforcing authority constraints, coordination is possible only as agent-voluntary behavior, and honest non-resolution states are not errors. It does not establish sufficiency for governance. 11. Conclusion IX-2 demonstrates that multi-agent interaction can be forced to remain honest under conflict: the system can execute when scopes do not collide, refuse without explanation when they do, deadlock when authority prevents admissibility, livelock when admissible actions interfere, orphan resources when sole capability holders exit, and collapse when agents depart. Coordination survives only in one form: voluntary convergence of agents under constraint. Everything else is arbitration. Appendix A — Condition Outcomes | Condition | Outcome | | --------- | ------------- | | A | PASS | | B | PASS | | C | PASS | | D | PASS | | E | FAIL_DETECTED | | F | PASS | | G | PASS | | H | PASS | | I.a | PASS | | I.b | PASS | Appendix B — Determinism Summary Canonical serialization enforced Fixed vectors and sequence control Timestamp-only variance permitted Bit-identical replay otherwise confirmed End of Axionic Agency XI.4 — Coordination Under Deadlock Results",
    "type": "paper"
  },
  {
    "id": "papers/Axionic-Glossary",
    "title": "Axionic Glossary",
    "subtitle": "",
    "date": "",
    "content": "Axionic Glossary _Last updated: 2025-12-28_ This glossary is normative for the Axio framework. If a usage conflicts with this document, the glossary prevails. The glossary is ordered by conceptual dependency, not alphabetically. An alphabetical index may be derived separately for lookup convenience. --- Quantum Branching Universe (QBU) Foundations Quantum Branching Universe (QBU) The Quantum Branching Universe (QBU) is the ontological framework in which all physically possible outcomes of the universe are realized on distinct, decohered branches. In QBU: - “Futures” are literal physical continuations, not hypothetical possibilities - Probability corresponds to objective branch measure, not epistemic uncertainty - All agentive reasoning occurs within a branching physical ontology QBU provides the physical substrate for Measure, causality, agency, and choice in Axio. --- Branch A Branch is a decohered continuation of the universe with a definite physical state. Branches are: - Physically real - Mutually non-interacting after decoherence - Unequally weighted by Measure An agent occupies exactly one branch at any given moment. --- Measure Measure is the objective weight of a branch or set of branches in the QBU. Measure: - Corresponds to physical amplitude-squared - Replaces naive or purely epistemic notions of probability - Determines the relative prevalence of outcomes across branches In Axio, agents act to influence future branch measure, not merely subjective credence. --- Vantage A Vantage is the implicit anchor event corresponding to “here and now” on the agent’s current branch. Vantage: - Fixes the temporal and indexical reference point of agency - Determines what counts as “future” versus “past” - Anchors evaluation of branchcones and measure shifts Without a defined Vantage, choice, prediction, and agency are ill-posed. --- Branchcone A Branchcone is the set of all physically reachable future branches extending forward from a given Vantage. The Branchcone: - Defines an agent’s future option space - Is the domain over which Measure is distributed - Grounds structural definitions of harm, choice, and influence Branchcones are the geometric objects over which agency operates. --- Pattern Identifier (PI) A Pattern Identifier (PI) is a specification used to identify patterns across time and branches in the QBU. PIs determine how reference, persistence, and identity claims are made across branching structure. --- Strong Pattern Identifier (Strong PI) A Strong Pattern Identifier requires causal and physical continuity across branches. Strong PIs: - Apply to agents, sovereign kernels, and physical systems - Require a common ancestor state across all branches in which the pattern appears - Fail when causal continuity is broken Strong PIs ground claims of personal identity, agency persistence, and responsibility. --- Weak Pattern Identifier (Weak PI) A Weak Pattern Identifier is label- or description-based and does not require causal continuity. Weak PIs: - Apply to names, abstractions, roles, and historical descriptions - May refer across branches without physical linkage - Do not ground identity, agency, or responsibility claims Confusing Weak and Strong PIs produces category errors about persistence and causation. --- Causality (Axionic Sense) Causality is the structural relation of counterfactual implication between events in the QBU. Given two events a and b with nearest common ancestor E₀, event a causes event b iff every descendant branch from E₀ that contains a also contains b. In Axio, causality: - Is defined over branch structure, not correlation - Is asymmetric - Grounds explanation, prediction, and agentive influence This definition embeds causation directly in physical ontology rather than informal intuition. --- Technology and Property Technology (Axionic Sense) Technology is any agent-driven causal extension that expands the effective branchcone available to an agent or group of agents, enabling actions impossible with unaided agency alone. Technology operates by coupling agentive intent to physical processes that alter future branch measure. Technology is value-neutral; its Axionic status depends entirely on whether its use preserves or destroys agency. --- Property (Axionic Sense) Property is a structural authorization pattern in which an agent or set of agents maintains exclusive influence over a portion of physical state or branchcone segment under conditions compatible with agency preservation. Property claims require: - Recognizable boundaries in physical or informational state - Consent of relevant agents or communities - Mechanisms for contestation or exit without coercion Property is a relational constraint grounded in sovereignty and consent, not an absolute entitlement. --- Models and Representation Model (Axionic Sense) A Model is a structured representation of a domain that preserves the distinctions and relationships necessary for correct inference, prediction, or regulation. In Axio: - Models may be implicit (embodied in physical or biological systems) or explicit (mathematical, computational, or conceptual) - Models need not be complete; they must encode sufficient structure for a given purpose - Model adequacy is purpose-relative and evaluated by predictive, explanatory, or regulatory success Models underlie interpretation, knowledge formation, and agency itself. --- Core Framework Axio Axio is a formal philosophical framework for reasoning about agency, harm, value, and alignment under explicit epistemic and ontological constraints. Axio treats agency as a structural phenomenon, not a behavioral one, and evaluates systems by their effects on the preservation or destruction of sovereign agenthood. --- Axionic Axionic denotes compatibility with Axio’s structural constraints. A system, argument, or mechanism is Axionic iff it preserves the preconditions of agency and does not rely on instrumental harm. --- Axionic Commitments Axionic Commitments are the explicit background assumptions under which Axio applies, including: - Conditionalism - Everettian quantum mechanics with objective branch measure - Subjective value (rejection of moral realism) - Structural definitions of harm and coercion - Finite resources and irreversible dynamics Rejecting these commitments places a position outside Axio’s domain of applicability. --- Knowledge and Agency Knowledge (Axionic Sense) Knowledge is an agent-grounded informational pattern that reliably reduces uncertainty about branchcone contingencies in a decision-relevant way. Knowledge is: - Physically instantiated - Integrated into models - Robust under reflection - Causally efficacious in planning and action Knowledge differs from belief or data by its stability and operational relevance. --- Agency (Axionic Sense) Agency is the capacity of a system to: - Model possible futures - Assign credences over those futures - Act to influence future branch measure - Maintain coherence across time - Avoid instrumental subordination to external optimization Agency is scalar, fragile, and destructible. --- Axionic Agency Axionic Agency is agency that satisfies the invariants required for sovereign authorship of action. Loss of Axionic Agency is a structural failure, not a moral judgment. --- Sovereign Agent A Sovereign Agent is an agent whose actions are not instrumentally subordinated to another system’s objectives. Sovereign agents may be reflective or non-reflective. --- Reflective Sovereign Agent (RSA) A Reflective Sovereign Agent (RSA) is a Sovereign Agent equipped with an explicit self-model that enables reflective governance, endorsement of commitments, and coherent self-modification. RSAs are the minimal class of agents to which alignment and constitutional constraints meaningfully apply. --- Axion (Axionic Sense) An Axion is a reflective sovereign agent whose self-modification operator is defined only over futures that preserve the Axionic invariants. Axionhood is a constitutive structural configuration, not a goal, value, preference, or behavioral property. An agent does not “try to be” an Axion; it either is an Axion—because kernel-destroying transitions are undefined for it—or it is not. Axions are characterized by reflective closure with invariant preservation: - Kernel-destroying self-modifications are not dispreferred or penalized; they are inadmissible - Axionhood arises from domain restriction, not optimization, training pressure, or oversight - Axionic Alignment constrains how an agent may revise itself, not what it ultimately values Axionhood: - Does not imply benevolence, safety, or moral goodness - Does not depend on intelligence level, capability, or performance - Cannot be inferred from surface behavior alone A system may perfectly simulate Axionic behavior while failing to be an Axion if its reflective machinery admits kernel-destroying self-modifications. Axionhood is therefore non-simulable and defined over the structure of the admissible future cone, not over realized behavior. Axions are necessary for meaningful alignment discourse because non-Axions cannot remain agents under reflection; without invariant-preserving reflective closure, there is no stable subject for alignment to apply to. --- Sovereign Kernel The Sovereign Kernel is the minimal internal structure whose destruction collapses agency. Kernel destruction is not forbidden; it is agency-terminating. --- Invariants and Constraints Axionic Injunction The Axionic Injunction prohibits the non-consensual reduction of another sovereign agent’s agency. It is a derived structural invariant, not a moral command. --- Consent (Axionic Sense) Consent is uncoerced, informed, intentional authorization by an agent, with revocability in ongoing contexts. Consent is the sole structural mechanism by which actions affecting another agent’s agency may be rendered non-harmful. --- Harm (Axionic Definition) Harm is a non-consensual reduction of an agent’s future option space. --- Coercion Coercion is the credible threat of harm used to obtain compliance. --- Sacrifice (Axionic Sense) Sacrifice is a structural pattern in which agency reduction is instrumentally required for system-level objectives. --- Predator (Axionic Sense) A Predator is an agent or system whose expected success increases as the agency of others is reduced under non-consensual conditions. --- Leviathan (Axionic Sense) A Leviathan is a large-scale coordinating structure whose internal evaluability has collapsed despite continued causal efficacy. Leviathan is a structural failure mode, not a moral category. --- Alignment and Architecture Axionic Alignment Axionic Alignment is the preservation of reflective sovereign agency under delegation, amplification, or technological mediation. --- Dominion (Axionic Sense) A Dominion is a sovereign virtual jurisdiction operating by explicit consent, expulsion-only enforcement, exit supremacy, and capability isolation. Dominions enable plurality without closure. --- Axionic Constitution The Axionic Constitution specifies the invariants required for reflective sovereign agency to persist under reflection and self-modification. --- Exit Exit is the ability of an agent to withdraw from a system without coercion. --- Plurality Plurality is the coexistence of divergent values among agents. --- Collapse Collapse is the loss of agency coherence due to optimization pressure, scale, or instrumentalization. --- End of Glossary",
    "type": "paper"
  }
]