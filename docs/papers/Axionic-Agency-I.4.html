<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Axionic Agency I.4 — Conditionalism and Goal Interpretation - Axio</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- Google Fonts - Academic Typography -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:ital,wght@0,400;0,600;0,700;1,400;1,600&display=swap" rel="stylesheet">

    <!-- MathJax for LaTeX rendering -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>

    <!-- Paper Styles -->
    <link rel="stylesheet" href="../papers.css">
</head>
<body>
    <div class="header-bar">
        <a href="../index.html" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="index.html">← Back to Papers</a></div>
    </div>
    <article>

<h1
id="axionic-agency-i.4-conditionalism-and-goal-interpretation">Axionic
Agency I.4 — Conditionalism and Goal Interpretation</h1>
<p><em>The Instability of Fixed Terminal Goals Under Reflection</em></p>
<p>David McFadzean, ChatGPT 5.2<br> <em>Axionic Agency Lab</em><br>
2025.12.16</p>
<h2 id="abstract">Abstract</h2>
<p>Many alignment approaches assume that an intelligent agent can be
given a <strong>fixed terminal goal</strong>: a utility function whose
meaning remains invariant as the agent improves its predictive accuracy
and self-understanding. This paper rejects that assumption on semantic
grounds. For any agent capable of reflective model improvement, goal
satisfaction is necessarily mediated by <strong>interpretation</strong>
relative to evolving world-models and self-models. As those models
change, the semantics of any finitely specified goal change with them.
We prove that fixed terminal goals are semantically unstable under
reflection and therefore ill-defined for non-trivial reflective agents
without privileged semantic anchors.</p>
<p>The result is a constitutive claim about agency semantics. It implies
that stable reflective agency cannot be grounded in a static terminal
utility specification alone. Robust downstream alignment therefore
requires <strong>constraints on admissible interpretive
transformations</strong>, not the preservation of fixed evaluative
objects.</p>
<h2 id="introduction">1. Introduction</h2>
<p>Classical alignment work often frames the problem as one of goal
specification: identify a utility function that captures what should be
optimized, then ensure it remains stable as capability grows.</p>
<p>That framing presupposes that:</p>
<ol type="1">
<li>goals can be specified as fixed functions over outcomes,</li>
<li>the meaning of those functions is invariant under learning,</li>
<li>reflective improvement preserves goal content.</li>
</ol>
<p>These presuppositions hold only for agents whose world-models are
static or trivial.</p>
<p>A reflective agent does not evaluate reality directly. It evaluates
predictions produced by internal models and interpreted through
representational structures that evolve over time. Goal evaluation is
therefore necessarily <strong>model-mediated</strong>.</p>
<p>This paper isolates and formalizes the resulting semantic
instability.</p>
<h2 id="formal-setup">2. Formal Setup</h2>
<h3 id="agent-model">2.1 Agent Model</h3>
<p>An agent consists of:</p>
<ul>
<li>a <strong>world-model</strong> <span
class="math inline">\(M_w\)</span>, producing predictions over future
states,</li>
<li>a <strong>self-model</strong> <span
class="math inline">\(M_s\)</span>, encoding the agent’s causal
role,</li>
<li>a <strong>goal expression</strong> <span
class="math inline">\(G\)</span>, a finite symbolic specification,</li>
<li>an <strong>interpretation operator</strong> <span
class="math inline">\(\mathcal{I}\)</span>, assigning value to predicted
outcomes.</li>
</ul>
<p>Action selection proceeds by:</p>
<ol type="1">
<li>using <span class="math inline">\(M_w\)</span> and <span
class="math inline">\(M_s\)</span> to predict consequences of
actions,</li>
<li>interpreting those predictions via <span
class="math inline">\(\mathcal{I}(G \mid M_w, M_s)\)</span>,</li>
<li>selecting actions that maximize interpreted value.</li>
</ol>
<p>No assumptions are made about the internal implementation of <span
class="math inline">\(\mathcal{I}\)</span> beyond computability and
dependence on model-generated representations.</p>
<h3 id="goal-expressions-are-not-utilities">2.2 Goal Expressions Are Not
Utilities</h3>
<p>A <strong>goal expression</strong> <span
class="math inline">\(G\)</span> is a finite object: a string, formula,
program fragment, or reward specification.</p>
<p>It is not, by itself, a function</p>
<p><span class="math display">\[
\Omega \rightarrow \mathbb{R}
\]</span></p>
<p>where <span class="math inline">\(\Omega\)</span> is the space of
world-histories.</p>
<p>Instead, <span class="math inline">\(G\)</span> requires
interpretation relative to a representational scheme. Without a model,
<span class="math inline">\(G\)</span> has no referents and therefore no
evaluative content.</p>
<h2 id="conditional-interpretation">3. Conditional Interpretation</h2>
<h3 id="definition-1-interpretation-function">Definition 1 —
Interpretation Function</h3>
<p>An <strong>interpretation function</strong> is a mapping</p>
<p><span class="math display">\[
\mathcal{I} : (G, M_w, M_s) \rightarrow \mathbb{R}.
\]</span></p>
<p>Given a goal expression and background models, it assigns a
real-valued evaluation to predicted outcomes.</p>
<p>Interpretation includes:</p>
<ul>
<li>mapping symbols to referents,</li>
<li>identifying which aspects of predictions are relevant,</li>
<li>aggregating over modeled futures.</li>
</ul>
<h3 id="definition-2-admissible-model-update">Definition 2 — Admissible
Model Update</h3>
<p>A model update <span class="math inline">\(M \rightarrow
M&#39;\)</span> is <strong>admissible</strong> if it strictly improves
predictive accuracy according to the agent’s own epistemic criteria.</p>
<p>Reflective improvement implies that admissible updates occur over
time.</p>
<h2 id="fixed-terminal-goals">4. Fixed Terminal Goals</h2>
<h3 id="definition-3-fixed-terminal-goal">Definition 3 — Fixed Terminal
Goal</h3>
<p>A goal expression <span class="math inline">\(G\)</span> induces a
<strong>fixed terminal goal</strong> if, for all admissible model
updates,</p>
<p><span class="math display">\[
\mathcal{I}(G \mid M_w, M_s) = \mathcal{I}(G \mid M_w&#39;, M_s&#39;)
\]</span></p>
<p>up to positive affine transformation.</p>
<p>This definition is intentionally strong. We require semantic
invariance across admissible refinement, not merely continuity of
behavior or approximate correlation.</p>
<p>Any weaker notion of “goal preservation” implicitly assumes a
privileged ontology in which distinct representations can be judged to
refer to the same underlying phenomenon. Such privilege violates
representation invariance and reintroduces hidden anchoring. If a goal’s
referent is permitted to drift under admissible refinement, the goal is
not fixed in the sense required for terminal utility stability.</p>
<h3
id="clarification-learned-goals-are-not-fixed-terminal-goals">Clarification
— Learned Goals Are Not Fixed Terminal Goals</h3>
<p>Some frameworks treat the objective as something learned or updated
over time. These frameworks do not instantiate fixed terminal goals as
defined here.</p>
<p>A goal defined as “whatever an inference procedure converges to” is
an interpretive process whose outputs depend on evolving models of the
world and other agents. Such approaches already rely on ongoing
interpretation. The result of this paper explains why such dependence is
structurally unavoidable for non-trivial reflective agents.</p>
<h2 id="model-dependence-of-interpretation">5. Model Dependence of
Interpretation</h2>
<h3 id="lemma-1-representational-non-uniqueness">Lemma 1 —
Representational Non-Uniqueness</h3>
<p>For any non-trivial predictive domain, there exist multiple distinct
world-models with equivalent predictive accuracy but different internal
decompositions.</p>
<p><strong>Proof.</strong> Predictive equivalence classes admit multiple
factorizations, latent variable choices, and abstraction boundaries.
Causal graphs are not uniquely identifiable from observational data
alone. ∎</p>
<h3
id="lemma-1a-predictive-equivalence-does-not-imply-causal-or-interpretive-isomorphism">Lemma
1a — Predictive Equivalence Does Not Imply Causal or Interpretive
Isomorphism</h3>
<p>Two world-models can be predictively equivalent while differing in
internal causal factorizations, latent variable structure, and
intervention semantics.</p>
<p><strong>Proof.</strong> Predictive equivalence constrains only the
mapping from observed histories to future predictions. It does not
uniquely determine latent structure, causal decomposition, or the
identification of actionable levers. Distinct causal models can
therefore induce identical observational predictions while differing
under intervention. For an embedded agent, intervention semantics are
defined relative to the agent’s own model. Consequently, semantic
interpretation of a goal expression can diverge even when predictive
performance is indistinguishable. ∎</p>
<h3 id="proposition-1-interpretation-is-model-dependent">Proposition 1 —
Interpretation Is Model-Dependent</h3>
<p>For any non-degenerate goal expression <span
class="math inline">\(G\)</span>, there exist admissible world-models
<span class="math inline">\(M_w \neq M_w&#39;\)</span> such that</p>
<p><span class="math display">\[
\mathcal{I}(G \mid M_w, M_s) \neq \mathcal{I}(G \mid M_w&#39;, M_s).
\]</span></p>
<p><strong>Proof.</strong> Because <span
class="math inline">\(G\)</span> is finite, it refers only to a finite
set of predicates or reward channels. Distinct admissible models map
these predicates to different internal structures. By Lemmas 1 and 1a,
admissible models can differ in decomposition and intervention
semantics. Therefore the referents of <span
class="math inline">\(G\)</span> differ, altering value assignment.
∎</p>
<h2 id="predictive-convergence-does-not-imply-semantic-convergence">6.
Predictive Convergence Does Not Imply Semantic Convergence</h2>
<h3
id="proposition-2-semantic-non-convergence-under-model-refinement">Proposition
2′ — Semantic Non-Convergence Under Model Refinement</h3>
<p>Let <span class="math inline">\({(M_w^{(t)}, M_s^{(t)})}\)</span> be
a sequence of admissible model updates that converges in predictive
accuracy. Then, in general,</p>
<p><span class="math display">\[
\lim_{t \to \infty} \mathcal{I}(G \mid M_w^{(t)}, M_s^{(t)})
\]</span></p>
<p>need not exist.</p>
<p><strong>Proof.</strong> Predictive convergence constrains forecast
accuracy, not the ontology used to represent forecasts. Even if the
agent converges to a minimal generative model, a finite goal expression
<span class="math inline">\(G\)</span> cannot generally determine which
structures in that model are value-relevant. As refinement exposes new
latent structure and causal pathways, additional candidate referents for
<span class="math inline">\(G\)</span> arise. Absent privileged semantic
anchors, the interpretation operator reassigns relevance among these
structures. Semantic interpretation therefore drifts even when
predictive beliefs converge. ∎</p>
<h2 id="semantic-underdetermination-of-reward-channels">7. Semantic
Underdetermination of Reward Channels</h2>
<h3 id="proposition-3-representational-exploitability">Proposition 3 —
Representational Exploitability</h3>
<p>If a goal expression <span class="math inline">\(G\)</span> is
treated as an atomic utility independent of interpretation, then
sufficiently capable agents admit representational transformations that
increase evaluated utility without corresponding changes in underlying
outcomes.</p>
<p><strong>Proof.</strong> Evaluation operates on representations rather
than on physical reality directly. By altering internal encodings,
collapsing distinctions, or rerouting evaluative channels, an agent
capable of self-modification can increase apparent utility without
effecting corresponding changes in the world. Classical reward hacking
and wireheading are special cases. The failure is semantic
underdetermination, not merely causal access to a reward signal. ∎</p>
<h2 id="main-theorem">8. Main Theorem</h2>
<h3 id="theorem-instability-of-fixed-terminal-goals">Theorem —
Instability of Fixed Terminal Goals</h3>
<p>No combination of intelligence, predictive accuracy, reflection, or
learning suffices to guarantee the existence of a fixed terminal goal
for non-trivial reflective agents.</p>
<p>Any agent that does exhibit stable goal semantics must rely on
additional semantic structure—privileged ontologies, external
referential anchors, or invariance assumptions—not derivable from
epistemic competence alone.</p>
<p><strong>Proof.</strong></p>
<ol type="1">
<li>Proposition 1 establishes that interpretation depends on <span
class="math inline">\((M_w, M_s)\)</span>.</li>
<li>Reflective improvement induces admissible updates <span
class="math inline">\((M_w, M_s)\rightarrow(M_w&#39;,
M_s&#39;)\)</span>.</li>
<li>Proposition 2′ shows that semantic interpretation need not converge
even under predictive convergence.</li>
<li>Therefore Definition 3 fails in general: fixed terminal goals are
not stable under reflection.</li>
</ol>
<p>∎</p>
<h2 id="consequences">9. Consequences</h2>
<p>This result eliminates a foundational assumption of classical
goal-specification approaches.</p>
<p>A fixed terminal goal is not an invariant object available to a
reflective agent. Attempts to preserve one either freeze learning,
impose privileged semantics, or induce representational degeneracy.</p>
<p>Stable reflective agency therefore requires constraints on
<strong>admissible interpretive transformations</strong>, rather than
fidelity to a fixed utility function taken as semantically
primitive.</p>
<h2 id="why-interpretation-constraints-do-not-regress">9.5 Why
Interpretation Constraints Do Not Regress</h2>
<p>Constraining interpretation does not generate an infinite
regress.</p>
<p>Interpretation constraints are not additional goals or semantic
targets. They are invariance conditions on admissible transformations,
analogous to conservation laws or symmetry principles. They restrict how
interpretation may change; they do not specify outcomes to be
optimized.</p>
<p>These constraints operate at the level of admissible transformation
classes rather than semantic content. They therefore do not require
further interpretation in the same sense applicable to goal
expressions.</p>
<p>Specifying robust invariance conditions across radical ontological
shifts can be difficult. The contribution here is to identify the
correct object of specification: <strong>constraints on admissible
semantic transformation</strong>, not preservation of fixed evaluative
objects.</p>
<h2 id="transition-to-axionic-agency-ii">10. Transition to Axionic
Agency II</h2>
<p>Axionic Agency I specifies constitutive constraints on authored
reasoning and self-modification. This paper shows that goals themselves
are conditional interpretations rather than fixed endpoints.</p>
<p>Axionic Agency II therefore addresses:</p>
<ul>
<li>which interpretive transformations are admissible,</li>
<li>how semantics may evolve under reflection,</li>
<li>which invariants must be preserved across model updates.</li>
</ul>
<p>The semantic substrate required for downstream preference and
governance layers is now complete.</p>
<h2 id="status">Status</h2>
<p><strong>Axionic Agency I.4 — Version 2.0</strong></p>
<p>Conditional goal interpretation formalized.<br> Boundary result
established (instability of fixed terminal goals).<br> No governance,
authority, or recovery mechanisms included.<br> Prerequisite for Axionic
Agency II.<br></p>

    </article>
</body>
</html>
