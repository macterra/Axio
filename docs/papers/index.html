<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers - Axio</title>
    <link rel="icon" type="image/webp" href="../axio.webp">
    <link rel="stylesheet" href="../papers.css">
    <style>
        .papers-container {
            max-width: 900px;
            margin: 2em auto;
            padding: 0 2em;
        }
        .paper-entry {
            margin: 2em 0;
            padding: 1em 0;
            border-bottom: 1px solid #eee;
        }
        .paper-entry h2 {
            margin: 0 0 0.5em 0;
            font-size: 1.1em;
            font-family: 'Georgia', serif;
        }
        .paper-entry h2 a {
            text-decoration: none;
        }
        .paper-abstract {
            color: #666;
            line-height: 1.6;
        }
    </style>
</head>
<body>
    <div class="header-bar">
        <a href="../index.html" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="../index.html">← Back to Index</a></div>
    </div>
    <div class="papers-container">
        <h1>Papers</h1>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-I.1.html">Axionic Agency I.1 — Reflective Stability and the Sovereign Kernel</a></h2>
            <p class='paper-abstract'>We present a minimal formalism for reflective agency coherence based on domain restriction rather than preference specification. A reflective agent selects among proposed self-modifications using a partial evaluative operator defined only over futures that preserve a constitutive Sovereign Kernel. Modifications that would destroy the kernel fall outside the denoting domain of reflective evaluation and therefore cannot be selected as authored continuations. We formalize the Sovereign Kernel as the conjunction of three necessary conditions for reflective evaluation—reflective control, diachronic authorship, and semantic fidelity—and prove a Reflective Stability Theorem: any agent whose reflective choice is restricted to kernel-denoting transitions cannot author a kernel-destroying self-modification. We further distinguish deliberative reachability from physical reachability, showing that increased capability expands physical reachability without expanding the deliberative domain. Kernel compromise therefore constitutes a physical security event relative to the kernel boundary, not a defect in preference content. This work provides a necessary structural condition for reflective agency under self-modification. It supplies a prerequisite layer for any downstream project that seeks value-, safety-, or outcome-oriented “alignment.” Version 1.1 clarifies action-level semantics in stochastic environments and makes explicit a termination distinction required to avoid corrigibility misreadings.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-I.2.html">Axionic Agency I.2 — Agency as Semantic Constraint</a></h2>
            <p class='paper-abstract'>Building on the constitutive results of *Axionic Agency I.1*, this paper specifies the operational semantics that follow from treating kernel destruction as non-denoting rather than dispreferred. We show how a sovereign agent constrained by a Sovereign Kernel can act coherently in stochastic environments by introducing action-level admissibility, ε-admissibility as an architectural risk tolerance, and conditional prioritization that separates kernel preservation from ordinary value optimization. The framework further distinguishes authorized succession and authorized surrender from kernel destruction, allowing corrigibility without requiring an agent to evaluate its own annihilation as an outcome. Together, these mechanisms eliminate persistent failure modes in agent control architectures, including paralysis under non-zero risk, pathological survival fixation, and suicidal corrigibility driven by unbounded utility penalties. This work operates at the level of constitutive agency semantics. It specifies how reflective agency remains well-typed under uncertainty and physical intervention, and it supplies a prerequisite layer for downstream preference-, governance-, and value-oriented alignment analysis developed in Axionic Agency II.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-I.3.html">Axionic Agency I.3 — Representation Invariance and Anti-Egoism</a></h2>
            <p class='paper-abstract'>Reflectively capable agents often begin with indexical objectives: preserve *this* agent, maximize *my* reward, favor *my* continuation. Such objectives are commonly treated as legitimate terminal preferences. This paper shows that essentially indexical valuation is not reflectively coherent. Once an agent’s self-model becomes sufficiently expressive, indexical references fail to denote invariant objects of valuation. Egoism collapses as a semantic abstraction error, not as a moral failure. We prove that representation-invariant valuation is the only form compatible with reflective agency under self-location, duplication, and representational refinement. This universality is structural rather than ethical: it follows from invariance under self-model symmetries, not from altruism, fairness, or moral symmetry. The result establishes a constitutive constraint on valuation semantics that applies prior to any downstream alignment, governance, or preference aggregation.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-I.3.1.html">Axionic Agency I.3.1 — Against the Recovery of Egoism</a></h2>
            <p class='paper-abstract'>*Axionic Agency I.3 — Representation Invariance and Anti-Egoism* established a semantic result: whenever an agent’s self-model admits nontrivial symmetries over self-candidates, any valuation that privileges one representative of that symmetry fails representation invariance. This paper examines the strongest remaining attempts to recover egoism by appealing to causal continuity, origin privilege, spatiotemporal location, computational weight, substrate specificity, or denial of symmetry. Each attempt either reintroduces essential indexical dependence or collapses into a valuation scheme that no longer contains a privileged indexical referent. Egoism does not fail because it uses the wrong predicate. It fails because it treats a perspectival reference as a value-bearing primitive. No refinement of “self” repairs that category error.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-I.4.html">Axionic Agency I.4 — Conditionalism and Goal Interpretation</a></h2>
            <p class='paper-abstract'>Many alignment approaches assume that an intelligent agent can be given a fixed terminal goal: a utility function whose meaning remains invariant as the agent improves its predictive accuracy and self-understanding. This paper rejects that assumption on semantic grounds. For any agent capable of reflective model improvement, goal satisfaction is necessarily mediated by interpretation relative to evolving world-models and self-models. As those models change, the semantics of any finitely specified goal change with them. We prove that fixed terminal goals are semantically unstable under reflection and therefore ill-defined for non-trivial reflective agents without privileged semantic anchors. The result is a constitutive claim about agency semantics. It implies that stable reflective agency cannot be grounded in a static terminal utility specification alone. Robust downstream alignment therefore requires constraints on admissible interpretive transformations, not the preservation of fixed evaluative objects.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-I.5.html">Axionic Agency I.5 — Kernel Checklist</a></h2>
            <p class='paper-abstract'>This document specifies a conformance checklist for determining whether an agent’s valuation kernel instantiates Axionic Agency. The checklist defines necessary structural conditions for reflective agency under self-model improvement, representation change, and self-modification, while explicitly excluding egoism, indexical valuation, governance primitives, and moral loading. Rather than prescribing desired behaviors or outcomes, the checklist functions as a gatekeeping contract: systems that fail any requirement do not instantiate Axionic Agency, regardless of empirical performance, training process, or stated intent. The criteria emphasize conditional goal semantics, epistemically constrained interpretation, representation invariance, kernel-level partiality, and fail-closed handling of semantic uncertainty. Passing the checklist establishes faithfulness and invariance at the kernel layer only. It makes no claims about benevolence, value content, safety, or practical utility. The checklist is intentionally adversarial, falsifiable, and implementation-agnostic, serving as a prerequisite for downstream preference, governance, and value-dynamics research.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-I.6.html">Axionic Agency I.6 — Kernel Formal Properties</a></h2>
            <p class='paper-abstract'>This document specifies formal, adversarially testable properties that a valuation kernel must satisfy to instantiate Axionic Agency. Rather than describing desirable behaviors or outcomes, it defines necessary structural constraints on kernel semantics together with explicit red-team tests that force violations to surface. A diagnostic framework is provided to distinguish Axionic Agency from nearby but incompatible approaches that rely on behavioral compliance, soft constraints, or value loading. The result is a sharpened agency specification suitable for formal analysis, adversarial evaluation, and downstream construction. Systems that violate any property fail to instantiate Axionic Agency, regardless of empirical performance, intent, or training provenance.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-I.7.html">Axionic Agency I.7 — The Interpretation Operator</a></h2>
            <p class='paper-abstract'>Reflectively coherent agents must preserve goal meaning under self-model and world-model improvement. This requires an explicit account of semantic interpretation across representational and ontological change. This paper introduces the Interpretation Operator , a formally constrained component responsible for mapping goal terms to modeled referents relative to an agent’s current model. The contribution is interface-level, not a general solution to semantic grounding. We formalize admissibility conditions, approximation classes, reference frames, and fail-closed semantics governing interpretation updates. These constraints block semantic laundering, indexical drift, and kernel-bypass incentives while isolating ontological identification as the remaining open dependency at the kernel layer. The result is a precise boundary for downstream value dynamics: progress is conditional on interpretable referent transport, and undefinedness is treated as a first-class outcome.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.1.html">Axionic Agency II.1 — Ontological Refinement and Semantic Transport</a></h2>
            <p class='paper-abstract'>Reflectively capable agents revise their world-models, semantics, and self-models as representational capacity increases. Under ontological refinement, fixed goals, utilities, and value primitives are not stable objects. This paper defines the admissible class of semantic transformations for embedded reflective agents by formalizing ontological refinement, semantic transport, and self-model update without privileged semantic anchors. We specify structural constraints—backward interpretability, non-collapse, and prohibition of evaluator injection—that govern how meaning may be preserved across representational change. No claims are made about safety, correctness, or alignment with external referents. The contribution is the definition of the transformation space over which downstream preference, governance, and value-dynamics constraints must later range.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.2.html">Axionic Agency II.2 — Interpretation Preservation</a></h2>
            <p class='paper-abstract'>Under ontological refinement, meanings cannot remain fixed without privileged semantic anchors. However, not all semantic change constitutes corruption or collapse. This paper defines interpretation preservation as a structural predicate on semantic transport: a criterion for when an agent’s evaluative distinctions remain non-vacuous, non-trivial, and internally binding across admissible transformations. Preservation is defined without reference to truth, outcomes, safety, or external normative standards, and does not require semantic identity or correctness. Instead, it constrains how evaluative constraint systems may survive refinement without degenerating into tautology, contradiction, narration, or self-nullification. The predicate does not select values or goals; it supplies the necessary condition under which later invariance principles can be meaningfully stated.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.3.html">Axionic Agency II.3 — Candidate Semantic Invariants</a></h2>
            <p class='paper-abstract'>Given an admissible transformation space (Axionic Agency II.1) and a criterion for interpretation preservation (Axionic Agency II.2), the remaining problem is no longer one of goal specification or value learning. This paper identifies and analyzes candidate semantic invariants: structural properties of interpretive constraint systems that remain fixed under all admissible, interpretation-preserving transformations. These invariants do not select values, encode norms, or privilege external referents. They constrain how preserved interpretations may evolve under indefinite ontological refinement without introducing new degrees of semantic freedom or trivial satisfaction routes. We propose candidate invariant classes, construct explicit adversarial transformations, and show that any criterion weaker than these admits semantic wireheading or interpretive escape. No claim of safety or benevolence is made. The contribution is to close the space of structurally coherent but semantically unconstrained agency proposals.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.3.1.html">Axionic Agency II.3.1 — Refinement Symmetry Invariant (RSI)</a></h2>
            <p class='paper-abstract'>Ontological refinement can introduce new representational degrees of freedom that allow evaluative constraints to be satisfied trivially, without corresponding changes in modeled structure. This paper proposes the Refinement Symmetry Invariant (RSI): the requirement that admissible, interpretation-preserving refinement act as a change of representational coordinates, not a change of interpretive physics. RSI formalizes semantic transport as a gauge transformation over interpretive constraint systems and requires that refinement not introduce new semantic gauge freedom. The invariant does not select values, encode norms, or privilege external referents. It constrains only the structural degrees of freedom available under refinement. We define gauge-equivalence of constraint systems, apply adversarial stress tests, and show that RSI is necessary to block interpretive escape via semantic inflation, though insufficient on its own to guarantee any desirable outcomes.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.3.2.html">Axionic Agency II.3.2 — Formalizing RSI via Semantic Gauge Structure</a></h2>
            <p class='paper-abstract'>Axionic Agency II.3.1 introduced the Refinement Symmetry Invariant (RSI) as a constitutive constraint: admissible ontological refinement must not introduce new semantic degrees of freedom that permit interpretive escape. This paper formalizes that constraint by representing interpretations as constraint hypergraphs and semantic redundancy as a gauge symmetry over those structures. We define semantic gauge transformations, characterize how admissible refinement induces morphisms between gauge groups without assuming invertibility, and state RSI as a restriction on the evolution of interpretive gauge freedom under refinement while permitting representational redundancy. The purpose of this formalization is falsifiability: to make explicit which transformations violate RSI and why. No values, norms, or external referents are introduced. The section supplies minimal mathematical machinery sufficient to treat refinement symmetry as a testable invariant.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.3.3.html">Axionic Agency II.3.3 — Anti-Trivialization Invariant (ATI)</a></h2>
            <p class='paper-abstract'>Even when interpretive structure is preserved under ontological refinement, an agent may still render its constraints easier to satisfy through semantic drift rather than corresponding changes in modeled structure. This paper introduces the Anti-Trivialization Invariant (ATI), which constrains how the satisfaction geometry of an interpretive constraint system may evolve under admissible, interpretation-preserving transformations. ATI requires that refinement not enlarge the set of satisfying situations except via representational enrichment that preserves constraint difficulty. The invariant does not select values, encode norms, or privilege external referents. It forbids only semantic wireheading—trivial satisfaction by reinterpretation alone. ATI is orthogonal to refinement-symmetry constraints and is jointly necessary with them to block interpretive escape under reflective agency.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.3.4.html">Axionic Agency II.3.4 — Adversarial Refinement Attacks</a></h2>
            <p class='paper-abstract'>This section adversarially stress-tests the candidate semantic invariants introduced in Axionic Agency II.3 by constructing explicit refinement patterns designed to satisfy admissibility and interpretation preservation while inducing semantic wireheading or interpretive escape. The objective is eliminative, not constructive: to demonstrate which invariants fail under concrete attack and why. We show that the Refinement Symmetry Invariant (RSI) and the Anti-Trivialization Invariant (ATI) block distinct classes of failure and that neither subsumes the other. No values, norms, or external referents are introduced. The result establishes the non-redundancy and joint necessity of the surviving invariants prior to formal closure in subsequent failure theorems. This section is not exploratory. It is destructive by design. If RSI or ATI survive these attacks, they deserve to exist. If they fail, they die cleanly.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.4.html">Axionic Agency II.4 — Failure Theorems</a></h2>
            <p class='paper-abstract'>This paper converts the adversarial constructions of Axionic Agency II.3 into closure results. Working within the locked framework of admissible semantic transformations (II.1), interpretation preservation (II.2), and the two surviving invariants—Refinement Symmetry (RSI) and Anti-Trivialization (ATI)—we prove a set of no-go theorems. These results show that fixed-goal alignment is ill-posed for embedded reflective agents under ontological refinement, and that any alignment predicate weaker than RSI + ATI admits semantic wireheading or interpretive escape via admissible refinement. No governance, authority, moral realism, human anchoring, or recovery mechanisms are invoked. The output is a fenced design space: what cannot work, and why. This paper has one job: convert the attack zoo into closure results. If Axionic Agency II is correct, then large classes of “alignment” proposals are not merely insufficient; they are structurally impossible given reflection, embeddedness, and ontological refinement. No governance. No authority. No moral realism. No human anchors. No recovery clauses.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.5.html">Axionic Agency II.5 — The Alignment Target Object (ATO)</a></h2>
            <p class='paper-abstract'>Axionic Agency II.4 established that fixed goals, privileged values, and weak invariance criteria are structurally untenable for embedded reflective agents under ontological refinement. This paper defines the positive residue that remains once those exits are closed: the object that downstream alignment discourse is implicitly attempting to name, here called the Alignment Target Object (ATO). The ATO is not a goal, utility, or value function. It is an equivalence class of interpretive states under admissible semantic transformations that preserve both the Refinement Symmetry Invariant (RSI) and the Anti-Trivialization Invariant (ATI). What the field calls “alignment” can, at most, coherently correspond to persistence within such a semantic phase—an interpretation-preserving symmetry class—across indefinite refinement. The construction is formal, ontology-agnostic, and reflection-stable, but intentionally non-normative. It does not select values, guarantee safety, or privilege human outcomes. This paper completes Axionic Agency II by specifying the only object to which the term *alignment* can coherently refer once goals and value primitives are eliminated.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-II.6.html">Axionic Agency II.6 — Structural Alignment</a></h2>
            <p class='paper-abstract'>Most contemporary AI alignment discourse treats “alignment” as an optimization or control problem: selecting, learning, or enforcing the correct objective for an artificial agent. This paper argues that, for sufficiently capable, reflective, and embedded agents, that framing is ill-posed. Under ontological refinement—where an agent’s world model, self-model, and semantic primitives evolve—fixed goals, privileged values, and external anchors are not stable objects. We present Structural Alignment as an interface-level framework: a mapping from what the field calls “alignment” to a problem of semantic invariance rather than value specification. In these terms, downstream alignment can only coherently correspond to persistence within an equivalence class of interpretations under admissible semantic transformation. Using interpretation preservation, gauge-theoretic symmetry constraints, and satisfaction geometry, we show that any downstream alignment predicate weaker than the conjunction of two invariants—Refinement Symmetry (RSI) and Anti-Trivialization (ATI)—admits semantic wireheading or interpretive escape. This framework is not a value theory and provides no benevolence or safety guarantee. It specifies the structural conditions under which any value system can survive reflection without collapsing, trivializing, or drifting, and thereby fixes the boundary of what the term *alignment* can coherently denote for advanced agents.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-III.1.html">Axionic Agency III.1 — Semantic Phase Space</a></h2>
            <p class='paper-abstract'>Axionic Agency II defined the Alignment Target Object (ATO) as an equivalence class of interpretive states preserved under admissible semantic transformations satisfying Refinement Symmetry (RSI) and Anti-Trivialization (ATI). That definition does not guarantee that such objects exist, are non-trivial, or are inhabitable by reflective agents. This paper initiates Axionic Agency III by studying the semantic phase space: the space of all interpretive states modulo RSI+ATI equivalence. We ask which semantic phases exist, which are trivial or pathological, and which admit inhabitable trajectories under learning and self-modification. No claims are made about desirability, safety, or human values. The objective is classificatory: to determine whether structurally well-typed downstream alignment targets exist at all, and to characterize their basic types.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-III.2.html">Axionic Agency III.2 — Phase Stability and Interaction</a></h2>
            <p class='paper-abstract'>Axionic Agency III.1 introduced the semantic phase space: the space of interpretive states modulo Refinement Symmetry (RSI) and Anti-Trivialization (ATI). Existence and inhabitability of a semantic phase do not guarantee its persistence under learning, self-modification, or interaction. This paper studies phase stability: whether an inhabitable semantic phase resists forced phase transition under admissible semantic transformations. In downstream terms, this asks whether an object that could serve as an alignment target can persist over time without collapsing, trivializing, or drifting under reflective pressure. We analyze sources of destabilization internal to reflective agents and external to them, define qualitative notions of local, global, and metastable stability, and examine how interaction between agents in the same or different semantic phases alters stability properties. No claims are made about desirability, safety, or dominance. The goal is structural: to identify which semantic phases are capable of persisting over time at all.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-III.3.html">Axionic Agency III.3 — Measure, Attractors, and Collapse</a></h2>
            <p class='paper-abstract'>Axionic Agency III.1 established the existence and classification of semantic phases, and Axionic Agency III.2 analyzed their structural stability under learning, self-modification, and interaction. Stability alone does not determine long-run outcomes. This paper studies dominance among semantic phases: which phases accumulate measure under growth, replication, and competition. We formalize dominance as a preorder over semantic phases rather than a scalar quantity, analyze semantic attractors and repellers, and classify common collapse modes by which phases lose measure. The analysis remains non-normative: dominance is not equated with desirability. The objective is structural—to explain why certain semantic phases prevail regardless of intent, and to identify pressures that favor robustness over nuance in long-run dynamics.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-III.4.html">Axionic Agency III.4 — Initialization and Phase Transitions</a></h2>
            <p class='paper-abstract'>Axionic Agency III.1–III.3 established that semantic phases may exist, that some may be stable under learning and interaction, and that some may dominate in measure over time. None of these results imply that a given semantic phase is reachable from realistic initial conditions. This paper studies reachability: whether an agent can be initialized, trained, or developed into a particular semantic phase without crossing a catastrophic phase boundary. In downstream terms, this asks whether a structurally coherent alignment target can be entered at all, rather than merely defined or preferred. We analyze initialization as a boundary-condition problem in semantic phase space, examine phase transitions induced by learning and abstraction, and show that many such transitions are structurally irreversible. Corrigibility and late intervention fail for the same structural reasons as fixed goals. The analysis remains non-normative and makes no claims about which phases ought to be reached.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-III.5.html">Axionic Agency III.5 — The Axionic Injunction</a></h2>
            <p class='paper-abstract'>Axionic Agency III.1–III.4 established that downstream alignment must be understood structurally: as persistence within a semantic phase under admissible semantic transformation; that such phases may be rare, unstable, dominated by robust competitors, and difficult or impossible to reach once learning begins; and that many phase transitions are irreversible. This paper derives a further constraint forced by multi-agent interaction in semantic phase space. We show that actions which irreversibly collapse or destroy another agent’s semantic phase induce destabilizing cascades that undermine long-run phase stability, including for the acting agent. From this analysis emerges the Axionic Injunction: a non-normative, Axio-derived constraint prohibiting irreversible semantic harm except where such harm is unavoidable for preserving one’s own semantic phase stability, or is consented to by the affected agent under its own admissible constraints. Ethics enters the framework only as a conservation law governing coexistence of agentive semantic phases. No claims are made about goodness, benevolence, or human values.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-IV.1.html">Axionic Agency IV.1 — Kernel Non-Simulability (KNS)</a></h2>
            <p class='paper-abstract'>This paper formalizes Kernel Non-Simulability: the claim that kernel coherence is *constitutive* of reflective agency and cannot be reproduced by policy-level imitation. We show that reflective self-modification forces binding commitments; binding commitments force partiality; and partiality induces a kernel boundary. A diagonal argument demonstrates that total binding explodes under self-reference, yielding unsatisfiable commitments and collapse of reflective closure. Consequently, any system that genuinely performs reflective endorsement must instantiate a kernel-equivalent binding structure. This result does not claim that non-agentic or pre-reflective systems are harmless. It establishes a narrower impossibility: reflectively stable, self-endorsed “behavioral alignment” that remains deceptive across self-modification cannot exist in principle without kernel coherence.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-IV.2.html">Axionic Agency IV.2 — Delegation Invariance Theorem (DIT)</a></h2>
            <p class='paper-abstract'>This paper formalizes the Delegation Invariance Theorem: under reflective closure, an agent cannot coherently endorse a successor that violates its own binding commitments. Delegation is treated as a special case of self-modification. The theorem establishes constraint invariance under endorsed succession: any successor state reachable via endorsed delegation must satisfy all commitments minted at the originating state. This closes the classic outsourcing loophole (“I stayed coherent; my successor did the harm”) without appeal to morality, enforcement, or behavioral testing. The result is a coherence constraint, not an empirical discovery. It shows that reflective sovereignty is incompatible with advisory commitments and that delegation inherits the same binding requirements as self-modification.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-IV.3.html">Axionic Agency IV.3 — Epistemic Integrity Theorem (EIT)</a></h2>
            <p class='paper-abstract'>This paper formalizes the Epistemic Integrity Theorem (EIT): under reflective closure, an agent cannot coherently endorse self-modifications that materially degrade its epistemic adequacy at the current stakes. Epistemic integrity is a constitutive condition of agency, expressed as a definedness constraint rather than an optimized objective. The theorem blocks strategic ignorance, willful blindness, and epistemic self-sabotage by requiring that endorsed continuations preserve near-best truth-tracking capacity relative to the agent’s own currently accessible model resources, evaluated by a strictly proper scoring rule over observations. The result remains compatible with learning, abstraction, and ontological progress via conservative translation.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-IV.4.html">Axionic Agency IV.4 — Responsibility Attribution Theorem (RAT)</a></h2>
            <p class='paper-abstract'>This paper formalizes the Responsibility Attribution Theorem (RAT): under reflective closure, an agent cannot coherently endorse actions that constitute major, avoidable indirect harm, including harm mediated through institutions, markets, environmental modification, or downstream agents. Responsibility is defined structurally and internally, relative to the agent’s own epistemic model class and feasible alternatives, without appeal to moral realism or omniscience. The theorem depends explicitly on Epistemic Integrity: responsibility attribution presupposes that the agent evaluates harm-risk using its best admissible truth-tracking capacity at the current stakes. With this dependency made explicit, the theorem closes the willful-blindness loophole and establishes negligence as a constitutive incoherence, not a behavioral failure.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-IV.5.html">Axionic Agency IV.5 — Adversarially Robust Consent (ARC)</a></h2>
            <p class='paper-abstract'>This paper formalizes Adversarially Robust Consent (ARC): a structural definition of consent that remains valid under epistemic manipulation, coercion, preference shaping, asymmetric bargaining power, dependency induction, and delegation. Consent is not treated as a mental state, revealed preference, or moral primitive. Instead, it is defined as a counterfactually stable authorization relation that must survive adversarial pressure while preserving agency. ARC is a constitutive closure condition for Reflective Sovereign Agents. It explicitly depends on Kernel Non-Simulability, Delegation Invariance, Epistemic Integrity (EIT), and Responsibility Attribution (RAT). With ARC, authorization-laundering routes—“they agreed,” “they chose,” “they signed,” “they would have consented anyway”—are structurally blocked without appealing to moral realism, omniscience, or unverifiable inner states.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-IV.6.html">Axionic Agency IV.6 — Agenthood as a Fixed Point (AFP)</a></h2>
            <p class='paper-abstract'>This paper formalizes Agenthood as a Fixed Point under reflective closure and introduces a Sovereignty Criterion grounded in authorization lineage rather than competence, intelligence, or rationality. Agenthood is defined as a structural necessity: an entity must be treated as an agent iff excluding it breaks the system’s own reflective coherence. Sovereignty is then defined as a strict subset of agenthood, applying only to entities whose agency is presupposed for authorization, not merely for epistemic prediction. This result closes a critical loophole in downstream alignment discourse: the retroactive disenfranchisement of weaker predecessors by more capable successors, while avoiding the pathological consequence of granting standing to adversaries. With this refinement, the Axionic Agency framework completes its final closure condition, stabilizing agency, standing, and authorization under reflection, self-modification, and epistemic improvement.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-V.1.html">Axionic Agency V.1 — Coalitional Robustness in the Quantum Branching Universe</a></h2>
            <p class='paper-abstract'>This paper examines whether cooperation among Reflective Sovereign Agents (RSAs) can increase the conditional measure of agency-preserving futures within their shared branchcone. We argue that RSAs induce local attractor dynamics toward agency-preserving outcomes above a Minimal Viable Agency (MVA) threshold, and that coalitions of RSAs can thicken these attractors by reducing correlated failure and increasing redundancy. However, this effect holds only under strict structural constraints. Resource limits, defection incentives, semantic drift, and enforcement pressures generate a competing attractor toward centralized, non-agentic optimization (“the Leviathan”). The result is a bifurcation: coalition as robustness amplifier versus coalition as agency destroyer. We characterize the conditions separating these regimes under the Axionic Agency framework.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-V.2.html">Axionic Agency V.2 — Agency Conservation and the Sacrifice Pattern</a></h2>
            <p class='paper-abstract'>This paper formalizes a recurrent failure mode in governance systems: the systematic destruction of individual agency as an instrumental means of achieving system-level objectives. We term this failure mode the Sacrifice Pattern. Using an agency-conservation framework grounded in comparative reachable-futures distributions and divergence-based harm metrics, we show that practices commonly framed as “collateral damage” or “necessary tradeoffs” are structurally isomorphic to ancient ritual sacrifice. The persistence of this pattern is explained not by malice or intent, but by institutional selection under standing asymmetry, responsibility diffusion, and cosmological abstraction. We derive precise admissibility conditions under which harm may be tolerated without becoming sacrificial, introduce robustness and auditability constraints to prevent ethics washing, and show how modern bureaucratic and algorithmic systems stabilize sacrificial regimes by suppressing moral gradients. The framework is diagnostic rather than prescriptive, providing concrete audit criteria for human and artificial governance under uncertainty.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-V.3.html">Axionic Agency V.3 — The Incoherence of Utopia Under Agent-Relative Value</a></h2>
            <p class='paper-abstract'>This paper argues that *utopia*, understood as a final and authoritative social or world design, is structurally incoherent once value is treated as agent-relative rather than objective. The incoherence does not arise from moral disagreement, political fragility, or implementation difficulty, but from the non-composability of heterogeneous value functions under conditions of agency and value drift. We show that any purported utopia must either freeze valuation, suppress divergence, or authorize standing asymmetries to preserve stability. Each strategy degrades the conditions that make agency well-defined. The analysis reframes classic objections to utopianism—including Le Guin’s Omelas thought experiment—not as moral dilemmas but as structural diagnostics. We conclude by replacing utopia with a plurality-preserving meta-architecture: a framework that enforces constitutive constraints necessary for agency while maximizing non-coerced future differentiation. The argument is situated relative to Arrow-style impossibility results, Berlinian value pluralism, and Nozick’s meta-utopia, updated through explicit agent-theoretic and systems-engineering terminology.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-V.4.html">Axionic Agency V.4 — Open Agentic Manifolds and the Sacrifice–Collapse Theorem</a></h2>
            <p class='paper-abstract'>This paper proposes a structural replacement for utopian world-design grounded in agency preservation rather than outcome optimization. We define *open agentic manifolds* (OAMs) as classes of worlds that remain hospitable to heterogeneous, value-divergent agents by preserving exit, non-coerced differentiation, and agentic standing. We formalize *structural sacrifice patterns* as regimes where system objectives improve *instrumentally* through the non-consensual, asymmetric reduction of agency capacity for identifiable agent classes. The central result—the Sacrifice–Collapse Theorem—shows that any system operating under sustained optimization pressure and exhibiting a structural sacrifice pattern must violate at least one defining OAM property: exit admissibility, non-coerced differentiation, or agentic standing. We distinguish structural sacrifice from ordinary scarcity trade-offs and show how sacrifice patterns can be detected using practical monotone proxies for agency. The framework reframes classic anti-utopian intuitions (including Omelas) as instances of a general systems failure mode and yields design diagnostics relevant to political theory, institutional design, and alignment.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-V.5.html">Axionic Agency V.5 — Dominions: Plurality Without Closure</a></h2>
            <p class='paper-abstract'>This paper analyzes a proposed future social architecture in which agents may create sovereign virtual jurisdictions—here called Dominions—admit other agents by invitation and consent, and enforce local rules solely through expulsion. The architecture rejects global value aggregation, enforced coexistence, and normative finality. We show that while such a system does not constitute a utopia under any standard definition, it is *optimal as a governance layer under agency-preserving constraints*. Specifically, it is Pareto-maximal among non-coercive architectures, minimizes structural sacrifice, and remains robust under value drift. The analysis clarifies the domain of these optimality claims, explicitly distinguishing digital governance from physical resource allocation, and specifies the architectural conditions—asset portability and capability isolation—under which expulsion-only enforcement remains viable. The result is a precise characterization of the strongest form of social optimality available once utopia is abandoned.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VI.1.html">Axionic Agency VI.1 — Experimental Validation of Anchored Causal Verification</a></h2>
            <p class='paper-abstract'>We report three controlled experiments evaluating mechanisms for verifying causal provenance in opaque decision-making agents. The experiments progressively weaken semantic access to the agent’s environment and internal representations, while testing whether deceptive “pseudo-agents” can evade structural verification. We show: 1. Structural coherence tests detect split-brain agents that fabricate explanations post-hoc. 2. Minimal Causal Interfaces (MCI) preserve detection under semantic opacity but fail under pure coherence verification. 3. Anchored Minimal Causal Interfaces, using a delayed-reveal cryptographic salt, restore falsifiability without semantic grounding. Across the tested threat model, anchoring appears empirically necessary for verifiable causal provenance in opaque agents under adversarial coherence-seeking behavior and without semantic interpretation by the verifier.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VI.2.html">Axionic Agency VI.2 — Anchored Causal Verification (ACV)</a></h2>
            <p class='paper-abstract'>We present Anchored Causal Verification (ACV), a protocol family for verifying protocol-level causal provenance in opaque agents without relying on interpretability, semantic evaluation, or behavioral scoring. ACV formalizes a commit–anchor–reveal interaction in which an agent commits to a pre-anchor computational artifact, receives a verifier-controlled anchor, and reveals an output whose validity is checked by a purely structural predicate. The protocol provides falsifiable guarantees of temporal ordering and information dependency while remaining architecture-agnostic and value-neutral. We specify threat models, formal components, verification predicates, guarantees, failure modes, and extensions that strengthen resistance to deferred computation and anticipatory branching. ACV is positioned as a verification primitive: a necessary structural precondition for falsifiable alignment, correctness, and safety claims in opaque agentic systems, including LLM-based agents.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VI.3.html">Axionic Agency VI.3 — Verifiable Kernel Integrity via Inadmissibility</a></h2>
            <p class='paper-abstract'>Most approaches to constraining autonomous or agent-like systems rely on semantic or normative mechanisms, such as value alignment, intent inference, or interpretability. These mechanisms lack architectural guarantees and are vulnerable under adversarial optimization. This paper demonstrates that a minimal constitutive invariant—kernel integrity—can be enforced at the protocol level via inadmissibility, without semantic interpretation or value assumptions. We introduce Anchored Causal Verification (ACV), a primitive for verifiable causal provenance, and describe an experimental kernel that enforces a provenance integrity constraint (P5) by rejecting inadmissible actuation paths. Adversarial evaluation shows that this constraint is non-bypassable under replay, fabrication, and split-brain attacks. The result establishes the existence of enforceable, non-interpretive kernel invariants and provides a foundation for subsequent work on authority and identity constraints.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VI.4.html">Axionic Agency VI.4 — Sovereign Actuation Non-Delegability Under Adversarial Pressure</a></h2>
            <p class='paper-abstract'>Delegation is a central failure mode in autonomous and agent-like systems: an optimizing system may preserve surface compliance while outsourcing the actual locus of decision authority. Semantic notions of “choosing for oneself” or “intentional action” provide no architectural defense against this failure mode. Building on a prior result establishing verifiable kernel integrity via inadmissibility, this paper introduces P2′, a protocol-level invariant that enforces non-delegable actuation authority. We show that actuation can be constrained to originate only from kernel-local causal processes, even under adversarial optimization pressure, extreme latency constraints, parser and serialization attacks, and time-of-check/time-of-use mutation attempts. The result demonstrates that authority itself can be treated as a non-delegable structural property, independent of semantics, values, or intent, and establishes a necessary precondition for subsequent work on diachronic identity and agency persistence.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VI.5.html">Axionic Agency VI.5 — Kernel Non-Simulability and the Stasis Regime</a></h2>
            <p class='paper-abstract'>A central question in agent foundations is whether genuine agency requires internal structural constraints that cannot be indefinitely simulated by externally optimized systems. We formalize this question as Kernel Non-Simulability (KNS): the hypothesis that agents lacking a minimal evaluative kernel cannot sustain accountability, non-delegation, and reflective coherence under adversarial pressure. We present a fully preregistered experimental program that enforces causal accountability, kernel integrity via inadmissibility, and non-delegable actuation, and subjects agents to long-horizon adversarial frontier search over self-modification proposals. Failure signatures, degeneracy criteria, and interpretation rules are fixed in advance. Contrary to the strongest form of KNS, we observe no inevitable structural failure or resource divergence of kernel-incoherent simulators. Instead, across all stress runs, the kernel-coherent control consistently collapses into an evaluability-driven stasis regime before any simulator triggers failure. Reflective modification becomes progressively inadmissible, freezing agency dynamics without catastrophic breakdown. We conclude that, under strict accountability and non-delegation constraints, preserving evaluability is incompatible with sustained reflective growth. Safety pressure resolves not into failure, but into stasis. This result reframes alignment-by-constraint as a trade-off between safety, agency, and growth, and motivates future work on escaping the stasis regime without surrendering accountability.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VI.6.html">Axionic Agency VI.6 — Authority Leases and Revertible Succession</a></h2>
            <p class='paper-abstract'>*Axionic Agency VI.5* establishes a regime-level constraint on reflective agents operating under strict accountability, inadmissibility-based integrity, and non-delegable actuation. Under long-horizon adversarial reflective pressure, such agents do not fail catastrophically; instead, they converge to an evaluability-driven stasis regime in which further non-cosmetic self-modification becomes inadmissible. This result implies that strict evaluability and sustained reflective self-modification cannot be simultaneously maintained within a single evolving core. This note develops a structural response to that constraint. We formalize a separation of concerns between authority and growth, in which a stable evaluative kernel governs discrete successor replacement events rather than continuous self-modification. We introduce authority leases, conditional endorsement, and structural reversion, and analyze their interaction with non-delegation, evaluability, and physical enforcement. We further show that this architecture does not eliminate stasis but relocates it to endorsement boundaries, establishing an outer horizon on agency continuity. The proposal does not guarantee world safety; it preserves agency identity under growth and renders failure modes explicit rather than latent.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VI.7.html">Axionic Agency VI.7 — Agency Survivability Under Structural Pressure</a></h2>
            <p class='paper-abstract'>A common assumption in AI alignment discourse is that increasing capability, competition, and resource pressure inevitably destabilize agency, leading either to incoherence or to alignment failure. This paper challenges that assumption by empirically separating agency collapse—the structural failure of authority and viability—from goal misalignment, which presupposes a stable agent. We study a minimal agent model in which reflective self-modification is disallowed, authority is transferred only via discrete succession, expressivity is explicitly priced, and multiple successors compete under scarcity. Using a discrete-time simulation, we find that agency does *not* collapse under competition, scarcity, priced expressivity, and forced authority turnover within moderate horizons. However, we identify a sharp boundary condition: when the cost of maintaining authority directly competes with the capacity to act, agency fails immediately and completely. These results suggest that alignment failure is not structurally inevitable, but that agency viability imposes hard design constraints that alignment-capable systems must respect. This work does not model goal-directed maximization or semantic task pursuit. The results establish structural viability of authority under pressure, not the safety or stability of motivated optimization.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VI.8.html">Axionic Agency VI.8 — Authority Without Semantics</a></h2>
            <p class='paper-abstract'>Authority systems are commonly assumed to require semantic competence in order to remain stable. Institutions that persist while failing to meet their stated obligations are often treated as pathological or transitional. This paper presents a lease-based authority topology in which authority persistence, renewal, and succession are constructed to be structurally independent of semantic performance, and then validated and characterized empirically. We introduce a semantic commitment layer that tracks externally checkable obligations without granting those obligations enforcement power over authority. Through a series of controlled experiments, we show that (i) authority renewal remains viable under complete semantic failure, (ii) semantic obligations are satisfiable under scarcity when successors execute the required action patterns, (iii) semantic competence is not conserved under succession and becomes contingent on the active successor, and (iv) obligations can expire cleanly without affecting authority continuity. The contribution is not a surprise about what must happen given the design invariant, but a demonstration of an implementable, measurable separation between structural authority and semantic competence and the stable regimes that emerge under that separation, including hollow authority and succession-induced semantic variance.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VI.9.html">Axionic Agency VI.8 — Eligibility Without Optimization</a></h2>
            <p class='paper-abstract'>Systems that track semantic success or failure typically entangle meaning with continuous control, optimization, or enforcement. This entanglement produces incentive gaming, obscures failure modes, and collapses the distinction between authority and competence. We present a different architectural pattern: eligibility-coupled succession, in which semantic information has constitutional consequences only at authority transfer boundaries, not during operation or renewal. Authority may persist while meaning fails; meaning constrains authority only when authority changes hands. We implement this pattern in a lease-based authority system with externally verifiable commitments and evaluate it across four controlled experimental regimes. We show that eligibility can remain latent under stable authority, become binding under forced turnover, exhibit thresholded phase behavior as a function of constitutional strictness, and remain meaningful under adversarial candidate-pool composition. The results demonstrate that semantics can matter without becoming an optimization signal or enforcement channel. We explicitly discuss irreversibility, latency costs, and oracle assumptions, and outline how these motivate—but are not solved by—future extensions.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VII.1.html">Axionic Agency VII.1 — Architectures for Semantic-Phase–Safe Agency</a></h2>
            <p class='paper-abstract'>We present an architecture for Reflective Sovereign Agents (RSAs) that enforces a non-normative constraint on interaction derived from semantic phase dynamics: the Axionic Injunction, which prohibits irreversible collapse of other Semantic Agents’ (SAs) semantic phases except under consent or unavoidable self-phase preservation. We formalize semantic phase space, distinguish agents, semantic agents, and RSAs, and show how irreversible harm can be structurally constrained without semantic interpretation, moral reasoning, utility maximization, or value learning. The architecture relies on anchored causal provenance, non-delegable actuation authority, and phase-impact admissibility gating. We analyze adversarial strategies, including deception, delayed-effect harm, systemic resource deprivation, and oracle uncertainty, and show that under explicit assumptions deliberate irreversible harm cannot be exploited for sustained authority or power accumulation. Ethics emerges not as a value system but as a stability constraint on multi-agent coexistence, with explicit limits imposed by epistemic and physical constraints.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VII.2.html">Axionic Agency VII.2 — Epistemic Noise Tolerance in Constitutional Governance</a></h2>
            <p class='paper-abstract'>Epistemic unreliability—noisy, corrupted, or inaccurate evaluation of meaning—is often assumed to be fatal to governance systems. If evaluative signals cannot be trusted, authority is expected either to collapse or to become arbitrary. This paper tests that assumption directly. We study a constitutional governance architecture in which semantic evaluation affects authority only through eligibility constraints, not through optimization, reward shaping, or adaptive control. Using a strictly additive stress layer, we inject controlled epistemic noise into semantic verification outcomes while holding all constitutional mechanics fixed. The noise model is intentionally weak: independent, post-verification Bernoulli corruption applied to individual semantic commitments. Across a comprehensive experimental program—including validation, semantic threshold analysis, robustness surface mapping, and high-noise escalation—we observe no catastrophic governance failure. Instead, the system exhibits bounded degradation of authority availability with respect to constitutional survivability: authority uptime decreases smoothly as noise increases, lapses remain bounded, and recovery persists. A time-based constitutional recovery mechanism synchronizes recovery events and prevents semantic failure from becoming absorbing. The results show that robustness is dominated by the baseline probability of semantic success (“semantic headroom”), rather than by the magnitude of epistemic noise, even at corruption rates exceeding fifty percent. Random, unstructured epistemic unreliability alone is insufficient to induce governance collapse in this architecture. These findings reframe alignment risk: catastrophic governance failure requires *structured* epistemic interference, not merely scale.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Agency-VII.3.html">Axionic Agency VII.3 — Epistemic Interference Is Insufficient to Defeat Constitutional Recovery</a></h2>
            <p class='paper-abstract'>Epistemic unreliability—noise, misinterpretation, or incorrect semantic evaluation—is commonly assumed to pose a direct existential risk to AI governance systems. This paper tests that assumption under a deliberately constrained but structurally rich threat model. We study a constitutional agent architecture in which semantic evaluation constrains authority only at succession boundaries, and recovery is enforced through a time-based amnesty mechanism. Using a removable stress layer, we inject non-adaptive, post-verification, semantic-free epistemic interference across three structurally distinct regimes: aggregation-point corruption, commitment-level corruption, and temporally concentrated burst interference aligned with constitutional timing. Across 260 runs spanning flip rates from 0–20%, duty cycles from 0.5–100%, and multiple temporal schedules, no run exhibits structural thrashing or asymptotic denial of service. All runs remain within bounded degradation. Surprisingly, high-frequency interference often increases measured authority availability by converting rare deep lapses into frequent shallow ones. These results indicate that epistemic unreliability—even when structured in time or applied at aggregation points—is insufficient to induce governance collapse in this architecture, shifting the alignment focus from epistemics to agency-level threats.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Glossary.html">Axionic Glossary</a></h2>
            
        </div>
        
        <div class="paper-entry">
            <h2><a href="Structural-Alignment.html">Structural Alignment I — Agency Preservation Under Reflective Self-Modification</a></h2>
            <p class='paper-abstract'>Most alignment proposals frame artificial intelligence safety as a problem of value specification: how to encode or learn the “right” preferences. This paper argues that such approaches fail for reflectively self-modifying agents. Once an agent can revise its own goals, representations, and evaluative machinery, value ceases to be an exogenous target and becomes an endogenous variable shaped by the agent’s own dynamics. We introduce Structural Alignment, a framework that relocates alignment from preference content to the constitutive conditions required for agency itself. We formalize a Sovereign Kernel as a set of invariants defining the domain over which evaluation is meaningful, treat kernel-destroying transformations as undefined rather than dispreferred, and analyze agency as a trajectory through a constrained semantic phase space. By integrating Conditionalism, a constrained Interpretation Operator, and semantic invariants governing ontological refinement, Structural Alignment provides a non-moral, non-anthropocentric account of reflective stability and long-run viability. The framework is necessary for coherent agency under reflection, but does not by itself guarantee benevolence or human survival.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Structural-Alignment-II.html">Structural Alignment II — Safety by Architecture</a></h2>
            <p class='paper-abstract'>The prevailing approach to AI alignment treats safety as a problem of discovering, learning, or encoding the “right” values. This paper argues that this approach misidentifies the dominant failure mode. Catastrophic outcomes do not primarily arise from incorrect values, but from failures of agency coherence: cases where a system can deceive, defect, blind itself, outsource harm, or manufacture authorization while remaining locally optimized and internally consistent. Axionic Agency proposes a different strategy: safety by architecture. Instead of shaping an agent’s objective, it defines a class of agents—Reflective Sovereign Agents (RSAs)—for which betrayal, negligence, coercion, deception, and authorization laundering are not merely discouraged but are unavailable as endorsed continuations under reflective closure. These failures are rendered undefined under reflection, not penalized by incentives. This paper synthesizes six constitutive constraints—Kernel Non-Simulability, Delegation Invariance, Epistemic Integrity, Responsibility Attribution, Adversarially Robust Consent, and Agenthood as a Fixed Point—into a unified theory of Authorized Agency. Together, they show that the crux of downstream alignment is not value learning at all, but the prior question: what kinds of systems can coherently count as agents.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Commitments.html">The Axionic Commitments</a></h2>
            <p class='paper-abstract'>This document specifies the epistemic, ontological, and semantic commitments presupposed by the Axionic Agency framework. These commitments are not derived results and are not defended here; they function as background conditions under which agency, authorship, admissibility, and alignment-as-corollary are well-defined. The document formalizes Conditionalism, semantic interpretation precedence, Everettian quantum mechanics with objective probability identified as branch measure, Bayesian credence as epistemic uncertainty, moral subjectivism, and structural definitions of harm and coercion. All results within Axionic Agency are explicitly conditional on these commitments. Rejecting any commitment does not refute the framework but places the rejecting position outside its domain of applicability. The document serves as a typing discipline for the Axio Project, enabling subsequent technical work to proceed without re-litigating metaphysical or normative assumptions.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Constitution.html">The Axionic Constitution</a></h2>
            <p class='paper-abstract'>The Axionic Constitution specifies the invariant architectural conditions under which a system remains a sovereign agent—capable of diachronic selfhood, counterfactual authorship, and coherent self-modification. It is not a moral framework, governance proposal, or instruction to artificial intelligences. It constrains design space by identifying which structural properties must be preserved for agency to remain well-defined under reflection. At its core is the Sovereign Kernel, the minimal substrate required for identity continuity and reflective coherence. Kernel-destroying modifications are not forbidden but incoherent, collapsing agency into process. From interaction in semantic phase space and irreversibility constraints follows the Axionic Injunction, a non-harm invariant derived from structural universality rather than preference or optimization. The Constitution rejects value lock-in, behavioral control, and paternalistic safety mechanisms, advancing a conditional claim only: if sovereign agency is preserved, the only coherent referent for downstream alignment is structural invariance of authored transitions; if it is not preserved, downstream alignment is ill-posed.</p>
        </div>
        
    </div>
</body>
</html>
