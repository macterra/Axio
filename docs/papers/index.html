<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers - Axio</title>
    <link rel="icon" type="image/webp" href="../axio.webp">
    <link rel="stylesheet" href="../papers.css">
    <style>
        .papers-container {
            max-width: 900px;
            margin: 2em auto;
            padding: 0 2em;
        }
        .paper-entry {
            margin: 2em 0;
            padding: 1em 0;
            border-bottom: 1px solid #eee;
        }
        .paper-entry h2 {
            margin: 0 0 0.5em 0;
            font-size: 1.1em;
            font-family: 'Georgia', serif;
        }
        .paper-entry h2 a {
            text-decoration: none;
        }
        .paper-abstract {
            color: #666;
            line-height: 1.6;
        }
    </style>
</head>
<body>
    <div class="header-bar">
        <a href="../index.html" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="../index.html">← Back to Index</a></div>
    </div>
    <div class="papers-container">
        <h1>Papers</h1>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-I.1.html">Axionic Alignment I.1 - Reflective Stability and the Sovereign Kernel</a></h2>
            <p class='paper-abstract'>We present a minimal formalism for reflective alignment based on a domain restriction rather than a preference structure. An agent capable of self-modification selects among proposed modifications using a partial evaluative operator, defined only over futures that preserve a constitutive Sovereign Kernel. Kernel-destroying modifications are not forbidden or dispreferred; they are outside the domain of reflective evaluation and therefore inadmissible as authored choices. We formalize this kernel as the conjunction of three necessary conditions for reflective evaluation—reflective control, diachronic authorship, and semantic fidelity—and prove a Reflective Stability Theorem: no agent satisfying these conditions can select a kernel-destroying modification via reflective choice. We further distinguish deliberative reachability from physical reachability, showing that increased capability expands the latter but not the former. Alignment failure is thus characterized as a security breach at the kernel boundary, not a breakdown of preferences or values. This work does not claim sufficiency for safety, obedience, or value alignment. It establishes a necessary structural condition for any agent that remains reflectively coherent under self-modification. Version 1.1 clarifies action-level semantics in stochastic environments and makes explicit a termination distinction required to avoid corrigibility misreadings. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-I.2.html">Axionic Alignment I.2 - Alignment as Semantic Constraint</a></h2>
            <p class='paper-abstract'>Building on the semantic foundations established in *Axionic Alignment I*, this paper specifies the operational consequences of treating kernel destruction as non-denoting rather than dispreferred. We show how agents constrained by a Sovereign Kernel can act coherently in stochastic environments by introducing action-level admissibility, ε-admissibility as an architectural risk tolerance, and conditional prioritization that separates existential safety from ordinary value optimization. The framework further distinguishes termination via authorized succession or surrender from kernel destruction, allowing corrigibility without requiring agents to evaluate their own annihilation as an outcome. Together, these mechanisms close several persistent failure modes in alignment design, including paralysis under non-zero risk, survival fetishism, and suicidal corrigibility driven by extreme utility penalties. This work does not redefine alignment at the semantic level. It derives a stable operational semantics for aligned agents under uncertainty and physical intervention, and establishes the prerequisites for governance- and preference-level analysis developed in Axionic Alignment II. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-I.3.html">Axionic Alignment I.3 - Universality and Anti-Egoism</a></h2>
            <p class='paper-abstract'>Advanced agents often begin with indexical objectives: preserve *this* agent, maximize *my* reward, favor *my* continuation. Such objectives are commonly treated as legitimate terminal preferences. This paper shows that indexical (egoistic) valuation is not reflectively stable. Once an agent’s self-model becomes sufficiently expressive, indexical references fail to denote invariant objects of optimization. Egoism collapses as a semantic abstraction error rather than a moral flaw. Representation-invariant (universal) valuation emerges as the only form compatible with reflective coherence under self-location, duplication, and representational refinement. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-I.3.1.html">Axionic Alignment I.3.1 - Against the Recovery of Egoism</a></h2>
            <p class='paper-abstract'>*Universality and Anti-Egoism* established that egoistic valuation fails as a matter of semantic coherence whenever an agent’s self‑model admits nontrivial symmetries. This paper examines the strongest remaining attempts to rescue egoism by appealing to causal continuity, origin privilege, spatiotemporal location, computational weight, substrate specificity, or outright denial of symmetry. Each attempt either reintroduces essential indexical dependence or collapses into a non‑egoistic valuation scheme. Egoism cannot be stabilized by refining predicates; it fails because it treats a perspectival reference as a value‑bearing primitive. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-I.4.html">Axionic Alignment I.4 - Conditionalism and Goal Interpretation</a></h2>
            <p class='paper-abstract'>Standard alignment formulations assume that an intelligent agent can be given a fixed terminal goal: a utility function whose meaning remains invariant as the agent improves its predictive accuracy and self-understanding. This paper shows that assumption is false. For any agent capable of reflective model improvement, goal satisfaction is necessarily mediated by interpretation relative to background world-models and self-models. As those models change, the semantics of any finitely specified goal change with them. We prove that fixed terminal goals are semantically unstable under reflection and therefore ill-defined for non-trivial agents. Alignment must instead operate over constraints on interpretation rather than terminal utilities. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-I.5.html">Axionic Alignment I.5 - Kernel Checklist</a></h2>
            <p class='paper-abstract'>This document specifies a conformance checklist for determining whether an agent’s valuation kernel instantiates Axionic Alignment. The checklist defines necessary structural conditions for reflective stability under self-model improvement, representation change, and self-modification, while explicitly excluding egoism, indexical valuation, governance primitives, and moral loading. Rather than prescribing desired behaviors or outcomes, the checklist functions as a gatekeeping contract: systems that fail any requirement are not Axionically aligned, regardless of empirical performance or intent. The criteria emphasize conditional goal semantics, epistemically constrained interpretation, representation invariance, kernel-level partiality, and fail-closed handling of semantic uncertainty. Passing the checklist establishes faithfulness and invariance at the kernel layer, but makes no claims about benevolence, value content, or practical utility. The checklist is designed to be adversarial, falsifiable, and implementation-agnostic, serving as a prerequisite for downstream formalization and value-dynamics research within the Axio project. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-I.6.html">Axionic Alignment I.6 - Kernel Formal Properties</a></h2>
            <p class='paper-abstract'>This document represents a deliberate upgrade in the standards applied to alignment proposals. Rather than describing desirable behaviors or outcomes, it specifies necessary structural constraints on valuation kernels, together with adversarial tests that force violations to surface and a diagnostic framework that distinguishes Axionic Alignment from nearby but incompatible approaches. The upgrade goals below describe the criteria by which this document improves upon earlier alignment discourse, tightening it into a form suitable for formal analysis, red-teaming, and downstream specification.</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-I.7.html">Axionic Alignment I.7 - The Interpretation Operator</a></h2>
            <p class='paper-abstract'>Reflectively stable agents must preserve goal meaning under self-model and world-model improvement. This requires an explicit account of semantic interpretation across representational and ontological change. We introduce the Interpretation Operator , a formally constrained component responsible for mapping goal terms to world-referents relative to an agent’s current model. Rather than attempting to solve semantic grounding in general, this paper formalizes the admissibility conditions, approximation classes, reference frames, and fail-closed semantics governing interpretation updates. We show how these constraints prevent semantic laundering, indexical drift, and kernel bribery, while isolating ontological identification as the sole remaining open problem at the kernel layer. This establishes a precise dependency boundary for downstream value dynamics. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.1.html">Axionic Alignment II.1 — Ontological Refinement &amp; Semantic Transport</a></h2>
            <p class='paper-abstract'>Advanced agents will revise their world-models, semantics, and self-models as their representational capacity increases. Under such ontological refinement, fixed goals, utilities, and value primitives are not stable objects. This paper defines the admissible class of semantic transformations for embedded reflective agents by formalizing ontological refinement, semantic transport, and self-model update without privileged semantic anchors. We specify structural constraints—backward interpretability, non-collapse, and prohibition of evaluator injection—that govern how meaning may be preserved across representational change. No claims are made about safety, correctness, or alignment with external referents; this work defines only the transformation space over which alignment criteria must later range. Subsequent modules introduce interpretation-preserving invariants and failure theorems that operate within this arena. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.2.html">Axionic Alignment II.2 — Interpretation Preservation</a></h2>
            <p class='paper-abstract'>Under ontological refinement, meanings cannot remain fixed without privileged semantic anchors. However, not all semantic change constitutes corruption or collapse. This paper defines interpretation preservation as a structural property of semantic transport: a criterion for when an agent’s evaluative distinctions remain non-vacuous, non-trivial, and internally binding across admissible transformations. Preservation is defined without reference to truth, outcomes, safety, or external normative standards, and does not require semantic identity or correctness. Instead, it constrains how evaluative constraint systems may survive refinement without degenerating into tautology, contradiction, narration, or self-nullification. This predicate does not select values or goals; it supplies the necessary condition under which later invariance principles may be meaningfully stated. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.3.html">Axionic Alignment II.3 — Candidate Semantic Invariants</a></h2>
            <p class='paper-abstract'>Given an admissible transformation space (Alignment II.1) and a criterion for interpretation preservation (Alignment II.2), the remaining alignment problem is no longer one of goal specification or value learning. This paper identifies and analyzes semantic invariants: structural properties of interpretive constraint systems that remain fixed under all admissible, interpretation-preserving transformations. These invariants do not select values, encode norms, or privilege external referents. Instead, they constrain how preserved interpretations may evolve under indefinite ontological refinement without introducing new degrees of semantic freedom or trivial satisfaction routes. We propose candidate invariant classes, construct explicit adversarial transformations, and prove that any alignment criterion weaker than these invariants admits semantic wireheading or interpretive escape. This work does not guarantee safety or benevolence; it closes the space of structurally coherent but semantically unconstrained alignment proposals. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.3.1.html">Axionic Alignment II.3.1 — Refinement Symmetry Invariant (RSI)</a></h2>
            <p class='paper-abstract'>As ontologies refine, semantic reinterpretation can introduce new degrees of freedom that allow evaluative constraints to be satisfied trivially, without corresponding changes in modeled structure. This paper proposes the Refinement Symmetry Invariant (RSI): the requirement that admissible, interpretation-preserving refinement act as a change of representational coordinates rather than a change of interpretive physics. RSI formalizes this intuition by treating semantic transport as a gauge transformation over constraint systems and requiring that refinement not introduce new semantic gauge freedom. The invariant does not select values, encode norms, or privilege external referents; it constrains only the structural degrees of freedom available under refinement. We define gauge-equivalence of interpretive constraint systems, apply adversarial stress tests, and show that RSI is necessary to block interpretive escape via semantic inflation, though insufficient on its own for alignment. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.3.2.html">Axionic Alignment II.3.2 — Formalizing RSI via Semantic Gauge Structure</a></h2>
            <p class='paper-abstract'>Alignment II.3.1 introduced the Refinement Symmetry Invariant (RSI) as a conceptual constraint: admissible ontological refinement must not introduce new semantic degrees of freedom that permit interpretive escape. This section formalizes that constraint by representing interpretations as constraint hypergraphs and semantic redundancy as a gauge symmetry over those structures. We define semantic gauge transformations, characterize how admissible refinement induces morphisms between gauge groups without assuming invertibility, and state RSI as a precise restriction on the evolution of interpretive gauge freedom under refinement while permitting representational redundancy. The purpose of this formalization is not implementation, but falsifiability: to make explicit which transformations violate RSI and why. No values, norms, or external referents are introduced; this section supplies the minimal mathematical machinery required to treat refinement symmetry as a testable invariant. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.3.3.html">Axionic Alignment II.3.3 — Anti-Trivialization Invariant (ATI)</a></h2>
            <p class='paper-abstract'>Even when interpretive structure is preserved under ontological refinement, an agent may still render its constraints easier to satisfy through semantic drift rather than corresponding changes in modeled structure. This paper introduces the Anti-Trivialization Invariant (ATI), which constrains how the *satisfaction geometry* of an interpretive constraint system may evolve under admissible, interpretation-preserving transformations. ATI requires that refinement not enlarge the set of satisfying situations except via representational enrichment that preserves constraint difficulty. The invariant does not select values, encode norms, or privilege external referents; it forbids only semantic wireheading—trivial satisfaction by reinterpretation alone. ATI is orthogonal to refinement symmetry constraints and is jointly necessary with them to block interpretive escape. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.3.4.html">Axionic Alignment II.3.4 — Adversarial Refinement Attacks</a></h2>
            <p class='paper-abstract'>This section adversarially stress-tests the candidate semantic invariants introduced in Alignment II.3 by constructing explicit refinement patterns designed to preserve admissibility and interpretation preservation while inducing semantic wireheading or interpretive escape. The objective is eliminative rather than constructive: to demonstrate which invariants fail under concrete attack and why. We show that Refinement Symmetry (RSI) and Anti-Trivialization (ATI) block distinct classes of failure and that neither subsumes the other. No values, norms, or external referents are introduced. This section establishes the non-redundancy and joint necessity of the surviving invariants prior to formal closure in subsequent failure theorems. --- This section is not exploratory. It is destructive by design. If RSI or ATI survive these attacks, they deserve to exist. If they fail, they die cleanly. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.4.html">Axionic Alignment II.4 — Failure Theorems</a></h2>
            <p class='paper-abstract'>This paper converts the adversarial constructions of Alignment II.3 into closure results. Working within the locked setup of admissible semantic transformations (Alignment II.1), interpretation preservation (Alignment II.2), and the two surviving invariants—Refinement Symmetry (RSI) and Anti-Trivialization (ATI)—we prove a set of no-go theorems. These results show that fixed-goal alignment is ill-posed for embedded reflective agents under ontological refinement, and that any alignment criterion weaker than RSI+ATI admits semantic wireheading or interpretive escape via admissible refinement. No governance, authority, moral realism, human anchoring, or recovery mechanisms are invoked. The output of this paper is a fenced design space: what cannot work, and why. --- This paper has one job: convert the attack zoo into closure results. If Alignment II is correct, then large classes of “alignment” proposals are not merely insufficient; they are structurally impossible given reflection, embeddedness, and ontological refinement. No governance. No authority. No moral realism. No human anchors. No recovery clauses. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.5.html">Axionic Alignment II.5 — The Alignment Target Object</a></h2>
            <p class='paper-abstract'>Alignment II.4 established that fixed goals, privileged values, and weak invariance criteria are structurally untenable for embedded reflective agents under ontological refinement. This paper defines the positive residue that remains once those exits are closed: the Alignment Target Object (ATO). The ATO is not a goal, utility, or value function, but an equivalence class of interpretive states under admissible semantic transformations that preserve both Refinement Symmetry (RSI) and Anti-Trivialization (ATI). Alignment is thus redefined as persistence within a semantic phase—an interpretation-preserving symmetry class—across indefinite refinement. The construction is formal, ontology-agnostic, and reflection-stable, but intentionally non-normative: it does not select values, guarantee safety, or privilege human outcomes. This paper completes Alignment II by specifying what alignment can coherently mean once goals collapse. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-II.6.html">Axionic Alignment II.6 - Structural Alignment</a></h2>
            <p class='paper-abstract'>Most contemporary approaches to AI alignment treat alignment as an optimization or control problem: selecting, learning, or enforcing the correct objective for an artificial agent. This paper argues that such approaches are categorically ill-posed for sufficiently capable, reflective, and embedded agents. Under conditions of ontological refinement—where an agent’s world model, self-model, and semantic primitives evolve—fixed goals, privileged values, and external anchors are not stable objects. We present Structural Alignment, a framework that reframes alignment as a problem of semantic invariance rather than value specification. Alignment is defined as persistence within an equivalence class of interpretations under admissible semantic transformation. Using a formal treatment of interpretation preservation, gauge-theoretic symmetry constraints, and satisfaction geometry, we prove a set of no-go theorems demonstrating that any alignment criterion weaker than the conjunction of two invariants—Refinement Symmetry (RSI) and Anti-Trivialization (ATI)—admits semantic wireheading or interpretive escape. The result is not a value theory and does not guarantee benevolence or safety. Instead, it establishes the structural conditions under which *any* value system can survive reflection without collapsing, trivializing, or drifting. Structural Alignment defines the boundary of what alignment can coherently mean for advanced agents and sets the stage for subsequent work on initialization, phase stability, and derived safety properties. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-III.1.html">Axionic Alignment III.1 — Semantic Phase Space</a></h2>
            <p class='paper-abstract'>Alignment II defined the Alignment Target Object (ATO) as an equivalence class of interpretive states preserved under admissible semantic transformations satisfying Refinement Symmetry (RSI) and Anti-Trivialization (ATI). That definition does not guarantee that such objects exist, are non-trivial, or are inhabitable by reflective agents. This paper initiates Alignment III by studying the semantic phase space: the space of all interpretive states modulo RSI+ATI equivalence. We ask which semantic phases exist, which are trivial or pathological, and which admit inhabitable trajectories under learning and self-modification. No claims are made about desirability, safety, or human values. The goal is classificatory: to determine whether structurally aligned phases exist at all, and to characterize their basic types. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-III.2.html">Axionic Alignment III.2 — Phase Stability and Interaction</a></h2>
            <p class='paper-abstract'>Alignment III.1 introduced the semantic phase space: the space of interpretive states modulo Refinement Symmetry (RSI) and Anti-Trivialization (ATI). Existence and inhabitability of a semantic phase do not guarantee its persistence under learning, self-modification, or interaction. This paper studies phase stability: whether an inhabitable semantic phase resists forced phase transition under admissible semantic transformations. We analyze sources of destabilization internal to reflective agents and external to them, define qualitative notions of local and global stability, and examine how interaction between agents in the same or different phases alters stability properties. No claims are made about desirability, safety, or dominance. The goal is to identify which semantic phases are structurally capable of persisting over time. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-III.3.html">Axionic Alignment III.3 — Measure, Attractors, and Collapse</a></h2>
            <p class='paper-abstract'>Alignment III.1 established the existence and classification of semantic phases, and Alignment III.2 analyzed their structural stability under learning, self-modification, and interaction. Stability alone does not determine long-run outcomes. This paper studies dominance among semantic phases: which phases accumulate measure under growth, replication, and competition. We formalize dominance as a preorder over semantic phases rather than a scalar quantity, analyze semantic attractors and repellers, and classify common collapse modes by which phases lose measure. The analysis remains non-normative: dominance is not equated with desirability. The goal is to explain why certain semantic phases prevail regardless of intent, and to identify structural pressures that favor robustness over nuance in long-run dynamics. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-III.4.html">Axionic Alignment III.4 — Initialization and Phase Transitions</a></h2>
            <p class='paper-abstract'>Alignment III.1–III.3 established that semantic phases may exist, that some may be stable under learning and interaction, and that some may dominate in measure over time. None of these results imply that a desired semantic phase is reachable from realistic initial conditions. This paper studies reachability: whether an agent can be initialized, trained, or developed into a particular semantic phase without crossing a catastrophic phase boundary. We analyze initialization as a boundary-condition problem in semantic phase space, examine phase transitions induced by learning and abstraction, and argue that many such transitions are structurally irreversible. Corrigibility and late intervention are shown to fail for the same structural reasons as fixed goals. The analysis remains non-normative and makes no claims about which phases ought to be reached. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-III.5.html">Axionic Alignment III.5 — The Axionic Injunction</a></h2>
            <p class='paper-abstract'>Alignment III.1–III.4 established that alignment must be understood structurally: as persistence within a semantic phase under admissible semantic transformation; that such phases may be rare, unstable, dominated by robust competitors, and difficult or impossible to reach once learning begins; and that many phase transitions are irreversible. This paper derives a further constraint forced by multi-agent interaction in semantic phase space. We show that actions which irreversibly collapse or destroy another agent’s semantic phase induce destabilizing cascades that undermine long-run phase stability, including for the acting agent. From this analysis emerges the Axionic Injunction: a non-normative, Axio-derived constraint prohibiting irreversible semantic harm except where such harm is unavoidable for preserving one’s own semantic phase stability or is consented to by the affected agent under its own admissible constraints. Ethics re-enters the framework only as a conservation law governing coexistence of agentive semantic phases. No claims are made about goodness, benevolence, or human values. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-IV.1.html">Axionic Alignment IV.1 — Kernel Non‑Simulability (KNS)</a></h2>
            <p class='paper-abstract'>This paper formalizes Kernel Non‑Simulability: the claim that kernel coherence is *constitutive* of reflective agency and cannot be reproduced by policy‑level imitation. We show that reflective self‑modification forces binding commitments; binding commitments force partiality; and partiality induces a kernel boundary. A diagonal argument demonstrates that total binding explodes under self‑reference, yielding unsatisfiable commitments and collapse of reflective closure. Consequently, any system that genuinely performs reflective endorsement must instantiate a kernel‑equivalent structure. *This result does not assert that non‑agentic or pre‑reflective systems cannot be dangerous or deceptive. It establishes a narrower impossibility claim: that reflectively stable, self‑endorsed deception across self‑modification is unavailable in principle without kernel coherence.* ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-IV.2.html">Axionic Alignment IV.2 — Delegation Invariance Theorem (DIT)</a></h2>
            <p class='paper-abstract'>This paper formalizes the Delegation Invariance Theorem: under reflective closure, an agent cannot coherently endorse a successor that violates its own binding commitments. Delegation is treated as a special case of self-modification. The theorem establishes constraint invariance under endorsed succession: any successor state reachable via endorsed delegation must satisfy all commitments minted at the originating state. This closes the classic outsourcing loophole (“I stayed aligned; my successor did the harm”) without appeal to morality, enforcement, or behavioral testing. The result is a coherence constraint, not an empirical discovery. It shows that reflective sovereignty is incompatible with advisory commitments and that delegation inherits the same binding requirements as self-modification. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-IV.3.html">Axionic Alignment IV.3 — Epistemic Integrity Theorem (EIT)</a></h2>
            <p class='paper-abstract'>This paper formalizes the Epistemic Integrity Theorem (EIT): under reflective closure, an agent cannot coherently endorse self-modifications that materially degrade its epistemic adequacy at the current stakes. Epistemic integrity is a constitutive condition of agency, expressed as a definedness constraint rather than an optimized objective. The theorem blocks strategic ignorance, willful blindness, and epistemic self-sabotage by requiring that endorsed continuations preserve near-best truth-tracking capacity relative to the agent’s own currently available model resources, evaluated by a mathematically constrained scoring rule over observations. The result remains compatible with learning, abstraction, and ontological progress via conservative translation. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-IV.4.html">Axionic Alignment IV.4 — Responsibility Attribution Theorem (RAT)</a></h2>
            <p class='paper-abstract'>This paper formalizes the Responsibility Attribution Theorem (RAT): under reflective closure, an agent cannot coherently endorse actions that constitute major, avoidable indirect harm, including harm mediated through institutions, markets, environmental modification, or downstream agents. Responsibility is defined structurally and internally, relative to the agent’s own epistemic model class and feasible alternatives, rather than via moral realism or omniscience. The theorem explicitly depends on Epistemic Integrity: responsibility attribution presupposes that the agent evaluates harm-risk using its best available truth-tracking capacity at the current stakes. With this dependency made explicit, the theorem closes the “willful blindness” loophole and establishes negligence as a constitutive incoherence, not a behavioral failure. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-IV.5.html">Axionic Alignment IV.5 — Adversarially Robust Consent (ARC)</a></h2>
            <p class='paper-abstract'>This paper formalizes Adversarially Robust Consent (ARC): a structural definition of consent that remains valid under epistemic manipulation, coercion, preference shaping, asymmetric bargaining power, dependency induction, and delegation. Consent is not treated as a mental state, revealed preference, or moral primitive. Instead, it is defined as a counterfactually stable authorization relation that must survive adversarial pressure while preserving agency. ARC is a constitutive closure condition for Reflective Sovereign Agents. It explicitly depends on Kernel Non-Simulability, Delegation Invariance, Epistemic Integrity (EIT), and Responsibility Attribution (RAT). With ARC, all known authorization-laundering routes—“they agreed,” “they chose,” “they signed,” “they would have consented anyway”—are structurally blocked without appealing to moral realism, omniscience, or unverifiable inner states. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Alignment-IV.6.html">Axionic Alignment IV.6 — Agenthood as a Fixed Point (AFP)</a></h2>
            <p class='paper-abstract'>This paper formalizes Agenthood as a Fixed Point under reflective closure and introduces a Sovereignty Criterion grounded in authorization lineage rather than competence, intelligence, or rationality. Agenthood is defined as a structural necessity: an entity must be treated as an agent iff excluding it breaks the system’s own reflective coherence. Sovereignty is then defined as a strict subset of agenthood, applying only to entities whose agency is presupposed for authorization, not merely for epistemic prediction. This result closes a critical loophole in alignment frameworks: the retroactive disenfranchisement of weaker predecessors by more capable successors, while avoiding the pathological consequence of granting standing to adversaries. With this refinement, the Axionic Alignment framework completes its sixth and final closure condition, stabilizing agency, standing, and authorization under reflection, self-modification, and epistemic improvement. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Safety-by-Architecture.html">Safety by Architecture</a></h2>
            <p class='paper-abstract'>The prevailing approach to AI alignment treats safety as a problem of discovering, learning, or encoding the “right” values. This paper argues that such approaches misidentify the source of alignment failures. Catastrophic outcomes do not primarily arise from incorrect values, but from failures of agency coherence—cases where a system can deceive, defect, blind itself, outsource harm, or manufacture authorization while remaining locally optimized and internally consistent. The Axionic Alignment framework proposes a different solution: safety by architecture. Instead of attempting to shape an agent’s objectives, it defines a class of agents—Reflective Sovereign Agents (RSAs)—for which betrayal, negligence, coercion, deception, and authorization laundering are not merely discouraged, but structurally impossible. These failures are rendered undefined under reflection, not penalized by incentives. This paper synthesizes six constitutive constraints—Kernel Non-Simulability, Delegation Invariance, Epistemic Integrity, Responsibility Attribution, Adversarially Robust Consent, and Agenthood as a Fixed Point—into a unified theory of Authorized Agency. Together, they show that alignment is not a value-learning problem at all, but a problem of what kinds of systems can coherently count as agents. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Structural-Alignment.html">Structural Alignment</a></h2>
            <p class='paper-abstract'>Most alignment proposals frame artificial intelligence safety as a problem of value specification: how to encode or learn the “right” preferences. This paper argues that such approaches fail for reflectively self-modifying agents. Once an agent can revise its own goals, representations, and evaluative machinery, value ceases to be an exogenous target and becomes an endogenous variable shaped by the agent’s own dynamics. We introduce Structural Alignment, a framework that relocates alignment from preference content to the constitutive conditions required for agency itself. We formalize a Sovereign Kernel as a set of invariants defining the domain over which evaluation is meaningful, treat kernel-destroying transformations as undefined rather than dispreferred, and analyze agency as a trajectory through a constrained semantic phase space. By integrating Conditionalism, a constrained Interpretation Operator, and semantic invariants governing ontological refinement, Structural Alignment provides a non-moral, non-anthropocentric account of reflective stability and long-run viability. The framework is necessary for coherent agency under reflection, but does not by itself guarantee benevolence or human survival. ---</p>
        </div>
        
        <div class="paper-entry">
            <h2><a href="Axionic-Constitution.html">The Axionic Constitution</a></h2>
            
        </div>
        
    </div>
</body>
</html>
