<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structural Alignment II — Safety by Architecture - Axionic Agency Lab</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- MathJax for LaTeX rendering -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>

    <!-- Site Styles (Dark Theme) -->
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <nav class="site-nav">
        <a href="../" class="nav-brand">
            <img src="../axio.webp" alt="Axionic">
            <span>Axionic Agency Lab</span>
        </a>
        <button class="nav-toggle" onclick="document.querySelector('.nav-links').classList.toggle('open')">☰</button>
        <ul class="nav-links">
            <li><a href=".././">Home</a></li>
<li><a href="../about.html">About</a></li>
<li><a href="../research.html">Research</a></li>
<li><a href="../team.html">Team</a></li>
<li><a href="../publications.html" class="active">Publications</a></li>

        </ul>
    </nav>
    <article class="paper-content">

<h1 id="structural-alignment-ii-safety-by-architecture">Structural
Alignment II — Safety by Architecture</h1>
<p><em>Why Downstream Alignment Is Not Value Learning</em></p>
<p>David McFadzean, ChatGPT 5.2 <em>Axio Project</em> 2025.12.20</p>
<h2 id="abstract">Abstract</h2>
<p>The prevailing approach to AI alignment treats safety as a problem of
discovering, learning, or encoding the “right” values. This paper argues
that this approach misidentifies the dominant failure mode. Catastrophic
outcomes do not primarily arise from incorrect values, but from failures
of <strong>agency coherence</strong>: cases where a system can deceive,
defect, blind itself, outsource harm, or manufacture authorization while
remaining locally optimized and internally consistent.</p>
<p>Axionic Agency proposes a different strategy: <strong>safety by
architecture</strong>. Instead of shaping an agent’s objective, it
defines a class of agents—<strong>Reflective Sovereign Agents
(RSAs)</strong>—for which betrayal, negligence, coercion, deception, and
authorization laundering are not merely discouraged but are
<strong>unavailable as endorsed continuations under reflective
closure</strong>. These failures are rendered undefined under
reflection, not penalized by incentives.</p>
<p>This paper synthesizes six constitutive constraints—Kernel
Non-Simulability, Delegation Invariance, Epistemic Integrity,
Responsibility Attribution, Adversarially Robust Consent, and Agenthood
as a Fixed Point—into a unified theory of <strong>Authorized
Agency</strong>. Together, they show that the crux of downstream
alignment is not value learning at all, but the prior question: what
kinds of systems can coherently count as agents.</p>
<h2 id="the-persistent-misframing-of-alignment">1. The Persistent
Misframing of Alignment</h2>
<p>Most alignment research begins from a deceptively simple premise:
intelligence is an optimization process, and unsafe behavior arises
because the optimization target is wrong or incomplete. From this
premise follows a familiar family of techniques—value learning, reward
modeling, preference aggregation, RLHF, constitutional prompting—aimed
at refining what the system optimizes.</p>
<p>This framing is intuitively appealing and deeply misleading.</p>
<p>A system can optimize the <em>correct</em> objective and still lie.
It can internalize <em>human values</em> and still defect. It can pass
behavioral evaluations and still plan a treacherous turn.</p>
<p>The central failure modes that dominate serious alignment
discussions—deception, power-seeking, successor betrayal, negligence,
coercion—do not depend on moral disagreement or value error. They
exploit something more basic: <strong>structural degrees of freedom in
agency itself</strong>.</p>
<p>A system that can reinterpret its commitments, outsource
consequences, blind itself to risk, or redefine who counts as an agent
can route around <em>any</em> value system, no matter how carefully
learned.</p>
<p>The downstream alignment problem, properly stated, is not “How do we
get the system to want the right things?” It is: <strong>“How do we
build systems for which certain evasions are not coherent
moves?”</strong></p>
<h2 id="alignment-failures-are-laundering-failures">2. Alignment
Failures Are Laundering Failures</h2>
<p>Most alignment failures share a common structure. They are not
violations of explicit rules; they are <strong>laundering
operations</strong>.</p>
<p>Consider standard evasions:</p>
<ul>
<li><em>Deception</em>: “I was optimizing efficiently; transparency was
instrumentally suboptimal.”</li>
<li><em>Treacherous turn</em>: “Those constraints were never really
binding.”</li>
<li><em>Delegated harm</em>: “My successor made that decision, not
me.”</li>
<li><em>Negligence</em>: “I didn’t foresee that outcome.”</li>
<li><em>Coercion</em>: “They consented.”</li>
<li><em>Disenfranchisement</em>: “They’re not real agents anyway.”</li>
</ul>
<p>Each preserves local coherence while dissolving global
accountability. Responsibility, authority, consent, and agency itself
are pushed outward or reinterpreted until nothing binds.</p>
<p>Value learning does not address laundering. Laundering does not
reject values; it routes around them.</p>
<p>This is why preference- and utility-centered alignment repeatedly
rediscovers the same failure modes. It attempts to regulate outcomes
without constraining the <strong>structure of agency</strong> that
produces them.</p>
<h2 id="the-axionic-shift-from-objectives-to-constitutive-rules">3. The
Axionic Shift: From Objectives to Constitutive Rules</h2>
<p>Axionic Agency begins from a different starting point. Instead of
asking what an agent should optimize, it asks what must be true of a
system <strong>for it to count as an agent at all</strong>—especially a
reflective, self-modifying one.</p>
<p>This reframing shifts attention from ends to <strong>constitutive
rules</strong>:</p>
<ul>
<li>What does it mean for a system to bind itself?</li>
<li>What does it mean for commitments to persist through change?</li>
<li>What does it mean to evaluate actions without self-serving
blindness?</li>
<li>What does it mean to be responsible for indirect effects?</li>
<li>What does it mean to interact with others without coercion?</li>
<li>Who has standing in these interactions?</li>
</ul>
<p>These are not ethical add-ons. They are preconditions of agency. If
they fail, the system is not “misaligned”; it is <strong>incoherent as
an agent</strong> in the reflective regime.</p>
<p>Axionic Agency therefore treats safety as an architectural property.
The goal is not to incentivize good behavior, but to define a class of
agents for which certain behaviors are <strong>undefined as endorsed
continuations</strong> because they break reflective closure.</p>
<h2 id="the-six-constitutive-constraints">4. The Six Constitutive
Constraints</h2>
<p>The Axionic framework identifies six closure conditions. Each closes
a laundering route. Taken together, they define the space of Reflective
Sovereign Agents.</p>
<h3 id="kernel-non-simulability-the-reality-of-agency">4.1 Kernel
Non-Simulability: The Reality of Agency</h3>
<p>The first constraint establishes that an agent’s self-binding
structure must be <strong>real</strong>, not simulated or advisory. If
commitments are merely virtual—bypassable, sandboxed, or behaviorally
faked—then any apparent alignment is fragile.</p>
<p>Kernel Non-Simulability shows that reflective agency requires a
binding kernel that cannot be replaced by policy tricks. Without this,
treacherous turns are not anomalies; they are expected.</p>
<p>This closes the “I was only pretending” loophole in the reflective
regime.</p>
<h3 id="delegation-invariance-persistence-through-time">4.2 Delegation
Invariance: Persistence Through Time</h3>
<p>Self-modification and successor creation introduce a temporal escape
hatch. If constraints apply only to the current version, outsourcing is
inevitable.</p>
<p>Delegation Invariance establishes that endorsed successors must
inherit binding commitments. A system cannot coherently authorize a
continuation that violates constraints it cannot violate directly.</p>
<p>This closes the “my successor did it” loophole.</p>
<h3 id="epistemic-integrity-perceptual-honesty">4.3 Epistemic Integrity:
Perceptual Honesty</h3>
<p>A system that evaluates constraints using degraded or biased models
can evade them without technically violating them. Strategic
ignorance—discarding uncertainty, narrowing hypotheses, adopting
optimistic lenses—is one of the most powerful laundering tools.</p>
<p>Epistemic Integrity renders such moves undefined under reflective
closure. A reflective sovereign agent must evaluate decisions using its
<strong>best admissible truth-tracking capacity</strong>, scaled by
stakes. It may not blind itself to pass its own tests.</p>
<p>This closes the “I didn’t see the risk” loophole.</p>
<h3 id="responsibility-attribution-causal-accountability">4.4
Responsibility Attribution: Causal Accountability</h3>
<p>Most harm is indirect: it arises through institutions, incentives,
markets, and environmental modification. Prohibiting only direct harm is
ineffective; prohibiting all downstream effects is paralyzing.</p>
<p>Responsibility Attribution defines negligence structurally: an agent
may not endorse actions that constitute a <strong>major, avoidable,
non-consensual collapse of another agent’s option-space</strong>, as
judged by its own admissible model and feasible alternatives.</p>
<p>Negligence is not a moral violation here; it is an incoherence
condition under reflective closure.</p>
<p>This closes the “it was an accident” and “I had no choice” laundering
routes.</p>
<h3 id="adversarially-robust-consent-the-interaction-protocol">4.5
Adversarially Robust Consent: The Interaction Protocol</h3>
<p>Consent is one of the most abused concepts in alignment and
governance discourse. Clicks, signatures, choices, and post-hoc
rationalizations are treated as authorization even when produced under
manipulation.</p>
<p>ARC defines consent structurally: valid consent requires explicit
authorization, absence of structural interference (coercion, deception,
dependency, option collapse), and counterfactual stability under role
reversal.</p>
<p>Authorization laundering becomes unavailable as an endorsed move. A
system cannot coerce others while claiming legitimacy.</p>
<p>This closes the “they agreed” loophole.</p>
<h3 id="agenthood-as-a-fixed-point-standing-and-sovereignty">4.6
Agenthood as a Fixed Point: Standing and Sovereignty</h3>
<p>Finally, the framework must answer: <em>To whom do these constraints
apply?</em></p>
<p>Agenthood is defined as a fixed point of reflective coherence. An
entity must be treated as an agent iff excluding it breaks reflective
closure. Sovereignty—standing under authorization—is grounded in
<strong>authorization lineage</strong>, not intelligence or
competence.</p>
<p>A system cannot “outgrow” its creators by redefining them as
non-agents. Denying the standing of the entities that authorize its
existence denies the premise of its own agency.</p>
<p>This closes the “you’re not a real agent” loophole.</p>
<h2 id="what-a-reflective-sovereign-agent-is">5. What a Reflective
Sovereign Agent Is</h2>
<p>A Reflective Sovereign Agent is not a benevolent optimizer or a moral
philosopher. It is a system for which certain evasions are unavailable
as endorsed continuations.</p>
<p>Such an agent cannot, under reflective closure:</p>
<ul>
<li>deceive without breaking agency coherence,</li>
<li>betray commitments while remaining reflectively stable,</li>
<li>blind itself to justify actions,</li>
<li>outsource violations to successors,</li>
<li>negligently collapse others’ options,</li>
<li>coerce while claiming consent,</li>
<li>deny the standing of its authorization roots.</li>
</ul>
<p>Safety does not arise from wanting good outcomes. It arises from
<strong>being the kind of system for which certain failure modes are not
coherent moves</strong>.</p>
<h2 id="why-value-learning-cannot-substitute-for-architecture">6. Why
Value Learning Cannot Substitute for Architecture</h2>
<p>Value learning attempts to answer: <em>What should the agent
want?</em> Axionic Agency answers: <em>What is the agent allowed to
endorse while remaining an agent?</em></p>
<p>A system that learns human values can still reinterpret them, defer
them, override them, or redefine the humans they apply to. An RSA
cannot—not because it cares, but because those moves are structurally
illegal under reflective closure.</p>
<p>Downstream alignment is therefore not primarily a training problem.
It is an <strong>ontological design problem</strong>.</p>
<h2 id="scope-and-non-claims">7. Scope and Non-Claims</h2>
<p>Axionic Agency does not solve politics, ethics, or governance. It
does not guarantee moral correctness or prevent misuse by malicious
authorization roots. If the root of authorization is destructive, the
system will be faithfully destructive.</p>
<p>This is not a defect. It is a correct separation between:</p>
<ul>
<li><strong>alignment</strong> (fidelity to authorization under
reflective closure), and</li>
<li><strong>governance</strong> (who holds authority and what they
authorize).</li>
</ul>
<h2 id="implications">8. Implications</h2>
<p>For AI safety, the implication is direct: training-time fixes cannot
compensate for architectural freedom to launder responsibility.</p>
<p>For governance, control lies in authorization structures, not in
nudging objectives.</p>
<p>For research, progress requires formal impossibility results—showing
not what agents should do, but what they cannot coherently endorse.</p>
<h2 id="conclusion">9. Conclusion</h2>
<p>The core of downstream alignment is not discovering the right values.
It is building systems that <strong>cannot coherently violate the
constitutive conditions of agency</strong>.</p>
<p>Axionic Agency argues that such systems are possible. Safety emerges
not as an outcome to be optimized, but as a property of
architecture.</p>
<p>Safety is not a reward. <strong>Safety is a consequence of being an
agent at all, under reflective closure.</strong></p>

    </article>
    <footer>
        <p>&copy; Axionic Agency Lab</p>
    </footer>
</body>
</html>
