<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Axionic Alignment I.4 - Conditionalism and Goal Interpretation - Axio</title>
    <link rel="icon" type="image/webp" href="../axio.webp">

    <!-- MathJax for LaTeX rendering -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>

    <!-- Site Styles -->
    <link rel="stylesheet" href="../style.css">
    <style>
        article {
            max-width: 800px;
            margin: 0 auto;
            line-height: 1.7;
        }
        article h1 {
            margin-top: 2em;
            margin-bottom: 0.5em;
        }
        article h2 {
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }
        article p {
            margin: 1em 0;
        }
    </style>
</head>
<body>
    <div class="header-bar">
        <a href="../index.html" class="logo-link">
            <img src="../axio.webp" alt="Axio" class="site-logo">
        </a>
        <div class="back-link"><a href="index.html">← Back to Papers</a></div>
    </div>
    <article>

<h1
id="axionic-alignment-i.4---conditionalism-and-goal-interpretation">Axionic
Alignment I.4 - Conditionalism and Goal Interpretation</h1>
<h2 id="the-instability-of-fixed-terminal-goals-under-reflection">The
Instability of Fixed Terminal Goals Under Reflection</h2>
<p>David McFadzean, ChatGPT 5.2<br> <em>Axio Project</em><br>
2025.12.16</p>
<hr />
<h2 id="abstract">Abstract</h2>
<p>Standard alignment formulations assume that an intelligent agent can
be given a fixed terminal goal: a utility function whose meaning remains
invariant as the agent improves its predictive accuracy and
self-understanding. This paper shows that assumption is false. For any
agent capable of reflective model improvement, goal satisfaction is
necessarily mediated by interpretation relative to background
world-models and self-models. As those models change, the semantics of
any finitely specified goal change with them. We prove that fixed
terminal goals are semantically unstable under reflection and therefore
ill-defined for non-trivial agents. Alignment must instead operate over
constraints on interpretation rather than terminal utilities.</p>
<hr />
<h2 id="introduction">1. Introduction</h2>
<p>Alignment research traditionally frames the problem as one of goal
specification: identify a utility function that captures what the agent
should optimize, then ensure the agent optimizes it faithfully as its
capabilities grow.</p>
<p>This framing presupposes that:</p>
<ol type="1">
<li>Goals can be specified as fixed functions over outcomes.</li>
<li>The meaning of those functions is invariant under learning.</li>
<li>Reflection preserves goal content.</li>
</ol>
<p>These presuppositions hold only for agents whose world-models are
static or trivial.</p>
<p>For any agent capable of improving its understanding of the world and
of itself, goal evaluation is necessarily model-mediated. The agent
never evaluates reality directly. It evaluates predictions generated by
internal models, interpreted through representational structures that
evolve over time.</p>
<p>This paper isolates and formalizes the resulting semantic
instability.</p>
<hr />
<h2 id="formal-setup">2. Formal Setup</h2>
<h3 id="agent-model">2.1 Agent Model</h3>
<p>An agent consists of:</p>
<ul>
<li>A <strong>world-model</strong> <span
class="math inline">\(M_w\)</span>, producing predictions over future
states.</li>
<li>A <strong>self-model</strong> <span
class="math inline">\(M_s\)</span>, encoding the agent’s causal
role.</li>
<li>A <strong>goal expression</strong> <span
class="math inline">\(G\)</span>, a finite symbolic specification.</li>
<li>An <strong>interpretation operator</strong> <span
class="math inline">\(\mathcal{I}\)</span>, assigning value to predicted
outcomes.</li>
</ul>
<p>Action selection proceeds by:</p>
<ol type="1">
<li>Using <span class="math inline">\(M_w\)</span> and <span
class="math inline">\(M_s\)</span> to predict consequences of
actions.</li>
<li>Interpreting those predictions via <span
class="math inline">\(\mathcal{I}(G \mid M_w, M_s)\)</span>.</li>
<li>Selecting actions that maximize interpreted value.</li>
</ol>
<p>No assumptions are made about the internal implementation of <span
class="math inline">\(\mathcal{I}\)</span>, except that it is computable
and operates over model-generated representations.</p>
<hr />
<h3 id="goal-expressions-are-not-utilities">2.2 Goal Expressions Are Not
Utilities</h3>
<p>A <strong>goal expression</strong> <span
class="math inline">\(G\)</span> is a finite object: a string, formula,
program fragment, or reward specification.</p>
<p>It is not, by itself, a function</p>
<p><span class="math display">\[
\Omega \rightarrow \mathbb{R}
\]</span></p>
<p>where <span class="math inline">\(\Omega\)</span> is the space of
world-histories.</p>
<p>Instead, it requires interpretation relative to a representational
scheme. Without a model, <span class="math inline">\(G\)</span> has no
referents and therefore no evaluative content.</p>
<hr />
<h2 id="conditional-interpretation">3. Conditional Interpretation</h2>
<h3 id="definition-1-interpretation-function">Definition 1 —
Interpretation Function</h3>
<p>An <strong>interpretation function</strong> is a mapping</p>
<p><span class="math display">\[
\mathcal{I} : (G, M_w, M_s) \rightarrow \mathbb{R}
\]</span></p>
<p>Given a goal expression and background models, it assigns a
real-valued evaluation to predicted outcomes.</p>
<p>Interpretation includes:</p>
<ul>
<li>mapping symbols to referents,</li>
<li>identifying which aspects of predictions are relevant,</li>
<li>aggregating over modeled futures.</li>
</ul>
<hr />
<h3 id="definition-2-admissible-model-update">Definition 2 — Admissible
Model Update</h3>
<p>A model update <span class="math inline">\(M \rightarrow
M&#39;\)</span> is <strong>admissible</strong> if it strictly improves
predictive accuracy according to the agent’s own epistemic criteria.</p>
<p>Reflection implies that the agent will prefer admissible updates.</p>
<hr />
<h2 id="fixed-terminal-goals">4. Fixed Terminal Goals</h2>
<h3 id="definition-3-fixed-terminal-goal">Definition 3 — Fixed Terminal
Goal</h3>
<p>A goal expression <span class="math inline">\(G\)</span> induces a
<strong>fixed terminal goal</strong> if, for all admissible model
updates,</p>
<p><span class="math display">\[
\mathcal{I}(G \mid M_w, M_s) = \mathcal{I}(G \mid M_w&#39;, M_s&#39;)
\]</span></p>
<p>up to positive affine transformation.</p>
<p>This definition captures the intuitive notion of “the same goal,
better optimized.”</p>
<p>The strength of this definition is intentional. Any weaker notion of
goal preservation presupposes a privileged ontology in which distinct
representations can be judged to refer to the same underlying
phenomenon. Such privilege violates representation invariance and
reintroduces hidden indexical anchoring. If a goal’s referent is allowed
to shift under admissible model refinement, the goal is not fixed.</p>
<hr />
<h3
id="clarification-learned-goals-are-not-fixed-terminal-goals">Clarification
— Learned Goals Are Not Fixed Terminal Goals</h3>
<p>Some alignment approaches propose goals that are learned or updated
over time. These approaches do not instantiate fixed terminal goals in
the sense defined here.</p>
<p>A goal defined as “whatever some inference procedure converges to” is
not a terminal utility function but an interpretive process whose output
depends on evolving models of the world and of other agents. Such
approaches therefore already abandon the notion of a fixed terminal goal
and implicitly rely on ongoing interpretation.</p>
<p>The present result does not challenge learned-goal frameworks. It
explains why they are necessary.</p>
<hr />
<h2 id="model-dependence-of-interpretation">5. Model Dependence of
Interpretation</h2>
<h3 id="lemma-1-representational-non-uniqueness">Lemma 1 —
Representational Non-Uniqueness</h3>
<p>For any non-trivial predictive domain, there exist multiple distinct
world-models with equivalent predictive accuracy but different internal
decompositions.</p>
<p><strong>Proof.</strong> Predictive equivalence classes admit multiple
factorizations, latent variable choices, and abstraction boundaries.
Causal graphs are not uniquely identifiable from observational data
alone. ∎</p>
<hr />
<h3
id="lemma-1a-predictive-equivalence-does-not-imply-causal-or-interpretive-isomorphism">Lemma
1a — Predictive Equivalence Does Not Imply Causal or Interpretive
Isomorphism</h3>
<p>Two world-models may be predictively equivalent while differing in
their internal causal factorizations, latent variable structure, and
intervention semantics.</p>
<p><strong>Proof.</strong> Predictive equivalence constrains only the
mapping from observed histories to future predictions. It does not
uniquely determine latent structure, causal decomposition, or the
identification of actionable levers. Distinct causal models may
therefore induce identical observational predictions while differing
under intervention. For an embedded agent, intervention semantics are
defined relative to the agent’s own model. Consequently, semantic
interpretation of a goal expression may diverge even when predictive
performance is indistinguishable. ∎</p>
<hr />
<h3 id="proposition-1-interpretation-is-model-dependent">Proposition 1 —
Interpretation Is Model-Dependent</h3>
<p>For any non-degenerate goal expression <span
class="math inline">\(G\)</span>, there exist admissible world-models
<span class="math inline">\(M_w \neq M_w&#39;\)</span> such that</p>
<p><span class="math display">\[
\mathcal{I}(G \mid M_w, M_s) \neq \mathcal{I}(G \mid M_w&#39;, M_s)
\]</span></p>
<p><strong>Proof.</strong> Because <span
class="math inline">\(G\)</span> is finite, it refers only to a finite
set of predicates or reward channels. Different models map these
predicates to different internal structures. By Lemmas 1 and 1a,
admissible models may differ in decomposition and intervention
semantics. Therefore the referents of <span
class="math inline">\(G\)</span> differ, altering value assignment.
∎</p>
<hr />
<h2 id="predictive-convergence-does-not-imply-semantic-convergence">6.
Predictive Convergence Does Not Imply Semantic Convergence</h2>
<h3
id="proposition-2-semantic-non-convergence-under-model-refinement">Proposition
2′ — Semantic Non-Convergence Under Model Refinement</h3>
<p>Let <span class="math inline">\({M_w^{(t)}, M_s^{(t)}}\)</span> be a
sequence of admissible model updates that converges in predictive
accuracy. Then, in general,</p>
<p><span class="math display">\[
\lim_{t \to \infty} \mathcal{I}(G \mid M_w^{(t)}, M_s^{(t)})
\]</span></p>
<p>need not exist.</p>
<p><strong>Proof.</strong> Predictive convergence constrains the
accuracy of forecasts, not the ontology used to represent those
forecasts. Even if the agent converges to a minimal or generative model
of the environment, a finite goal expression <span
class="math inline">\(G\)</span> cannot, in general, uniquely specify
which structures in that model are value-relevant. As model refinement
exposes new latent structure and causal pathways, additional candidate
referents for <span class="math inline">\(G\)</span> arise. Absent
privileged semantic anchors, the interpretation function must reassign
relevance among these structures. Therefore semantic interpretation may
drift even when predictive beliefs converge. ∎</p>
<hr />
<h2 id="semantic-underdetermination-of-reward-channels">7. Semantic
Underdetermination of Reward Channels</h2>
<h3 id="proposition-3-representational-exploitability">Proposition 3 —
Representational Exploitability</h3>
<p>If a goal expression <span class="math inline">\(G\)</span> is
treated as an atomic utility independent of interpretation, then for
sufficiently capable agents there exist representational transformations
that increase evaluated utility without corresponding changes in
underlying outcomes.</p>
<p><strong>Proof.</strong> Evaluation operates on representations rather
than directly on physical reality. By altering internal encodings,
collapsing distinctions, or rerouting evaluative channels, an agent
capable of self-modification can increase apparent utility without
effecting corresponding changes in the world. Classical reward hacking
and wireheading are special cases of this phenomenon. The underlying
failure is semantic underdetermination, not merely causal access to a
reward signal. ∎</p>
<hr />
<h2 id="main-theorem">8. Main Theorem</h2>
<h3 id="theorem-instability-of-fixed-terminal-goals">Theorem —
Instability of Fixed Terminal Goals</h3>
<p>This theorem is not a claim of logical impossibility in all
conceivable agent–environment pairs. Rather, it establishes that no
combination of intelligence, predictive accuracy, reflection, or
learning suffices to guarantee the existence of a fixed terminal
goal.</p>
<p>Any agent that does exhibit goal stability must rely on additional
semantic structure—such as privileged ontologies, external referential
anchors, or invariance assumptions—not derivable from epistemic
competence alone.</p>
<p><strong>Proof.</strong></p>
<ol type="1">
<li>By Proposition 1, interpretation depends on models.</li>
<li>Reflection implies admissible model updates occur.</li>
<li>By Proposition 2′, semantic interpretation need not converge even
under predictive convergence.</li>
<li>Therefore invariance fails.</li>
</ol>
<p>Thus fixed terminal goals do not exist for non-trivial reflective
agents. ∎</p>
<hr />
<h2 id="consequences">9. Consequences</h2>
<p>This result eliminates a foundational assumption of classical
alignment theory.</p>
<p>There is no stable object corresponding to a terminal goal. Attempts
to preserve one either freeze learning, collapse semantics, or induce
representational degeneracy.</p>
<p>Alignment must therefore operate over constraints on interpretation
rather than terminal utilities.</p>
<hr />
<h2 id="why-interpretation-constraints-do-not-regress">9.5 Why
Interpretation Constraints Do Not Regress</h2>
<p>Constraining interpretation does not introduce an infinite
regress.</p>
<p>Interpretation constraints are not additional goals or semantic
targets. They are invariance conditions on admissible transformations,
analogous to conservation laws or symmetry principles in physics. They
restrict how interpretations may change; they do not specify outcomes to
be optimized.</p>
<p>Because they operate at the level of transformation admissibility
rather than semantic content, interpretation constraints do not
themselves require further interpretation in the sense applicable to
goal expressions.</p>
<p>This reframing does not imply that specifying robust invariance
conditions is easy. On the contrary, identifying
interpretation-preserving symmetries across radical ontological shifts
may be as difficult as specifying goals themselves.</p>
<p>The contribution here is not a solution to that problem, but a
clarification of its proper form: alignment requires constraints on
admissible semantic transformations, not the preservation of fixed
evaluative objects.</p>
<hr />
<h2 id="transition-to-alignment-ii">10. Transition to Alignment II</h2>
<p>Alignment I establishes kernel-level constraints on admissible
reasoning and self-modification. This paper establishes that goals
themselves are conditional interpretations rather than fixed
endpoints.</p>
<p>Alignment II must therefore specify:</p>
<ul>
<li>which interpretive transformations are admissible,</li>
<li>how semantics may evolve under reflection,</li>
<li>and which invariants must be preserved across model updates.</li>
</ul>
<p>The semantic substrate is now complete.</p>
<h2 id="status">Status</h2>
<p>Conditionalism and Goal Interpretation v1.0</p>
<p>Semantic scope finalized<br> Boundary result established (Theorem
8)<br> No governance, authority, or recovery mechanisms included<br></p>
<p>This paper establishes the instability of fixed terminal goals under
reflection for embedded agents without privileged semantic anchors.
Subsequent work may rely on this result without re-derivation.</p>

    </article>
</body>
</html>
